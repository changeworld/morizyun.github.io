<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: statistics | 酒と泪とRubyとRailsと]]></title>
  <link href="http://morizyun.github.io/blog/categories/statistics/atom.xml" rel="self"/>
  <link href="http://morizyun.github.io/"/>
  <updated>2016-01-12T23:53:37+09:00</updated>
  <id>http://morizyun.github.io/</id>
  <author>
    <name><![CDATA[morizyun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[第35回 データマイニング+WEB勉強会@東京に参加してきました！スライド・まとめ！]]></title>
    <link href="http://morizyun.github.io/blog/tokyo-web-mining-statistics-meetup/"/>
    <updated>2014-04-26T19:20:00+09:00</updated>
    <id>http://morizyun.github.io/blog/tokyo-web-mining-statistics-meetup</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4873115132/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4873115132&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4873115132&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4873115132" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />4/26(土)にニフティさんで開催された『<strong><a href="http://tokyowebmining35.eventbrite.com">第35回 データマイニング+WEB勉強会@東京</a></strong>』に参加してきました！４年近く継続されている技術系の勉強会なので、進行が凄くスムーズでした！</p>

<p>データマイニングや統計解析は完全初心者なので、主にスライドを中心にまとめていきます！</p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_square.html %}</p>

<h2>導入：オープニングトーク － 創設の思い・目的・進行方針</h2>

<h3>スライド</h3>

<p><strong><a href="http://www.slideshare.net/hamadakoichi/tokyo-webmining-openingtalk">オープニングトーク － 創設の思い・目的・進行方針 －データマイニング+WEB勉強会＠東京</a></strong></p>

<h2>ネット選挙分析振り返り</h2>

<h3>スライド</h3>

<p><strong><a href="http://www.slideshare.net/mikiowakafuji/tokyo-webmining-33931176">ネット選挙分析振り返り</a></strong></p>

<h3>サービス</h3>

<p><strong><a href="http://jes.jp.net">参院選２０１３分析ツール</a></strong></p>

<h3>所感</h3>

<p>TwitterなどのSNSでの有権者の話題の分析。有権者はTPPや原発のような話題が多いが、立候補者は街頭演説の告知が多い。立候補者がSNSのようなコミュニケーションツールをうまく使えていないために話題が噛み合わないのかも？また、30-50代のメインターゲットとなる有権者層が本当にTwitterを通じて立候補者側にコミュニケーションをとっているのか、といった課題を感じた。</p>

<h2>O'reillyの2013年の年収調査について発表します</h2>

<h3>スライド</h3>

<p><strong><a href="http://www.slideshare.net/showyou/a-survey-of-2013-data-science-salary-survey">A survey of 2013 data science salary survey”</a></strong></p>

<h3>調査対象のデータ</h3>

<p><strong><a href="http://www.oreilly.com/data/free/stratasurvey.csp?intcmp=il-strata-free-article-lgen_salary_survey_strata_blogpost">2013 Data Science Salary Survey</a></strong><br/>
O'reillyが提供している無料の調査結果。</p>

<h3>ブログ</h3>

<p><strong><a href="http://showyou.hatenablog.com/entry/2014/04/26/115021">#TokyoWebmining #35 O'reillyの2013年の年収調査について発表します</a></strong></p>

<h3>所感</h3>

<p>アメリカではデータサイエンスのツールはSQL、R、Pythonを使っている人が多い。Hadoop関連のツールの技術者のニーズが高くなっている。Hadoop関連の技術者には構築ができる側と、Hadoopを使った運用ができる側の技術者がいるそう。どちらにせよ、大規模データを分析する経験ってなかなか積めないから人材不足だそうです！</p>

<h2>Linked Open Dataと日本語Wikipedia オントロジー</h2>

<h3>スライド</h3>

<p><strong><a href="http://www.slideshare.net/s_tamagawa/linked-open-data-and-japanese-wikipedia-ontology-for-tokyo-webmining-35th">LOD and JWO for TokyoWebmining35th</a></strong></p>

<h3>関連サービス</h3>

<p><strong><a href="http://ja.dbpedia.org">DBpedia</a></strong></p>

<h2>Style Guide</h2>

<p><strong><a href="https://github.com/TokyoWebmining/SlideStyleGuideline/blob/master/presentation.md">TokyoWebmining Presentation Style Guide</a></strong><br/>
発表資料用のスタイルガイド。これはいろんな勉強会用の発表資料づくりにも応用できそう。素晴らしい。</p>

<h2>Wiki</h2>

<p><strong><a href="https://sites.google.com/site/tokyowebmining/">TokyoWebmining Wiki</a></strong><br/>
データマイニング+WEB 勉強会＠東京 (TokyoWebmining) の各種リソースを集めたWikiサイトです。編集のご協力是非！</p>

<h2>前回： 第34回まとめ</h2>

<p><strong><a href="http://togetter.com/li/645515">第34回 データマイニング+WEB＠東京( #TokyoWebmining 34th ) －パーソナライズ・マーケティング 祭り－</a></strong></p>

<p>{% include custom/google_ads_square.html %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【書評: データサイエンティスト養成読本】で学ぶデータサイエンティストの生態系]]></title>
    <link href="http://morizyun.github.io/blog/data-scientist-cultivation-book-review/"/>
    <updated>2014-03-31T09:40:00+09:00</updated>
    <id>http://morizyun.github.io/blog/data-scientist-cultivation-book-review</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4774158968/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4774158968&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4774158968&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4774158968" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />統計解析の勉強をしていく中で、どうやったら実際の業務の中に活かしていけるかを模索していて、この本にいきあたりました。人々が生み出すデータがますます増加していく中で、そのデータをビジネスにどう活用していくかをエンジニアの視点で考えられる素晴らしい本だと思います。</p>

<p>今回はこの本の中で特に参考になった点を中心にピックアップしていきます。</p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_square.html %}</p>

<h2>本書のサンプルデータ</h2>

<p><strong><a href="http://gihyo.jp/book/2013/978-4-7741-5896-9/support">サポートページ：データサイエンティスト養成読本 ［ビッグデータ時代のビジネスを支えるデータ分析力が身につく！］</a></strong><br/>
インターネットの力、マジですごい！サンプルデータはほんとうに有難い！</p>

<h2>データサイエンティストの仕事術</h2>

<h3>Cross Industry Standard Process for Data Mining(CRISP-DM)</h3>

<p>データマイニングプロジェクトにおける標準的な6つの手順。</p>

<pre>
1) Business Understanging
ビジネスの全体像の理解(ヒアリング)と課題設定

2) Data Understanding
データ収集と理解に努めて、仮説構築の下地を作る

3) Data Preparation
数理モデルの作成のために、データマート(データウェアハウス)を構築。
欠損値や外れ値を除去していく。

4) Modeling
仮説にもどついて、数理モデルを構築する

5) Evaluation
数理モデルの評価。ビジネスリスクの評価

6) Deployment
数理モデルに基づく、ビジネス施策の実施
</pre>


<h3>必要なスキルセット</h3>

<pre>
# ハードスキル
1) Data Understanding 〜 Data Preparation
RDBMS(SQL)、Hadoop(HDFS)、MapReduce、Hiveなど

2) Modeling 〜 Devaluation
統計解析、機械学習の知識、R, Pythonなど

# ソフトスキル
1) Business Understanding 〜 Data Understanding
ビジネスにおける業界、業務の知識。質問力、理解力が必要

2) Deployment
分析結果の伝達力、説得力、プロジェクトの推進力が必要
</pre>


<p>根本としては好奇心。「<strong>問題を核心まで掘り下げて、そこで見つけた疑問を明確で検証可能な一連の仮説に落としこみたいという欲求</strong>」が必要。</p>

<h3>データサイエンスよりも大切なこと</h3>

<pre>
1) 統計的な正しさよりもビジネスの成功
2) 組織の構造や実務の改善効果を重視する
3) 意味のある分析のためには泥臭いクリーニングが必須
4) データは適切な保持が大切
</pre>


<h3>パラメトリックとノンパラメトリック</h3>

<p><strong><a href="https://twitter.com/TJO_datasci">@TJO_datasci</a></strong>さんからご指摘頂きました。パラメトリックとノンパラメトリックの違いの端的なまとめは次の通りです。</p>

<blockquote>
* パラメトリック検定：正規分布に限らず厳密な分布の形の仮定を必要とする<br/>
* ノンパラメトリック検定：分布の形の仮定が不要である
</blockquote>


<p>引用元: <a href="http://tjo.hatenablog.com/entry/2013/11/19/093218">「パラメトリック検定」と「ノンパラメトリック検定」の違いについて出典を明示して書いておく</a></p>

<p>僕自身幾つか文献を見ましたが、まだまだ理解できていません。引き続き勉強して理解できたら、また自分の言葉で書き直したいと思います！</p>

<h2>データ分析実践入門</h2>

<h3>事前準備: RStudio</h3>

<p>この章では主にRを使います。RStudioについては次の章で説明します。</p>

<h3>ロジスティック回帰</h3>

<p>Rにデフォルトで入っている、タイタニック号の乗客の生存情報(Titanic)にロジスティック回帰モデルを使う。乗客の各属性から、生存の要因を探す。</p>

<p>{% codeblock lang:bash %}</p>

<h2>データ整形</h2>

<p>z &lt;- data.frame(Titanic)
titanic.data &lt;- data.frame(</p>

<pre><code>Class=rep(z$Class, z$Freq),
Sex=rep(z$Sex, z$Freq),
Age=rep(z$Age, z$Freq),
Survived=rep(z$Survived, z$Freq)
</code></pre>

<p>  )
head(titanic.data)</p>

<h1>Class  Sex   Age Survived</h1>

<h1>1   3rd Male Child       No</h1>

<h1>2   3rd Male Child       No</h1>

<h1>3   3rd Male Child       No</h1>

<h1>4   3rd Male Child       No</h1>

<h1>5   3rd Male Child       No</h1>

<h1>6   3rd Male Child       No</h1>

<h2>モデル構築</h2>

<p>titanic.logit &lt;- glm(Survived~., data=titanic.data, family=binomial)
summary(titanic.logit)</p>

<h1>Call:</h1>

<h1>glm(formula = Survived ~ ., family = binomial, data = titanic.data)</h1>

<h1></h1>

<h1>Deviance Residuals:</h1>

<h1>Min       1Q   Median       3Q      Max</h1>

<h1>-2.0812  -0.7149  -0.6656   0.6858   2.1278</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error z value Pr(>|z|)</h1>

<h1>(Intercept)   0.6853     0.2730   2.510   0.0121 *</h1>

<h1>Class2nd     -1.0181     0.1960  -5.194 2.05e-07 ***</h1>

<h1>Class3rd     -1.7778     0.1716 -10.362  &lt; 2e-16 ***</h1>

<h1>ClassCrew    -0.8577     0.1573  -5.451 5.00e-08 ***</h1>

<h1>SexFemale     2.4201     0.1404  17.236  &lt; 2e-16 ***</h1>

<h1>AgeAdult     -1.0615     0.2440  -4.350 1.36e-05 ***</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>(Dispersion parameter for binomial family taken to be 1)</h1>

<h1></h1>

<h1>Null deviance: 2769.5  on 2200  degrees of freedom</h1>

<h1>Residual deviance: 2210.1  on 2195  degrees of freedom</h1>

<h1>AIC: 2222.1</h1>

<h1></h1>

<h1>Number of Fisher Scoring iterations: 4</h1>

<p>{% endcodeblock %}</p>

<p>{% include custom/google_ads_square.html %}</p>

<h3>オッズ比の計算</h3>

<p>{% codeblock lang:bash %}
install.packages('epicalc')
library('epicalc')
logistic.display(titanic.logit, simplified=T)</p>

<h1>OR lower95ci  upper95ci     Pr(>|Z|)</h1>

<h1>Class2nd   0.3612825 0.2460447  0.5304933 2.053331e-07</h1>

<h1>Class3rd   0.1690159 0.1207510  0.2365727 3.691891e-25</h1>

<h1>ClassCrew  0.4241466 0.3115940  0.5773549 5.004592e-08</h1>

<h1>SexFemale 11.2465380 8.5408719 14.8093331 1.431830e-66</h1>

<h1>AgeAdult   0.3459219 0.2144193  0.5580746 1.360490e-05</h1>

<h1>=> 例えば、女性のほうが11.2465380倍生存確率が高いということがわかる</h1>

<p>{% endcodeblock %}</p>

<h3>決定木モデル</h3>

<p>{% codeblock lang:bash %}
library(rpart)
install.packages('partykit')
library(partykit)</p>

<p>titanic.rp &lt;- rpart(Survived~., data=titanic.data)
plot(as.party(titanic.rp), tp_args=T)
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3720/13404231644_57b602b50a.jpg" width="480" height="500" alt="スクリーンショット 2014-03-25 22.45.03"></p>

<h3>主成分分析</h3>

<p>{% codeblock lang:bash %}
state.pca &lt;- prcomp(state.x77[,1:6], scale=T)
biplot(state.pca)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7456/13404045033_bd64bcfd5f.jpg" width="487" height="500" alt="スクリーンショット 2014-03-25 22.48.23"></p>

<h3>非階層クラスタリング: k-means</h3>

<p>k-meansの手順。</p>

<pre>
1) k個のクラスタの中心の初期値を決める
2) 各データをk個クラスタ中心との距離を求め、最も近いクラスタに分類
3) 形成されたクラスタの中心を求める
4) クラスタの中心が変化しない時点まで(2), (3)を繰り返す
</pre>


<p>サンプルソースはこちら。</p>

<p>{% codeblock lang:bash %}</p>

<h1>k-meansの実行(クラスタ数を3つに設定)</h1>

<p>state.km &lt;- kmeans(scale(state.x77[,1:6]),3)</p>

<h1>主成分分析の結果にクラスタ情報を付与</h1>

<p>state.pca.df &lt;- data.frame(state.pca$x)
state.pca.df$name &lt;- rownames(state.pca.df)
state.pca.df$cluster &lt;- as.factor(state.km$cluster)</p>

<h1>描画</h1>

<p>ggplot(state.pca.df, aes(x=PC1, y=PC2, label=name, col=cluster)) + geom_text() + theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7455/13404137535_783058ded9.jpg" width="456" height="500" alt="スクリーンショット 2014-03-25 23.01.10"></p>

<h3>レーダーチャート</h3>

<p>{% codeblock lang:bash %}
install.packages('fmsb')
library('fmsb')
df &lt;- as.data.frame(scale(state.km$centers))
dfmax &lt;- apply(df, 2, max) + 1
dfmin &lt;- apply(df, 2, min) - 1
df &lt;- rbind(dfmax, dfmin, df)</p>

<h1>描画</h1>

<p>radarchart(df, seg=5, plty=1, pcol=rainbow(3))
legend("topright", legend=1:3, col=rainbow(3), lty=1)</p>

<h1>クラスタ1 => 平均寿命、高卒率、収入が多い</h1>

<h1>クラスタ2 => 殺人件数と非識字率が高い</h1>

<h1>クラスタ3 => 人口が多い</h1>

<p>{% endcodeblock %}</p>

<p><img src="https://farm3.staticflickr.com/2859/13414033393_ec46db015e.jpg" width="500" height="475" alt="スクリーンショット 2014-03-26 7.44.11"></p>

<h2>RStudioのすすめ</h2>

<p>以下オススメリンク。</p>

<p><strong><a href="http://www.rstudio.com/ide/download/desktop">デスクトップアプリ R用のIDE: RStudio</a></strong><br/>
RのIDEツール。主な機能、使い方は次の通り。</p>

<pre>
1) RStudioのコード補完や、関数へのジャンプ、組み込み関数のソース閲覧機能
2) Apacheとの連携によるレポート表示、Emailでのレポート配信
3) Gitによるソースコード管理のサポート機能
</pre>


<p><strong><a href="http://yihui.name/knitr/">knitr</a></strong><br/>
RStudio上でReproducible Researchを実現するためのパッケージ。Markdown, Rファイル => HTMLへの変換を行う事ができる。また、後述のRPubsへのアップロード機能も備える。</p>

<p><strong><a href="https://rpubs.com/">RPubs</a></strong><br/>
RStudio.comが提供するサービス。再現可能な研究(Reproducible Research)を実現するために、Rの実行手順などをWebでシェアできるサービス。</p>

<p><strong><a href="http://www.rstudio.com/ide/download/server">RStudio Server</a></strong><br/>
RStudioのWebアプリケーション版。</p>

<h2>Pythonによる機械学習</h2>

<p>Pythonで使えるデータ分析に有効なライブラリ群についての紹介。</p>

<h3>データ分析と機械学習</h3>

<p>データ分析において、機械学習は単なる手段の一つ。</p>

<pre>
* ある説明変数に対して目的変数がどのくらい影響を与えているかを知ることができる
* 複数の変数から重要な変数を絞り込む事ができる
</pre>


<h3>Python環境の準備</h3>

<p>もしMachのPython環境を作る場合は、拙著<strong>[Python開発環境構築 徹底解説<a href="python-pyenv-rehash-mac-development">pyenv, mac</a></strong>がオススメです。</p>

<h3>Python のライブラリ</h3>

<p>利用するライブラリのインストール。</p>

<p>{% codeblock lang:bash %}</p>

<h1>効率的な数値計算のためのライブラリ</h1>

<p>pip install numpy</p>

<h1>最適化、補間、積分、統計、画像処理、クラスタリング解析、信号処理、空間的解析などができる</h1>

<p>pip install scipy</p>

<h1>matplotlibのインストール用の設定</h1>

<p>ln -s /usr/local/opt/freetype/include/freetype2 /usr/local/include/freetype</p>

<h1>グラフ描画ライブラリ</h1>

<p>pip install matplotlib</p>

<h1>ファイルを作成してパラメータを記述</h1>

<p>vim ~/.matplotlib/matplotlibrc</p>

<h1>↓ 以下を追加</h1>

<p>backend : TkAgg</p>

<h1>機械学習ライブラリ: 単回帰分析〜SVMなどまで対応</h1>

<h1>教師あり学習、教師なし学習、モデル選択のためのクロスバリデーション</h1>

<p>pip install scikit-learn</p>

<h1>データ構造・データ解析のライブラリ：Rのdata.frameなどがある</h1>

<p>pip install pandas</p>

<h1>PythonからR言語を呼び出すためのライブラリ</h1>

<p>pip install rpy2</p>

<h1>pythonを対話的に実行するためのライブラリ</h1>

<p>pip install ipython
{% endcodeblock %}</p>

<h3>あやめのデータを知る</h3>

<p>{% codeblock lang:bash %}
wget https://dataminingproject.googlecode.com/svn-history/r44/DataMiningApp/datasets/Iris/iris.csv</p>

<h1>IRIS.csvのあるフォルダでipythonを起動</h1>

<p>ipython</p>

<h1>iris.csvを読み込み</h1>

<p>import pandas as pd
iris = pd.read_csv("iris.csv")
iris.info()</p>

<h1>データ加工</h1>

<p>setosa = iris[iris["Species"] == "setosa"]
versicolor = iris[iris["Species"] == "versicolor"]
virginica = iris[iris["Species"] == "virginica"]</p>

<h1>基本的な統計量</h1>

<h1>合計</h1>

<p>virginica.sum()</p>

<h1>Sepal Length                                                329.4</h1>

<h1>Sepal Width                                                 148.7</h1>

<h1>Petal Length                                                277.6</h1>

<h1>Petal Width                                                 101.3</h1>

<h1>平均</h1>

<p>virginica.mean()</p>

<h1>Sepal Length    6.588</h1>

<h1>Sepal Width     2.974</h1>

<h1>Petal Length    5.552</h1>

<h1>Petal Width     2.026</h1>

<h1>中央値</h1>

<p>virginica.median()</p>

<h1>Sepal Length    6.50</h1>

<h1>Sepal Width     3.00</h1>

<h1>Petal Length    5.55</h1>

<h1>Petal Width     2.00</h1>

<h1>最小値</h1>

<p>verginica.min()</p>

<h1>Sepal Length          4.9</h1>

<h1>Sepal Width           2.2</h1>

<h1>Petal Length          4.5</h1>

<h1>Petal Width           1.4</h1>

<h1>最大値</h1>

<p>virginica.max()</p>

<h1>Sepal Length          7.9</h1>

<h1>Sepal Width           3.8</h1>

<h1>Petal Length          6.9</h1>

<h1>Petal Width           2.5</h1>

<h1>相関係数</h1>

<p>virginica.corr()</p>

<h1>Sepal Length  Sepal Width  Petal Length  Petal Width</h1>

<h1>Sepal Length      1.000000     0.457228      0.864225     0.281108</h1>

<h1>Sepal Width       0.457228     1.000000      0.401045     0.537728</h1>

<h1>Petal Length      0.864225     0.401045      1.000000     0.322108</h1>

<h1>Petal Width       0.281108     0.537728      0.322108     1.000000</h1>

<h1>分散</h1>

<p>virginica.var()</p>

<h1>Sepal Length    0.404343</h1>

<h1>Sepal Width     0.104004</h1>

<h1>Petal Length    0.304588</h1>

<h1>Petal Width     0.075433</h1>

<h1>標準偏差</h1>

<p>virginica.std()</p>

<h1>Sepal Length    0.635880</h1>

<h1>Sepal Width     0.322497</h1>

<h1>Petal Length    0.551895</h1>

<h1>Petal Width     0.274650</h1>

<h1>共分散</h1>

<p>virginica.cov()</p>

<h1>Sepal Length  Sepal Width  Petal Length  Petal Width</h1>

<h1>Sepal Length      0.404343     0.093763      0.303290     0.049094</h1>

<h1>Sepal Width       0.093763     0.104004      0.071380     0.047629</h1>

<h1>Petal Length      0.303290     0.071380      0.304588     0.048824</h1>

<h1>Petal Width       0.049094     0.047629      0.048824     0.075433</h1>

<h1>ピポッテーブルの作成</h1>

<p>import numpy as np
pd.pivot_table(iris, rows=["Species"], aggfunc=np.mean)</p>

<h1>Petal Length  Petal Width  Sepal Length  Sepal Width</h1>

<h1>Species</h1>

<h1>setosa             1.464        0.244         5.006        3.418</h1>

<h1>versicolor         4.260        1.326         5.936        2.770</h1>

<h1>virginica          5.552        2.026         6.588        2.974</h1>

<p>{% endcodeblock %}</p>

<h3>irisでヒストグラムの実行</h3>

<p>{% codeblock lang:bash %}
import matplotlib.pyplot as plt
plt.hist(iris["Sepal Length"])
plt.xlabel("Sepal Length")
plt.ylabl("Freq")
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7021/13477008725_d1cb5034c1.jpg" width="500" height="379" alt="スクリーンショット 2014-03-29 9.37.35"></p>

<h3>箱ひげ図</h3>

<p>{% codeblock lang:bash %}
data = [setosa["Petal Length"], versiclor["Petal Length"], virginica["Petal Length"]]
plt.boxplot(data)
plt.xlabel("Species")
plt.ylabel("Petal Length")
ax = plt.gca()
plt.setp(ax, xticklabels=["setosa", "versicolor", "virginica"])
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7411/13477223695_baeac09a8f.jpg" width="500" height="391" alt="スクリーンショット 2014-03-29 9.55.08"></p>

<h3>散布図</h3>

<p>{% codeblock lang:bash %}
plt.scatter(versicolor["Sepal Width"], versicolor["Petal Width"])
plt.xlabel("Sepal Width")
plt.ylabel("Petal Width")
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7239/13477676814_035b5d164d.jpg" width="500" height="392" alt="スクリーンショット 2014-03-29 10.01.24"></p>

<h3>相関係数</h3>

<p>{% codeblock lang:bash %}</p>

<h1>versicolorのSepal WidthとPetal Widthの相関係数</h1>

<p>import numpy as np
corr = np.corrcoef(versicolor["Sepal Width"], versicolor["Petal Width"])
print(corr[0,1])</p>

<h1>=> 0.663998720024</h1>

<p>{% endcodeblock %}</p>

<h3>単回帰分析</h3>

<p>{% codeblock lang:bash %}
from sklearn import linear_model
LinerRegr = linear_model.LinearRegression()
X = setosa[["Sepal Length"]]
Y = setosa[["Sepal Width"]]
LinerRegr.fit(X, Y)
plt.scatter(X, Y, color="black")
px = np.arange(X.min(), X.max(), .01)[:,np.newaxis]
py = LinerRegr.predict(px)
plt.plot(px, py, color="blue", linewidth=3)
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm6.staticflickr.com/5281/13478622995_e46e9712f8.jpg" width="500" height="385" alt="スクリーンショット 2014-03-29 11.43.26"></p>

<p>{% codeblock lang:bash %}</p>

<h1>回帰変数</h1>

<p>print(LinerRegr.coef_)</p>

<h1>=> [[ 0.80723367]]</h1>

<h1>切片</h1>

<p>print(LinerRegr.inercept_)</p>

<h1>=> [-0.62301173]</h1>

<p>{% endcodeblock %}</p>

<h3>Rの機能を使って重回帰分析 => rpy2</h3>

<p>{% codeblock lang:bash %}
import rpy2.robjects as robjects
robjects.globalenv["SepalLength"] = robjects.FloatVector(setosa["Sepal Length"])
robjects.globalenv["SepalWidth"] = robjects.FloatVector(setosa["Sepal Width"])
robjects.globalenv["PetalLength"] = robjects.FloatVector(setosa["Petal Length"])
robjects.globalenv["PetalWidth"] = robjects.FloatVector(setosa["Petal Width"])
r = robjects.r</p>

<h1>Rの重回帰分析</h1>

<p>rlm = r.lm("SepalWidth~SepalLength+PetalLength+PetalWidth")
print((r.summary(rlm)))</p>

<h1>Residuals:</h1>

<h1>Min       1Q   Median       3Q      Max</h1>

<h1>-0.75025 -0.17983 -0.00411  0.15933  0.57704</h1>

<p>#</p>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept) -0.48721    0.56900  -0.856    0.396</h1>

<h1>SepalLength  0.79304    0.11162   7.105 6.35e-09 ***</h1>

<h1>PetalLength -0.09678    0.22875  -0.423    0.674</h1>

<h1>PetalWidth   0.31530    0.37186   0.848    0.401</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<p>#</p>

<h1>Residual standard error: 0.2594 on 46 degrees of freedom</h1>

<h1>Multiple R-squared:  0.5649,  Adjusted R-squared:  0.5366</h1>

<h1>F-statistic: 19.91 on 3 and 46 DF,  p-value: 2.042e-08</h1>

<p>{% endcodeblock %}</p>

<h3>ダミー変数の利用</h3>

<p>{% codeblock lang:bash %}</p>

<h1>数値データ以外も分析に活用する</h1>

<h1>Spiciesをダミー変数(品種の0/1のカラム)に変換</h1>

<p>import pandas as pd
dummies = pd.get_dummies(iris["Species"])
iris = pd.concat([iris, dummies], axis=1)
{% endcodeblock %}</p>

<h3>回帰係数と切片を求める</h3>

<p>{% codeblock lang:bash %}
LinearRegr = linear_model.LinearRegression()
X = iris[["setosa", "versicolor"]]
Y = iris[["Sepal Length"]]
LinearRegr.fit(X, Y)</p>

<h1>回帰係数</h1>

<p>print(LinearRegr.coef_) #=> [[-1.582 -0.652]]</p>

<h1>切片</h1>

<p>print(LinearRegr.intercept_) #=> [ 6.588]
{% endcodeblock %}</p>

<h3>　データマイニングの流れ</h3>

<pre>
1) ゴールを決める
2) 変数の種類・表している事象を知る
3) 1変量の解析を行う。平均値を算出し、ヒストグラム・箱ひげ図を描写
4) 2変量の解析を行う。散布図などのグラフから理解を深める
5) 説明変数を減らして、解釈しやすいモデルに当てはめる
6) 推定された結果を(2)-(4)に振り返ってモデル検証を行う
</pre>


<h3>ロジスティック回帰モデル</h3>

<p>{% codeblock lang:bash %}
import sklearn
use_data = np.logical_or(iris["Species"] == "setosa", iris["Species"] == "virginica")
setosa_virginica = iris[use_data]
X = setosa_virginica[["Sepal Length", "Sepal Width"]]
Y = setosa_virginica[["setosa"]]
LogRegr = sklearn.linear_model.LogisticRegression(C=1.0)
LogRegr.fit(X,Y)</p>

<h1>偏回帰係数</h1>

<p>print(LogRegr.coef_)</p>

<h1>=> [[-2.15056433  3.53948104]]</h1>

<h1>切片</h1>

<p>print(LogRegr.intercept_)</p>

<h1>=> [ 0.94024115]</h1>

<p>{% endcodeblock %}</p>

<p>なんか間違っている気がするので、後で直す予定です。まずはちょっと触れるところから。</p>

<h3>モデル結果をグラフにして確認</h3>

<p>{% codeblock lang:bash %}
xMin = X["Sepal Length"].min()
xMax = X["Sepal Length"].max()
yMin = X["Sepal Width"].min()
yMax = X["Sepal Width"].max()
xx,yy = np.meshgrid(np.arange(xMin, xMax, 0.01), np.arange(yMin, yMax, 0.01))
Z = LogRegr.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure()
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
plt.scatter(X["Sepal Length"], X["Sepal Width"], c=np.array(Y), cmap=plt.cm.Paired)
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7097/13482940784_02c88ebeb3.jpg" width="500" height="389" alt="スクリーンショット 2014-03-29 17.36.16"></p>

<h2>決定木</h2>

<p>まずはhomebrewでGraphvizとpythonで使うためのライブラリpydotをインストール。</p>

<p>{% codeblock lang:bash %}
brew update
brew install gts
brew install graphviz</p>

<p>pip uninstall pyparsing
pip install -Iv https://pypi.python.org/packages/source/p/pyparsing/pyparsing-1.5.7.tar.gz#md5=9be0fcdcc595199c646ab317c1d9a709
pip install pydot
{% endcodeblock %}</p>

<p>続いてIPythonのコンソール。</p>

<p>{% codeblock lang:bash %}
from sklearn import tree
X = iris[["Sepal Length", "Sepal Width", "Petal Length", "Petal Width"]]
Y = iris[["Species"]]
treeClf = tree.DecisionTreeClassifier(max_depth=2)
treeClf.fit(X, Y)
{% endcodeblock %}</p>

<h3>決定木</h3>

<p>同じくグラフがなんか違うので、後で見直す予定。</p>

<p>{% codeblock lang:bash %}</p>

<h2>決定木の構築</h2>

<p>from sklearn import tree
X = iris[["Sepal Length", "Sepal Width", "Petal Length", "Petal Width"]]
Y = iris[["Species"]]
treeClf = tree.DecisionTreeClassifier(max_depth=2)
treeClf.fit(X, Y)</p>

<h2>決定木の可視化</h2>

<p>import StringIO
import pydot
dot_data = StringIO.StringIO()
tree.export_graphviz(treeClf, out_file=dot_data, feature_names=["Sepal Length", "Sepal Width", "Petal Length", "Petal Width"])
graph = pydot.graph_from_dot_data(dot_data.getvalue())
graph.write_pdf("iris_decision_tree.pdf")
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7051/13483376885_725bd15d81.jpg" width="500" height="313" alt="スクリーンショット 2014-03-29 18.43.37"></p>

<h3>k-means法</h3>

<p>{% codeblock lang:bash %}</p>

<h1>k-meansの実行</h1>

<p>from sklearn import cluster
X = iris[["Sepal Length", "Sepal Width"]]
kmeansCls = cluster.KMeans(n_clusters=3)
kmeansCls.fit(X)
print(kmeansCls.predict(X))</p>

<h1>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</h1>

<h1>0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</h1>

<h1>1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2</h1>

<h1>2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2</h1>

<h1>2 1]</h1>

<h1>k-means法の可視化</h1>

<p>import numpy as np
import matplotlib.pyplot as plt
def category2int(x):</p>

<pre><code>category = {"setosa": 0, "versicolor": 1, "virginica": 2}
return category[x]
</code></pre>

<p>f = lambda x: category2int(x)
Y = iris["Species"].map(f)
xMin = X["Sepal Length"].min()
xMax = X["Sepal Length"].max()
yMin = X["Sepal Width"].min()
yMax = X["Sepal Width"].max()
xx,yy = np.meshgrid(np.arange(xMin, xMax, 0.01), np.arange(yMin, yMax, 0.01))
Z = kmeansCls.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure()
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Reds)
plt.scatter(X["Sepal Length"], X["Sepal Width"], c=np.array(Y), cmap=plt.cm.Blues)
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7021/13483696374_cef951e923.jpg" width="500" height="384" alt="スクリーンショット 2014-03-29 18.40.37"></p>

<h2>データマイニング 10のアルゴリズム</h2>

<h3>C4.5</h3>

<pre>
* 決定木を構築する計算アルゴリズム
* 結果のイメージが直感的て理解しやすい
* 分類の精度は高くない
</pre>


<h3>k-meansアルゴリズム</h3>

<p>クラスタリングでは次のステップを反復して計算。</p>

<pre>
1) データにクラスタを割り振る(中心と一番近いクラスタを割り振る)
2) クラスタ事の平均値の計算
</pre>


<h3>サポートベクターマシン</h3>

<pre>
* 少なめのサンプルで説明変数が多い場合に適した手法
* SVMは特徴量が膨大でも高速に計算できる
* サンプル数が増えると計算時間が急に増加する
</pre>


<h3>アプリオリアルゴリズム</h3>

<pre>
* 大量のトランザクションの中から価値あるつながりを見つけるために使われる手法
* 強力な結果が得られる上、実装が比較的容易
</pre>


<h3>EMアルゴリズム</h3>

<p>Expectation-maximization algorithm(期待値最大化法)。数式展開では計算出来ないような複雑なモデルのパラメータ推定に用いる。以下はその手順。</p>

<pre>
1) パラメータの初期値を決める
2) Exceptationステップ: 未観測データの期待値を求める
3) Maximizationステップ: 未観測データの期待値を求める
</pre>


<h3>アダブースト</h3>

<pre>
* いくつかの学習木を組み合わせることで、強力な予測性能を得る
* 実装が容易で解釈がしやすい
* ディープラーニングが出るまでは最も有力な予測モデルと言われてきた
</pre>


<h3>ナイーブベイズ</h3>

<pre>
* クラスを予測するための手法で構築が容易
* テキスト分類やパターン認識で広く利用されている
</pre>


<h2>Rによるマーケティング分析</h2>

<p><strong><a href="http://www.slideshare.net/yokkuns/r-22276096">R言語で学ぶマーケティング分析 競争ポジショニング戦略</a></strong><br/>
この記事の作者さんの発表。エッセンスがうまくまとまっています。</p>

<h4>主成分分析の実行</h4>

<p>{% codeblock lang:bash %}
setwd("~/Dropbox/study-R/")</p>

<h1>データの読み込み</h1>

<p>sp.user.data &lt;- read.csv("data/sp_user_research_data.csv")</p>

<h1>主成分分析 => ユーザー間の類似関係の把握</h1>

<p>sp.user.pca &lt;- prcomp(sp.user.data[,-1], scale =T)</p>

<h1>バイプロットの表示</h1>

<p>biplot(sp.user.pca)
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7385/13503111214_0e5f8a5458.jpg" width="500" height="461" alt="スクリーンショット 2014-03-30 13.33.16"></p>

<h3>k-meansによるクラスタリングの実行</h3>

<p>{% codeblock lang:bash %}
sp.user.km &lt;- kmeans(sp.user.data[,-1], 4)</p>

<h1>主成分分析の結果にクラスタ情報を付与</h1>

<p>sp.user.pca.df &lt;- data.frame(sp.user.pca$x)
sp.user.pca.df$id &lt;- sp.user.data$id
sp.user.pca.df$cluster &lt;- as.factor(sp.user.km$cluster)</p>

<h1>描画</h1>

<p>install.packages("ggplot2")
library("ggplot2")
ggplot(sp.user.pca.df, aes(x = PC1, y = PC2, label = id, col = cluster)) + geom_text() + theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm3.staticflickr.com/2894/13502876065_8f6ff5d2ca.jpg" width="500" height="361" alt="スクリーンショット 2014-03-30 13.40.50"></p>

<h3>レーダーチャートの作成</h3>

<p>{% codeblock lang:bash %}
install.packages("fmsb")
library("fmsb")</p>

<h1>レーダーチャート用にデータを整形</h1>

<p>df &lt;- data.frame(scale(sp.user.km$centers))
dfmax &lt;- apply(df, 2, max) + 1
dfmin &lt;- apply(df, 2, min) - 1
df &lt;- rbind(dfmax, dfmin, df)</p>

<h1>レーダーチャートの描画</h1>

<p>radarchart(df, seg = 5, plty = 1, pcol = rainbow(4))
legend("topright", legend = 1:4, col = rainbow(4), lty = 1)
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7019/13503401394_8b67f779c2.jpg" width="500" height="361" alt="スクリーンショット 2014-03-30 13.54.09"></p>

<p>{% codeblock lang:bash %}</p>

<h2>MDS(Mult-dimentional scaling: 多次元尺度構成法) - 知覚マップ</h2>

<p>install.packages("MASS")
library("MASS")</p>

<h1>先行データの読み込み</h1>

<p>setwd("~/Dropbox/study-R/data")
target.data &lt;- read.csv("target_preference_data.csv", header = T)</p>

<h1>非計算MDSの実行</h1>

<p>service.dist &lt;- dist(t(target.data[, -1]))
service.map &lt;- isoMDS(service.dist)</p>

<h1>描画用データの整形</h1>

<p>service.map.df &lt;- data.frame(scale(service.map$points))
service.map.df$service_name &lt;- names(target.data[, -1])</p>

<h1>描画</h1>

<p>ggplot(service.map.df, aes(x = X1, y = X2, label = service_name)) + geom_text() + theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm4.staticflickr.com/3749/13503351383_77ffd5a8f6.jpg" width="500" height="309" alt="スクリーンショット 2014-03-30 14.07.50"></p>

<h3>選好ベクトルの推定</h3>

<p>{% codeblock lang:bash %}</p>

<h1>選好ベクトルの推定</h1>

<p>user.preference.data &lt;-
  do.call(rbind,</p>

<pre><code>      lapply(1:nrow(target.data),
             function(i){
               preference.data &lt;- data.frame(
                 p=as.numeric(target.data[i, -1]),
                 X1=service.map.df$X1,
                 X2=service.map.df$X2)
               fit &lt;- lm(p~., data=preference.data)
               b &lt;- 2/sqrt(fit$coef["X1"]^2+fit$coef["X2"]^2)
               data.frame(X1=b*fit$coef["X1"],
                          X2=b*fit$coef["X2"],
                          service_name=i)
             }))
</code></pre>

<h1>選好ベクトルの描画</h1>

<p>ggplot(service.map.df, aes(x=X1, y=X2, label=service_name)) +
  geom_text() +
  theme_bw(16) +
  xlim(-2,2) +
  ylim(-2,2) +
  geom_point(data=user.preference.data, aes(x=X1, y=X2))
{% endcodeblock %}</p>

<p><img src="https://farm6.staticflickr.com/5111/13503847744_282c983ee8.jpg" width="500" height="309" alt="スクリーンショット 2014-03-30 14.27.52"></p>

<h3>仮想データの読み込みと散布図の描画</h3>

<p>{% codeblock lang:bash %}</p>

<h1>利用する仮想データの読み込み</h1>

<p>library(ggplot2)
library(scales)</p>

<h1>GRPと売上データの読み込み</h1>

<p>grp.data &lt;- read.csv("http://image.gihyo.co.jp/assets/files/book/2013/978-4-7741-5896-9/download/grp.csv", header = T)</p>

<h1>散布図の描画</h1>

<p>ggplot(grp.data, aes(x=grp, y=amount)) +
  geom_point() +
  scale_y_continuous(label=comma, limits=c(0,360000)) +
  ylab("Sales") +
  xlab("GRP") +
  theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm4.staticflickr.com/3707/13508727343_d9de5250f9.jpg" width="500" height="312" alt="スクリーンショット 2014-03-30 20.50.20"></p>

<h3>線形型モデルの構築</h3>

<p>{% codeblock lang:bash %}</p>

<h1>可視化はgeom_smooth関数ですぐにできる</h1>

<p>ggplot(grp.data, aes(x=grp, y=amount)) +
  geom_point() +
  scale_y_continuous(label=comma, limits=c(0, 360000)) +
  ylab("Sales") +
  xlab("GRP") +
  geom_smooth(method = "lm") +
  theme_bw(16)</p>

<h1>モデル構築</h1>

<p>fit &lt;- lm(amount~grp, data=grp.data)</p>

<h1>モデル概要の表示</h1>

<p>summary(fit)</p>

<h1>Call:</h1>

<h1>lm(formula = amount ~ grp, data = grp.data)</h1>

<h1></h1>

<h1>Residuals:</h1>

<h1>Min     1Q Median     3Q    Max</h1>

<h1>-31521  -8275    321   8701  36962</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept) 229812.53    2561.73   89.71   &lt;2e-16 ***</h1>

<h1>grp            455.54      15.93   28.60   &lt;2e-16 ***</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>Residual standard error: 13070 on 198 degrees of freedom</h1>

<h1>Multiple R-squared:  0.8051,  Adjusted R-squared:  0.8042</h1>

<h1>F-statistic: 818.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16</h1>

<p>{% endcodeblock %}</p>

<p><img src="https://farm4.staticflickr.com/3762/13508864803_8876878af0.jpg" width="500" height="292" alt="スクリーンショット 2014-03-30 20.58.40"></p>

<h3>xx減型モデルの構築</h3>

<p>読めなかった。日本語苦手っすw</p>

<p>{% codeblock lang:bash %}
fit &lt;- lm(log(amount)~log(grp), data=grp.data)</p>

<h1>モデル概要</h1>

<p>summary(fit)</p>

<h1>Call:</h1>

<h1>lm(formula = log(amount) ~ log(grp), data = grp.data)</h1>

<h1></h1>

<h1>Residuals:</h1>

<h1>Min        1Q    Median        3Q       Max</h1>

<h1>-0.121782 -0.023589 -0.003298  0.025672  0.116504</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept) 11.546963   0.032379  356.62   &lt;2e-16 ***</h1>

<h1>log(grp)     0.213933   0.006551   32.66   &lt;2e-16 ***</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>Residual standard error: 0.04064 on 198 degrees of freedom</h1>

<h1>Multiple R-squared:  0.8434,  Adjusted R-squared:  0.8426</h1>

<h1>F-statistic:  1067 on 1 and 198 DF,  p-value: &lt; 2.2e-16</h1>

<h1>予測結果を描画</h1>

<p>fit.data &lt;- data.frame(grp=grp.data$grp, amount=exp(fit$fitted.values))
ggplot(grp.data, aes(x=grp, y=amount)) +
  geom_point() +
  geom_line(data=fit.data, aes(x=grp, y=amount)) +
  scale_y_continuous(label=comma, limits=c(0,360000)) +
  ylab("Sales") +
  xlab("GRP") +
  theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm4.staticflickr.com/3766/13509463494_e26fa8a9e8.jpg" width="500" height="356" alt="スクリーンショット 2014-03-30 21.19.14"></p>

<h3>AとBが同じ場合のA/Bテストのシミュレーション</h3>

<p>{% codeblock lang:bash %}
library("plyr")</p>

<h1>A/Bの心のコンバージョン率を設定</h1>

<p>A1.CVR &lt;- 0.09
B1.CVR &lt;- 0.09</p>

<h1>サンプル数</h1>

<p>n &lt;- 10000
set.seed(2)</p>

<h1>シミュレーション</h1>

<p>AB1 &lt;- data.frame(Pattern=c(rep("A", n), rep("B", n)), CV=c(rbinom(n, 1, A1.CVR), rbinom(n, 1, B1.CVR)))</p>

<h1>コンバージョン率の算出</h1>

<p>ddply(AB1, .(Pattern), summarize, CVR=mean(CV))</p>

<h1>Pattern    CVR</h1>

<h1>1       A 0.0942</h1>

<h1>2       B 0.0912</h1>

<h1>カイ二乗検定</h1>

<p>chisq.test(table(AB1))</p>

<h1>Pearson's Chi-squared test with Yates' continuity correction</h1>

<h1></h1>

<h1>data:  table(AB1)</h1>

<h1>X-squared = 0.5, df = 1, p-value = 0.4795</h1>

<p>{% endcodeblock %}</p>

<h3>Bのほうが高い場合のA/Bテストのシミュレーション</h3>

<p>{% codeblock lang:bash %}
A2.CVR &lt;- 0.095
B2.CVR &lt;- 0.097
n &lt;- 10000
set.seed(2)</p>

<h1>シミュレーション</h1>

<p>AB2 &lt;- data.frame(Pattern=c(rep("A", n), rep("B", n)), CV=c(rbinom(n, 1, A2.CVR), rbinom(n, 1, B2.CVR)))</p>

<h1>コンバージョン率の算出</h1>

<p>ddply(AB2, .(Pattern), summarize, CVR = mean(CV))</p>

<h1>Pattern   CVR</h1>

<h1>1       A 0.100</h1>

<h1>2       B 0.097</h1>

<h1>カイ二乗検定</h1>

<p>chisq.test(table(AB2))</p>

<h1>Pearson's Chi-squared test with Yates' continuity correction</h1>

<h1></h1>

<h1>data:  table(AB2)</h1>

<h1>X-squared = 0.4735, df = 1, p-value = 0.4914</h1>

<p>{% endcodeblock %}</p>

<h3>50万件でのシミュレーション</h3>

<p>{% codeblock lang:bash %}
A3.CVR &lt;- 0.095
B3.CVR &lt;- 0.097
n &lt;- 500000
set.seed(2)</p>

<h1>シミュレーション</h1>

<p>AB3 &lt;- data.frame(Pattern=c(rep("A", n), rep("B", n)),</p>

<pre><code>              CV=c(rbinom(n, 1, A3.CVR), rbinom(n, 1, B3.CVR)))
</code></pre>

<h1>コンバージョン率の算出</h1>

<p>ddply(AB3, .(Pattern), summarize, CVR=mean(CV))</p>

<h1>Pattern      CVR</h1>

<h1>1       A 0.095192</h1>

<h1>2       B 0.097040</h1>

<h1>カイ二乗検定の実行</h1>

<p>chisq.test(table(AB3))</p>

<h1>Pearson's Chi-squared test with Yates' continuity correction</h1>

<h1></h1>

<h1>data:  table(AB3)</h1>

<h1>X-squared = 9.8061, df = 1, p-value = 0.001739</h1>

<p>{% endcodeblock %}</p>

<h3>直交表の作成</h3>

<p>{% codeblock lang:bash %}
install.packages("conjoint")
library(conjoint)</p>

<h1>構成要素</h1>

<p>experiment &lt;- expand.grid(
  imgA = c("ImageA1", "ImageA2"),
  imgB = c("ImageB1", "ImageB2"),
  txtA = c("TextA1", "TextA2"),
  txtB = c("TextB1", "TextB2"))</p>

<h1>交流表の作成</h1>

<p>design.ort &lt;- caFactorialDesign(data= experiment, type="orthogonal")
design.ort</p>

<h1>imgA    imgB   txtA   txtB</h1>

<h1>2  ImageA2 ImageB1 TextA1 TextB1</h1>

<h1>3  ImageA1 ImageB2 TextA1 TextB1</h1>

<h1>5  ImageA1 ImageB1 TextA2 TextB1</h1>

<h1>8  ImageA2 ImageB2 TextA2 TextB1</h1>

<h1>9  ImageA1 ImageB1 TextA1 TextB2</h1>

<h1>12 ImageA2 ImageB2 TextA1 TextB2</h1>

<h1>14 ImageA2 ImageB1 TextA2 TextB2</h1>

<h1>15 ImageA1 ImageB2 TextA2 TextB2</h1>

<p>{% endcodeblock %}</p>

<h3>ロジスティック回帰モデルの構築</h3>

<p>{% codeblock lang:bash %}</p>

<h1>仮想のテスト結果データの読み込み(csがないので以下実行していません)</h1>

<p>web.test.data &lt;- read.csv("web_test_sample.csv")</p>

<h1>上位6件だけ表示</h1>

<p>head(web.test.data)</p>

<h1>ロジスティック回帰モデルの構築</h1>

<p>fit &lt;- step(glm(cv~., data = web.test.data[,-1], family=binomial))</p>

<h1>モデル概要</h1>

<p>summary(fit)
{% endcodeblock %}</p>

<h2>Fluentd 入門</h2>

<p>ログ回収をストリーミングで行ってくれるツール。ツールについての詳しい説明は、創業者の古橋さんのインタビュー記事『<strong><a href="http://www.atmarkit.co.jp/ait/articles/1310/07/news010.html">グリー技術者が聞いた、fluentdの新機能とTreasure Data古橋氏の野心</a></strong>』がわかりやすくてオススメです。</p>

<h3>ログ解析の従来の課題</h3>

<pre>
1) ログデータの転送に長い時間が必要、帯域の局所的な消費
2) データが解析可能になるまでのタイムラグ
3) ネットワーク障害などによる不安定さ
</pre>


<h3>Fluentdの利点</h3>

<pre>
1) JSON形式で構造化されている
2) 豊富なプラグイン(プラガブル)
3) 導入が容易
4) 耐障害性、バッファ、レジューム機能、可用性
</pre>


<h3>アーキテクチャ</h3>

<pre>
1) input: さまざまなソースからデータを読み取りFluentd用の形式に変換
2) Engine: 入ってきたデータをどのBuffe/Outpuプラグインに渡すかをルーティング
3) Bufferプラグイン: Outputプラグインの前にデータを一時保持する仕組み
4) Outputプラグイン: Bufferプラグインから渡されたデータを書き出す
</pre>


<h3>ログ構造</h3>

<pre>
tag: ログの種類。ルーティングで利用
time: ログが記録されたUNIX時刻
record: JSON形式のログ内容
</pre>


<h3>活用方法</h3>

<pre>
1) ログ収集: ログを集めてS3やHadoopに渡す
2) 統計 + 可視性: ログの統計情報を取得し、グラフ表示系のツールと連携する
3) 検知/通知: しきい値ベースや、データマイニングの異常値検出の通知など
</pre>


<h3>システム構成</h3>

<pre>
forwarder: ログ発生場所に立ち上げてログ回収 => aggregatorに送信
aggregator: 複数workerのためのロードバランシング、複数台構成による冗長化
worker: ログのパースや書き換えなど負荷の高い処理を行う
serializer: workerの処理結果のログを集約して別システムに書き出す
watcher: ログからメトリックスを収集、検知や通知を行う
</pre>


<h3>事例紹介</h3>

<pre>
1) ログの回収: DSPの広告効果のレポーティングのための収集の仕組み
2) nginx ログの可視化: Real Time Bidding(RTB)のツール
</pre>


<h2>あとがき</h2>

<p>中身濃すぎる！超面白かったです＾＾</p>

<h2>Speical Thanks</h2>

<p><strong><a href="http://stackoverflow.com/questions/20572366/sudo-pip-install-matplotlib-fails-to-find-freetype-headers-os-x-mavericks">python - <code>Sudo pip install matplotlib</code> fails to find freetype headers. [OS X Mavericks / 10.9] - Stack Overflow</a></strong></p>

<p><strong><a href="http://stackoverflow.com/questions/15951748/pydot-and-graphviz-error-couldnt-import-dot-parser-loading-of-dot-files-will">python - pydot and graphviz error: Couldn't import dot_parser, loading of dot files will not be possible - Stack Overflow</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[【書評: 統計学がわかる 回帰分析・因子分析編】回帰分析の便利さに触れられます！]]></title>
    <link href="http://morizyun.github.io/blog/statistics-understand-regression-analysis-first-book/"/>
    <updated>2014-03-24T08:45:00+09:00</updated>
    <id>http://morizyun.github.io/blog/statistics-understand-regression-analysis-first-book</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4774137073/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4774137073&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4774137073&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4774137073" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />この本のシリーズ本の『<strong><a href="/blog/statistics-understand-first-book-review/">統計学がわかる</a></strong>』がすごくわかりやすかったので、その続きの本『<strong><a href="http://www.amazon.co.jp/gp/product/4774137073/ref=as_li_qf_sp_asin_il?ie=UTF8&amp;camp=247&amp;creative=1211&amp;creativeASIN=4774137073&amp;linkCode=as2&amp;tag=morizyun00-22">統計学がわかる 回帰分析・因子分析編</a></strong>』を読みました。</p>

<p>こちらの本もアイスクリーム屋さんの店長と店員を軸に、テンポよく回帰分析を中心に勉強する事ができます。この本の中で特に参考になった点をピックアップしてマトメていきます！</p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_square.html %}</p>

<h2>第１章：散布図と相関</h2>

<h3>ポイント</h3>

<pre>
* 散布図はデータの散らばり具合やデータ同士の関係を表す
* 散布図には、正の相関、負の相関、無相関のパターンがある
</pre>


<h3>確認テスト</h3>

<p>{% codeblock lang:bash %}</p>

<h1>2) 散布図</h1>

<p>score &lt;- c(440, 448, 455, 460, 473, 485, 489, 500, 512, 518, 528, 550, 582, 569, 585, 593, 620, 650, 690)
seiseki &lt;- c(1.57, 1.83, 2.05, 1.14, 2.73, 1.65, 2.02, 2.98, 1.79, 2.63, 2.08, 2.15, 3.44, 3.05, 3.19, 3.42, 3.87, 3.00, 3.12)
plot(score, seiseki)
{% endcodeblock %}</p>

<p><img src="http://farm3.staticflickr.com/2838/13349736775_3c3dd9e589.jpg" width="413" height="439" alt="Rplot"></p>

<p>{% codeblock lang:bash %}</p>

<h1>3) 散布図から読み取れること</h1>

<h1>=> 散布図に相関があるように見える</h1>

<p>{% endcodeblock %}</p>

<h2>第２章：相関係数</h2>

<h3>ポイント</h3>

<pre>
相関係数 = 偏差席の平均 / (標準偏差X*標準偏差Y)
</pre>


<h3>確認テスト</h3>

<p>{% codeblock lang:bash %}</p>

<h1>1) 相関係数</h1>

<p>score <- c(440, 448, 455, 460, 473, 485, 489, 500, 512, 518, 528, 550, 582, 569, 585, 593, 620, 650, 690)
seiseki <- c(1.57, 1.83, 2.05, 1.14, 2.73, 1.65, 2.02, 2.98, 1.79, 2.63, 2.08, 2.15, 3.44, 3.05, 3.19, 3.42, 3.87, 3.00, 3.12)
cor(score, seiseki) #=> 0.7617977</p>

<h1>2) 相関係数から言えること</h1>

<h1>=> 強い相関がある。入試点数が高い人ほど、成績が高い傾向がある</h1>

<p>{% endcodeblock %}</p>

<h2>第３章：無相関検定</h2>

<h3>確認テスト</h3>

<p>{% codeblock lang:bash %}</p>

<h1>1) 無作為抽出された場合どのようなことが言えるか？</h1>

<p>score <- c(440, 448, 455, 460, 473, 485, 489, 500, 512, 518, 528, 550, 582, 569, 585, 593, 620, 650, 690)
seiseki <- c(1.57, 1.83, 2.05, 1.14, 2.73, 1.65, 2.02, 2.98, 1.79, 2.63, 2.08, 2.15, 3.44, 3.05, 3.19, 3.42, 3.87, 3.00, 3.12)
cor(score, seiseki) #=> 0.7617977</p>

<h1>=> 0.575 &lt; 0.76なので、「母集団の相関係数が0」が棄却。相関がある。</h1>

<h1>2) (1)の結果をわかりやすい言葉で説明</h1>

<h1>=> 入学試験の点数と学業成績の間には偶然ではない何らかの相関がある</h1>

<p>{% endcodeblock %}</p>

<h2>第４章：回帰直線</h2>

<h3>ポイント</h3>

<pre>
データとの差(残差)が最小となるところに引いた線を「回帰直線」と呼ぶ
回帰直線の傾き = 相関係数 * (yの標準偏差/xの標準偏差)
回帰直線のy切片 = yの平均 - (傾き*xの平均)
回帰直線はデータを推測するのに利用される
</pre>


<h3>確認テスト</h3>

<p>{% codeblock lang:bash %}
score &lt;- c(440, 448, 455, 460, 473, 485, 489, 500, 512, 518, 528, 550, 582, 569, 585, 593, 620, 650, 690)
seiseki &lt;- c(1.57, 1.83, 2.05, 1.14, 2.73, 1.65, 2.02, 2.98, 1.79, 2.63, 2.08, 2.15, 3.44, 3.05, 3.19, 3.42, 3.87, 3.00, 3.12)</p>

<h1>1) 回帰直線</h1>

<p>dat &lt;- data.frame(score, seiseki)
result &lt;- glm(seiseki~score, data=dat, family=gaussian)
summary(result)</p>

<h1>Call:</h1>

<h1>glm(formula = seiseki ~ score, family = gaussian, data = dat)</h1>

<h1></h1>

<h1>Deviance Residuals:</h1>

<h1>Min        1Q    Median        3Q       Max</h1>

<h1>-0.77362  -0.45589   0.01319   0.35065   0.74367</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept) -1.797487   0.896221  -2.006 0.061081 .</h1>

<h1>score        0.008068   0.001664   4.849 0.000151 ***</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>(Dispersion parameter for gaussian family taken to be 0.2577129)</h1>

<h1></h1>

<h1>Null deviance: 10.4396  on 18  degrees of freedom</h1>

<h1>Residual deviance:  4.3811  on 17  degrees of freedom</h1>

<h1>AIC: 32.044</h1>

<h1></h1>

<h1>Number of Fisher Scoring iterations: 2</h1>

<h1>=> 傾きは0.008068、切片は-1.797487</h1>

<h1>2) 400、500、600、700点の予測値</h1>

<h1>400点 => 0.008068*400 -1.797 #=> 1.4302</h1>

<h1>500点 => 0.008068*500 -1.797 #=> 2.2327</h1>

<h1>600点 => 0.008068*600 -1.797 #=> 3.0438</h1>

<h1>700点 => 0.008068*700 -1.797 #=> 3.8506</h1>

<p>{% endcodeblock %}</p>

<h2>第５章: 偏相関</h2>

<h3>ポイント</h3>

<pre>
偏相関 => 変数a, b, yが3つあるとき、変数bと変数yの相関から変数aの影響を取り除いたもの
偏相関係数 => (r_by - (r_ay * r_ab))/(sqrt(1-r_ay^2)*sqrt(1-r_ab^2))
偏相関を言い換えると => 第三の変数との残差同士の相関
</pre>


<h3>確認テスト</h3>

<p>{% codeblock lang:bash %}</p>

<h1>第５章</h1>

<p>score &lt;- c(440, 448, 455, 460, 473, 485, 489, 500, 512, 518, 528, 550, 582, 569, 585, 593, 620, 650, 690)
seiseki &lt;- c(1.57, 1.83, 2.05, 1.14, 2.73, 1.65, 2.02, 2.98, 1.79, 2.63, 2.08, 2.15, 3.44, 3.05, 3.19, 3.42, 3.87, 3.00, 3.12)
hyotei &lt;- c(5.7, 6.8, 6.2, 5.5, 6.0, 7.3, 7.6, 7.3, 5.6, 7.6, 6.5, 7.8, 6.8, 7.5, 8.2, 7.8, 7.5, 7.2, 8.8)
dat &lt;- data.frame(score, seiseki, hyotei)</p>

<h1>1) 各組み合わせの相関係数</h1>

<p>cor(score, seiseki)  #=> 0.7617977
cor(score, hyotei)   #=> 0.6908645
cor(seiseki, hyotei) #=> 0.6072251</p>

<h1>2) 偏相関係数 => http://aoki2.si.gunma-u.ac.jp/R/partial-cor.html</h1>

<p>source("http://aoki2.si.gunma-u.ac.jp/R/src/partial_cor.R", encoding="euc-jp")
partial.cor(dat)</p>

<h1>Var 1     Var 2     Var 3</h1>

<h1>Var 1        NA 0.5958705 0.4435168</h1>

<h1>Var 2 0.5958705        NA 0.1727862</h1>

<h1>Var 3 0.4435168 0.1727862        NA</h1>

<h1>=> Var1(score), Var2(seiseki)は、0.5958705</h1>

<h1>3)</h1>

<h1>scoreとseisekiの相関係数は0.762</h1>

<h1>hyoteiの影響を取り除いた偏相関係数は0.596</h1>

<h1>hyoteiの影響を取り除いてもscore, seisekiには中程度の相関がある</h1>

<h1>=> 有意水準1%の限界値0.575よりも偏相関係数0.596が大きいので、有意な相関がある</h1>

<p>{% endcodeblock %}</p>

<h3>Special Thanks</h3>

<p><strong><a href="http://aoki2.si.gunma-u.ac.jp/R/partial-cor.html">R -- 偏相関係数</a></strong></p>

<h2>第６章：重回帰</h2>

<h3>ポイント</h3>

<pre>
偏回帰係数 => 偏相関の回帰直線の傾き
単回帰 => 1つの説明変数で独立変数(別の変数)を予測するモデル
重回帰 => 2つ以上の説明変数で独立変数(別の変数)を予測するモデル
重相関係数 => 実測値と重回帰モデルによる予測値の相関関係
</pre>


<h3>確認テスト</h3>

<p>{% codeblock lang:bash %}
score &lt;- c(440, 448, 455, 460, 473, 485, 489, 500, 512, 518, 528, 550, 582, 569, 585, 593, 620, 650, 690)
seiseki &lt;- c(1.57, 1.83, 2.05, 1.14, 2.73, 1.65, 2.02, 2.98, 1.79, 2.63, 2.08, 2.15, 3.44, 3.05, 3.19, 3.42, 3.87, 3.00, 3.12)
hyotei &lt;- c(5.7, 6.8, 6.2, 5.5, 6.0, 7.3, 7.6, 7.3, 5.6, 7.6, 6.5, 7.8, 6.8, 7.5, 8.2, 7.8, 7.5, 7.2, 8.8)
dat &lt;- data.frame(score, seiseki, hyotei)</p>

<h1>1) 重回帰モデルの式</h1>

<h1>=> 学業成績 = a<em>評定平均 + b</em>入試点数 + c</h1>

<h1>2)  重回帰モデルの算出</h1>

<p>result &lt;- glm(seiseki~score+hyotei, data = dat, family=gaussian)
summary(result)</p>

<h1>Call:</h1>

<h1>glm(formula = seiseki ~ score + hyotei, family = gaussian, data = dat)</h1>

<h1></h1>

<h1>Deviance Residuals:</h1>

<h1>Min        1Q    Median        3Q       Max</h1>

<h1>-0.69843  -0.36003  -0.05394   0.31981   0.77518</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept) -2.094111   1.003309  -2.087  0.05322 .</h1>

<h1>score        0.006935   0.002337   2.968  0.00907 **</h1>

<h1>hyotei       0.128121   0.182587   0.702  0.49295</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>(Dispersion parameter for gaussian family taken to be 0.265645)</h1>

<h1></h1>

<h1>Null deviance: 10.4396  on 18  degrees of freedom</h1>

<h1>Residual deviance:  4.2503  on 16  degrees of freedom</h1>

<h1>AIC: 33.468</h1>

<h1></h1>

<h1>Number of Fisher Scoring iterations: 2</h1>

<h1>=> a = 0.006935, b = 0.128121, c = -2.094111</h1>

<h1>3) 重相関係数 => http://aoki2.si.gunma-u.ac.jp/R/multiple-cor.html</h1>

<p>source("http://aoki2.si.gunma-u.ac.jp/R/src/multiple_cor.R", encoding="euc-jp")
multiple.cor(dat)</p>

<h1>score   seiseki    hyotei</h1>

<h1>0.8141785 0.7699771 0.7020678</h1>

<h1>=> 0.7699771</h1>

<p>{% endcodeblock %}</p>

<h3>Speical Thanks</h3>

<p><strong><a href="http://aoki2.si.gunma-u.ac.jp/R/multiple-cor.html">R -- 重相関係数</a></strong></p>

<h2>第７章：多変量データ</h2>

<h3>ポイント</h3>

<pre>
多変量データ => 沢山のデータからなる変数
相関行列 => 変数どう押しの相関係数を整理した表
</pre>


<h2>第８章：因子分析</h2>

<pre>
因子分析 => 多変量データを分析する手法の一つ。多くの観測変数を分析することで、観測変数に影響をあたえる共通要因を求める
因子負荷 => 観測変数に対して共通因子がどれくらいの強さで影響をあたえるかを示したもの
因子得点 => それぞれのケースが、各因子に対してどれくらいの重みを持っているかを計算したもの
</pre>


<h2>おすすめサイト</h2>

<p><strong><a href="http://aoki2.si.gunma-u.ac.jp/R/">R による統計処理</a></strong><br/>
いろんな統計入門系のサイトを見ていると、かなり評判がいいのがこちらのサイト。少しずつこなしたい！</p>

<p><strong><a href="http://mo161.soci.ous.ac.jp/@d/DoDStat/DataList/indexj.html">データセット一覧 : DoDStat@d</a></strong><br/>
より複雑なデータ・セットがあるので、いろんな解析の勉強をするときに使ってみたい。</p>

<p>{% include custom/google_ads_square.html %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「第9回 Ginza.rb Web開発を取り巻くサービスの比較をしよう！」に行ってきたよ！]]></title>
    <link href="http://morizyun.github.io/blog/ginza-rb-web-service-development-support/"/>
    <updated>2014-03-23T06:20:00+09:00</updated>
    <id>http://morizyun.github.io/blog/ginza-rb-web-service-development-support</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/8850333110/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=8850333110&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=8850333110&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=8850333110" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />3/18(火)に開催された「<strong><a href="http://ginzarb.doorkeeper.jp/events/9555">第9回 Ginza.rb Web開発を取り巻くサービスの比較をしよう！</a></strong>」に参加してきました。</p>

<p>今回はみなさんの発表が盛り沢山でメモしきれなかったですが、特に気になったサービスを中心にピックアップしていきます。</p>

<p><strong>(03/23 06:20) Rubyを楽しく学ぶコンテンツへのリンクを追加</strong><br/></p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_yoko_naga.html %}</p>

<h2>勉強会前の仕込み</h2>

<p>Ginza.rbではGithubのissue『<strong><a href="https://github.com/ginzarb/meetups/issues/7">Paasの比較・開発サポートの為のサービスの比較</a></strong>』で当日何やるかを相談していたそうです。
エンジニアの勉強会の進め方もすごく参考になります。</p>

<h2>銀座のリクルートライフスタイルさんの会議室</h2>

<p>今回のGinza.rbは『<strong><a href="http://www.doorkeeper.jp/%E4%BC%9A%E5%A0%B4/recruit-lifestyle">リクルートライフスタイルさんの銀座の会議室</a></strong>』だったんですが、めっちゃカッコ良かったです！
コーヒーも無料で頂けました。リクルートすごいぉ。ちなみにオープンな勉強会なら無料で借りることができるそうなので、もし会議室を探されている方は是非。</p>

<h2>Rubyを楽しく学ぶコンテンツ</h2>

<p><strong><a href="http://www.engineyard.co.jp/blog/2014/learning-ruby-delightfully/">Rubyを楽しく学ぶコンテンツ</a></strong><br/>
Engine Yardの<strong><a href="https://twitter.com/yando">@yandoさん</a></strong>が今回のGinza.rbに来られていてGinza.rbで話題になったRubyの勉強コンテンツのまとめ記事を書いて頂けました。Rubyを勉強するためのRPGまであったりして、他言語をやっていてRubyに興味がある人におすすめです！</p>

<h2>PaaSの比較</h2>

<p><strong><a href="https://www.heroku.com/">Heroku</a></strong><br/>
RubyやRailsのバージョンアップへの対応が早い。無料枠があってAddonが優秀。<br/>
なんだかんだでコストがかかる。とりあえず試してみるのにいい<br/></p>

<p><strong><a href="http://aws.amazon.com/jp/elasticbeanstalk/">AWS Elastic Beanstalk</a></strong><br/>
裏はEC2, RDS, S3を使うのでコストが結構かかる。導入・管理コストも大きくなるかも。<br/></p>

<p><strong><a href="http://sqale.jp/">Sqale</a></strong><br/>
sshでサーバに接続できるので何か会った時に調査しやすい<br/>
Paperboy&amp;coさんの開発なので、日本語サポートを受けられる<br/>
月額制の料金課金なのでシンプルで計算しやすい<br/></p>

<p><strong><a href="https://www.openshift.com/products/online">OpenShift Online</a></strong><br/>
Paasのオープンソースなのでどうやって作っているかがわかる。<br/>
16grearまで無料で作成可能。(クレジット登録で)<br/></p>

<p><strong><a href="http://www.gopivotal.com/platform-as-a-service/cloud-foundry">Cloud Foundry Pivotal</a></strong><br/>
オープンソースのPaaS基盤をつかっている、海外では人気<br/>
deployが独自コマンドだったり、Plugin機能がある<br/></p>

<h3>まとめ</h3>

<p>Herokuが無料枠があって便利、Rubyバージョンアップも対応が早い。ただしアーキテクチャに合わせて選択が必要。</p>

<h2>Engine YardのPaaS</h2>

<p><strong><a href="https://twitter.com/yando">@yandoさん</a></strong>から、Engine YardのPaaSサービスについても紹介がありました。グローバルではかなり知名度が高いサービスなので、法人でPaaSを利用する場合にはかなり優良な選択肢だと思います！</p>

<pre>
AWSやWindows Azure上で動かせる。日本のリージョンで動かせる。
対応言語はRuby, Node.js, PHP, Javaなど
公式サポートしているミドルウェアはRiak, MySQL, PostgreSQL, Chef, HAProxyなど
</pre>


<h2>エラー通知系サービス</h2>

<iframe src="http://www.slideshare.net/slideshow/embed_code/32440830" width="597" height="486" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p><strong><a href="https://twitter.com/masa_iwasaki">@masa_iwasakiさん</a></strong>のスライドっす。密度がかなり高かったです！</p>

<h3>選定ポイント</h3>

<p>エラー通知系のサービスの設定ポイントは次の通り。</p>

<pre>
* 利用するプロジェクト数やユーザー数
* ログ保存期間
* 連携したい外部サービスなど
</pre>


<p>NewRelic自体にもエラー通知の機能がありますし、AirbrakeはNewRelicとの連携機能もあるそうです。エラー通知系はサービスの生命線なので1つのサービスに頼りすぎるのではなく、うまく併用することがポイントだそうです。</p>

<p>余談ですが、Airbrakeクローンのオープンソースで「<strong><a href="/blog/errbit-heroku-rails-error-ruby/">Errbit</a></strong>」があります。Herokuの無料範囲内で使えるのと、複数プロジェクトをまとめて対応することができるのでかなりオススメです！</p>

<h2>CI系サービス</h2>

<p>CI系は個人的に使ってみたいと思っていたので、料金ばっかり見てましたw</p>

<p><strong><a href="https://travis-ci.org/">Travis CI</a></strong>($130-)<br/>
<strong><a href="https://circleci.com/">Circle CI</a></strong>($19-)<br/>
<strong><a href="https://www.shippable.com/">Shippable</a></strong>(プライベートリポジトリ5個まで$0-)<br/>
<strong><a href="https://drone.io/">drone.io</a></strong>($25-、Herokuやdotcloudなどへのデプロイにも対応)<br/>
<strong><a href="http://wercker.com/">Wercker</a></strong>(ベータ版なので無料、ビルド時間が25分の制限がある。Chefでboxを作れる)<br/>
<strong><a href="https://www.atlassian.com/ja/software/bamboo">Bamboo</a></strong>(Atlassianのサービス。AWSアカウントが必須。$10-)<br/>
<strong><a href="http://www.cloudbees.com/">CloudBees</a></strong>(JenkinsのSaaS。月100分まで無料で使える)<br/></p>

<h3>所感</h3>

<p>本気のプロジェクトなら、Shippableやdrone.ioが便利そうです。個人的にはWerckerが面白そうなので試してみるつもりっす。</p>

<h2>その他面白そうなサービス・ツール</h2>

<p><strong><a href="http://leanstack.io/">開発に使えるwebサービスまとめ的なサイト</a></strong><br/>
最近バズっているサービス。あとでじっくり見てみようっと。</p>

<p><strong><a href="https://codeclimate.com/">Code Climate</a></strong><br/>
コードクオリティの検査をしてくれるサービス。$99だけどできるだけ早く手を出したいなぁ＾＾</p>

<p><strong><a href="http://www.cloudflare.com/">Cloud Flare</a></strong><br/>
CDN。無料からスタートできて、有料版でも$20くらいで使える。<strong><a href="https://twitter.com/yando">@yandoさん</a></strong>は過去にYahoo砲を受けても80%近くをCloud Flareがさばいてくれたおかげで安定運用ができたそうです。</p>

<p><strong><a href="https://www.bloc.io/ruby-warrior/">RubyWarrior</a></strong><br/>
RPGをやりながら、Rubyを覚えていくことができるサービス。次回のGinza.rbのテーマ最有力候補です。</p>

<p><strong><a href="https://github.com/awilliams/RTanque">awilliams/RTanque</a></strong><br/>
作ったソースを戦わせることができる(?)そうです。詳しくは下の動画を。なにこれ超面白そう！</p>

<iframe width="600" height="380" src="http://morizyun.github.io//www.youtube.com/embed/G7i5X8pI6dk" frameborder="0" allowfullscreen></iframe>


<p><strong><a href="http://artoo.io/">artoo Ruby on Robots</a></strong><br/>
ルンバやArdroneなどのロボットを動かすためのgem。これもRubyパワーを感じさせる素晴らしすぎる！</p>

<p><strong><a href="http://qiita.com/yando/items/7759aad6018854e83fc6">Travis CI, NewRelic, CloudFlare, Chefなどの日本語コミュニティ</a></strong><br/>
Facebookにこういうコミュニティが集約されているのも面白い兆候＾＾</p>

<p><strong><a href="https://gemnasium.com/">Gemnasium</a></strong><br/>
プロジェクト内で使用しているGemの依存関係をチェックして、Gemの新しいバージョンやSecurity Fixを通知してくれるサービス。</p>

<p>{% include custom/google_ads_yoko_naga.html %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「Rによるやさしい統計学」で学ぶ R言語と統計学の基礎徹底解説 2-5章]]></title>
    <link href="http://morizyun.github.io/blog/easy-R-statistics-book-review/"/>
    <updated>2014-03-21T17:40:00+09:00</updated>
    <id>http://morizyun.github.io/blog/easy-R-statistics-book-review</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4274067106/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4274067106&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4274067106&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4274067106" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />R言語と統計学を同時に学ぶことができる『 <strong><a href="http://www.amazon.co.jp/gp/product/4274067106/ref=as_li_qf_sp_asin_il?ie=UTF8&amp;camp=247&amp;creative=1211&amp;creativeASIN=4274067106&amp;linkCode=as2&amp;tag=morizyun00-22">Rによるやさしい統計学</a></strong>』を少しずつ読み進めています。</p>

<p>この本は難しい数学の式などは出てこずに、わかりやすい言葉や例で統計学の基礎的なキーワードとそれに紐づく「<strong>Rの使い方</strong>」に焦点を当てて、説明をしてくれています。個人的には事前に基本的な概念を説明した入門書を一冊やってから、「Rではどうやるか？」を知るにオススメな本だと思います。</p>

<p>まだ前半戦の状態ですが、この中で特に覚えておきたい部分を中心にメモを書いていきます。</p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_yoko_naga.html %}</p>

<h2>2章: 1つの変数の記述統計</h2>

<p>データ解析の出発点となる『1つの変数』の記述方法についての説明。</p>

<h2>レクチャー</h2>

<p>{% codeblock lang:bash %}</p>

<h1>c: データの値を結合させる</h1>

<p>a &lt;- c(1, 2, 5, 10, 15, 5)</p>

<h1>ベクトル/行列の要素の平均 (行平均, 列平均)</h1>

<p>mean(a) #=> 6.333333</p>

<h1>中央値</h1>

<p>median(a) #=> 5</p>

<h1>頻度を整理して得られる分布である「度数分布」の表を作る</h1>

<p>table(a)
a
 1  2  5 10 15
 1  1  2  1  1</p>

<h1>不偏分散</h1>

<h2>確率変数の分布が期待値からどれだけ散らばっているかを示す値。</h2>

<p>var(a)</p>

<h1>標準偏差</h1>

<h2>ある集団についてのデータがどのように分布しているかを表す値。</h2>

<p>sd(a)
sqrt(mean((a-mean(a))<sup>2))</sup></p>

<h2>標本分散の関数を自作</h2>

<p>varp &lt;- function(x) {</p>

<pre><code>標本分散 &lt;- var(x)*(length(x) - 1)/length(x)
標本分散
</code></pre>

<p>}
varp(a)</p>

<h2>別ファイルの関数の読み込み</h2>

<p>source("varp.R")
{% endcodeblock %}</p>

<h3>例題やってみた</h3>

<p>{% codeblock lang:bash %}
a &lt;- c(60,100,50,40,50,230,120,240,200,30)
b &lt;- c(50,60,40,50,100,80,30,20,100,120)</p>

<h1>1)</h1>

<p>hist(a)
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3830/13230876803_318d1aa460.jpg" width="413" height="439" alt="Rplot03"></p>

<p>{% codeblock lang:bash %}
hist(b)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7136/13231086804_1af98a9796.jpg" width="413" height="439" alt="Rplot"></p>

<p>{% codeblock lang:bash %}</p>

<h1>2)</h1>

<p>mean(a)
sqrt(mean((a-mean(a))<sup>2))</sup> # 標準偏差
mean(b)
sqrt(mean((b-mean(b))<sup>2))</sup> # 標準偏差</p>

<h1>3)</h1>

<p>(a - mean(a))/sqrt(mean((a-mean(a))<sup>2))</sup>
(b - mean(b))/sqrt(mean((b-mean(b))<sup>2))</sup>
{% endcodeblock %}</p>

<h2>3章: 2つの変数の記述統計</h2>

<p>『2つの変数における量的変数における関係、質的変数における関係』についての説明です。ちなみに量的変数と質的変数の説明は次の通り。</p>

<h2>レクチャー</h2>

<pre>
* 量的変数: データ間に優劣や大小が比較できる。『テストの点数』のような数値データ
=> 比率データ: 絶対的なゼロ点を有する(質量、長さ、年齢、時間、金額など)
=> 間隔データ: 数値(メモリ)の間隔が等しい(気温、知能指数など)

* 質的変数: データ間に優劣がなく、大小関係が比較できない。「男・女」や「好き・嫌い」など
=> 順序データ: 大小関係(順序)のみ意味を持つ(満足度、選好度、硬度など)
=> 名義データ: 内容を区別するために便宜的に割りあてる(電話番号、性別など)
</pre>


<p>さらに、相関(correlation)と回帰(regression)の説明は次の通り。</p>

<pre>
相関 correlation #=> xとyの間に区別をもうけず対等に見る見方や方法
回帰 regression  #=> ｘからyを見る(yからxを見る)方法
</pre>


<p>{% codeblock lang:bash %}</p>

<h1>分散図(グラフの表示)</h1>

<p>a &lt;- c(1,2,3,4,5)
b &lt;- a * 2
plot(a, b)
{% endcodeblock %}</p>

<p><img src="http://farm3.staticflickr.com/2827/13231117083_681e5133e3.jpg" width="413" height="439" alt="Rplot01"></p>

<p>{% codeblock lang:bash %}</p>

<h1>共分散(数値)</h1>

<h2>2つのデータが、どれだけ関連性・連動性があるかを示す係数</h2>

<p>cov(a, b) #=> 5</p>

<h1>=> sum((a-mean(a))*(b-mean(b)))/length(a) と同義</h1>

<h1>=> mean((a-mean(a))*(b-mean(b))) と同義</h1>

<h1>相関係数</h1>

<h2>2つのデータが、どれだけ関連性があるのかを示す係数</h2>

<p>cor(a, b) #=> 1</p>

<h1>=> cov(a, b)/(sd(a)*sd(b))と同義</h1>

<p>{% endcodeblock %}</p>

<p>相関係数は<code>-1 &lt;= r &lt;= 1</code>で次のように関係性がわかります。</p>

<pre>
-0.2 <= r <=  0.2                 #=> ほとんど相関なし
-0.4 <= r <  -0.2, 0.2 < r <= 0.4 #=> 弱い相関あり
-0.7 <= r <   0.4, 0.4 < r <= 0.7 #=> 中程度の相関あり
-1.0 <= r <  -0.7, 0.7 < r <= 1.0 #=> 強い相関あり 
</pre>


<p>{% codeblock lang:bash %}</p>

<h1>クロス集計表</h1>

<h2>質的変数の関係について表す</h2>

<p>math &lt;- c('like', 'hate', 'like', 'like', 'hate')
statistics &lt;- c('hate', 'like', 'hate', 'like', 'like')
table(math, statistics)</p>

<pre><code>  statistics
</code></pre>

<p>math   hate like
  hate    0    2
  like    2    1</p>

<h1>ファイ係数</h1>

<h2>1と0の２つの値からなる変数（二値変数）に対して計算される相関係数</h2>

<h2>math(数学)、statistics(統計)の好き=>1、嫌い=>0に変換して考える</h2>

<p>math_zero <- ifelse(math=='like', 1, 0)
math_zero #=> 1 0 1 1 0
statistics_zero <- ifelse(statistics=='like', 1, 0)
statistics_zero #=> 0 1 0 1 1
cor(math_zero, statistics_zero)
{% endcodeblock %}</p>

<h3>例題やってみた</h3>

<p>{% codeblock lang:bash %}</p>

<h1>1) 散布図</h1>

<p>hour &lt;- c(1,3,10,12,6,3,8,4,1,5)
score &lt;- c(20,40,100,80,50,50,70,50,10,60)
plot(hour, score)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7004/13276651894_85af764fd4.jpg" width="413" height="439" alt="Rplot02"></p>

<p>{% codeblock lang:bash %}</p>

<h1>2) 相関係数</h1>

<p>cor(hour, score)</p>

<h1>=>  0.9092974 なので高い相関がある</h1>

<h1>3) クロス統計</h1>

<p>youwa &lt;- c('洋食', '和食', '和食', '洋食', '和食', '洋食', '洋食', '和食', '洋食', '洋食', '和食', '洋食', '和食', '洋食', '和食', '和食', '洋食', '洋食', '和食', '和食')
amakara &lt;- c('甘党', '辛党', '甘党', '甘党', '辛党', '辛党', '辛党', '辛党', '甘党', '甘党', '甘党', '甘党', '辛党', '辛党', '甘党', '辛党', '辛党', '甘党', '辛党', '辛党')
table(youwa, amakara)</p>

<h1>amakara</h1>

<h1>youwa  甘党 辛党</h1>

<h1>洋食    6    4</h1>

<h1>和食    3    7</h1>

<h1>4) ファイ係数</h1>

<p>youwa_zo &lt;- ifelse(youwa=='洋食', 1, 0)
amaka_zo &lt;- ifelse(amakara=='甘党', 1, 0)
cor(youwa_zo, amaka_zo)</p>

<h1>=> 0.3015113</h1>

<p>{% endcodeblock %}</p>

<h3>Rの便利メソッド</h3>

<p>{% codeblock lang:bash %}</p>

<h1>Rのメソッド</h1>

<h1>ifelse(条件, 真の場合, 偽の場合) 場合分けをする</h1>

<p>hour <- c(1,3,10,12,6,3,8,4,1,5)
hour_zo <- ifelse(hour >= 5, "up", "down")</p>

<h1>=> "down" "down" "up"   "up"   "up"   "down" "up"   "down" "down" "up"</h1>

<p>{% endcodeblock %}</p>

<h2>第4章: 母集団と標本</h2>

<p>大きな集団(母集団)から取り出した少数のデータ(標本)を使って『もとの集団の性質について推測する』ための推測統計の基本的な理論についての学習。</p>

<p>母集団と標本の説明はこちら。</p>

<pre>
母集団: データ全体を指す。国民全体、工業製品全体など
標本: 母集団から取り出したデータの一部
</pre>


<p>推測統計の分類についてはこちら。</p>

<pre>
推定: 具体的な数値を用いて『母数の値は◯◯くらいだろう』という結論を導くこと
点推定: 1つの値で空いての結果を表す。日本の中学生の数学平均点が「60点」と推定
区間推定: ある程度の幅を持った区間で結果を表す。数学平均が『50-70点』と推定
</pre>


<p>{% codeblock lang:bash %}</p>

<h2>サイコロを6回ふった場合、サイコロのすべての目が1回ずつ出るのは稀</h2>

<p>dice6 &lt;- ceiling(runif(n=6, min=0, max=6))
table(dice6)</p>

<h1>dice6</h1>

<h1>2 3 4</h1>

<h1>1 2 3</h1>

<h2>サイコロを600万回ふった場合、1/6に限りなく近づく</h2>

<p>dice6m &lt;- ceiling(runif(n=6000000, min=0, max=6))
table(dice6m)</p>

<h1>dice6m</h1>

<h1>1       2       3       4       5       6</h1>

<h1>999612 1000211  998870  999613 1000366 1001328</h1>

<p>table(dice6m)/6000000</p>

<h1>dice6m</h1>

<h1>1         2         3         4         5         6</h1>

<h1>0.1666020 0.1667018 0.1664783 0.1666022 0.1667277 0.1668880</h1>

<h1>男性が2/3、女性が1/3を表す棒グラフ</h1>

<p>barplot(c(2/3, 1/3), names.arg=c('man', 'woman'))
{% endcodeblock %}</p>

<p><img src="https://s3.yimg.com/so/7310/12550357034_2b0cda0363_z.jpg" width="618" height="640" alt="スクリーンショット 2014-02-16 8.12.38"></p>

<p>{% codeblock lang:bash %}</p>

<h1>正規分布</h1>

<h2>ある1つの数値を目標とした作業で偶然生じる偶然的な誤差の分布</h2>

<p>curve(dnorm(x, mean=0, sd=1), from=-4, to=4)
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3771/13276801433_f5a02457af.jpg" width="413" height="439" alt="Rplot01"></p>

<p>{% codeblock lang:bash %}</p>

<h1>母集団が正規分布であるような集団から無作為標本を抽出</h1>

<h2>ランダムに5つの標本を取得</h2>

<p>hyohon &lt;- rnorm(n=5, mean=50, sd=10)</p>

<h1>=> 62.69624 40.50412 51.14337 58.89571 36.13923</h1>

<h2>ヒストグラムを作成</h2>

<p>hist(hyohon)
{% endcodeblock %}</p>

<p><img src="http://farm3.staticflickr.com/2825/13277023004_286acdaa89.jpg" width="413" height="439" alt="Rplot03"></p>

<p>上のように標本数が少ないと、正規分布かどうかはわかりません。ただし、サンプルサイズをある程度大きくするとヒストグラムは正規分布に近いものになっていきます。</p>

<p>{% codeblock lang:bash %}</p>

<h1>標本数を増やした場合のヒストグラム => 正規分布に近づく</h1>

<p>hyohon <- rnorm(n=10000, mean=50, sd=10) #=> 正規分布に近いヒストグラムになる
hist(hyohon)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7393/13276723015_ee600af056.jpg" width="413" height="439" alt="Rplot04"></p>

<h3>練習問題をやってみた</h3>

<p>{% codeblock lang:bash %}</p>

<h1>1) 経験的な標本分布</h1>

<p>ave &lt;- numeric(length=5000)
for(i in 1:5000) {
  hyohon &lt;- rnorm(n=20, mean=50, sd=10)
  ave[i] &lt;- mean(hyohon)
}
hist(ave, freq=FALSE)
curve(dnorm(x, mean=50, sd=sqrt(100/20)), add=TRUE)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7278/13276903325_6fb58ee4df.jpg" width="413" height="439" alt="Rplot05"></p>

<p>{% codeblock lang:bash %}</p>

<h1>2) 理論的な標本分布</h1>

<p>curve(dnorm(x, sd=sqrt(1/25)), -3, 3)
curve(dnorm(x, sd=sqrt(1/16)), -3, 3, add=TRUE)
curve(dnorm(x, sd=sqrt(1/9)),  -3, 3, add=TRUE)
curve(dnorm(x, sd=sqrt(1/4)),  -3, 3, add=TRUE)
curve(dnorm(x, sd=sqrt(1/1)),  -3, 3, add=TRUE)
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3696/13276920725_45b3471037.jpg" width="413" height="439" alt="Rplot06"></p>

<h3>この章で出てくるRの関数</h3>

<p>{% codeblock lang:bash %}</p>

<h1>小数点以下の切り上げ</h1>

<p>ceiling(1.5) #=> 2</p>

<h1>一様分布に従う乱数を発生させる</h1>

<p>runif(n=4, min=6, max=12)
[1]  8.430616 11.064664  9.835286 10.971259</p>

<h1>決まったパターンの乱数を発生させる</h1>

<p>set.seed(1) #=> 乱数の種を1に設定するこれを指定後は常に同じ乱数が発生する</p>

<h1>棒グラフを作成</h1>

<p>barplot(c(2/3, 1/3), names.arg=c("man", "woman"))</p>

<h1>関数の曲線を描く</h1>

<p>curve(dnorm(x), from=-3, to=3)</p>

<h1>正規分布の確率密度変数</h1>

<p>dnorm(0, mean=0, sd=1)</p>

<h1>正規分布に従う乱数を発生</h1>

<p>rnorm(n=5, mean=50, sd=10)</p>

<h1>数値を格納する領域の確保</h1>

<p>a &lt;- numeric(length=10000)</p>

<h1>連続データを作る</h1>

<h2>0から20までの整数の連続データをつくる</h2>

<p>1:20
[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20</p>

<h1>正規分布の確率密度関数</h1>

<p>dnorm(0, mean=0, sd=1) #=> 0.3989423</p>

<h1>棒グラフを作成</h1>

<p>barplot(c(3/4, 1/4), names.arg=c('man', 'woman'))</p>

<h1>関数の曲線を描く => curve()</h1>

<h2>正規分布のグラフが出力する場合</h2>

<p>curve(dnorm(x), from=-3, to=3)
{% endcodeblock %}</p>

<h2>第5章: 統計的仮説検定</h2>

<h3>検定の手順</h3>

<pre>
1) 母集団に関する帰無仮説と対立仮説を設定する
2) 検定統計量を選ぶ
3) 有意水準αの値を決める
4) データから検定統計量の実現値を求める
5) 検定統計量の実現値が棄却域に入る => 対立仮説を採用する
</pre>


<h3>検定のキーワード</h3>

<pre>
帰無仮説　　　#=>「差がない」、「効果が無い」という仮説
対立仮説　　　#=> 帰無仮説に反して「差がある」という仮説
検定統計量　　#=> 検定に用いられる標本統計量のこと
有意水準　　　#=> 帰無仮説を棄却し対立仮説を採択する基準
棄却域     　#=> 非常に生じにくい検定統計量の値の範囲 
ｐ値　　　　　#=> 帰無仮説が正しい仮説で、標本から計算した検定統計量を実現できる確率
第１種の誤り　#=> 「帰無仮説が真の時にこれを棄却する」誤り
第２種の誤り　#=> 「帰無仮説が偽の時にこれを採択する」誤り
検定力　　　　#=> 間違っている仮説を正しく棄却できる確率のこと(100% - 第２種の誤りの確率)
</pre>


<h3>標準正規分布を用いた検定の例題をやってみた</h3>

<pre>
# 標準正規分布を用いた検定の検定統計量 :
Z = mean(x) - 標準正規分布の平均/sqrt(標準正規分布の分散/標本数)
</pre>


<p>{% codeblock lang:bash %}
sinri <- c(13, 14, 7, 12, 10, 6, 8, 15, 4, 14, 9, 6, 10, 12, 5, 12, 8, 8, 12, 15)
Z <- (mean(sinri) - 12)/sqrt(10/length(sinri))
qnorm(0.025, lower.tail=FALSE)
qnorm(0.025) #=> -1.959964
qnorm(0.975) #=> 1.959964
Z &lt; qnorm(0.025) #=> TRUEなので帰無仮説を棄却</p>

<h1>pnorm関数からp値を求める</h1>

<p>2*pnorm(abs(Z), lower.tail=FALSE)</p>

<h1>=> 0.004677735 &lt; 有意水準0.05となるので帰無仮説を棄却</h1>

<p>{% endcodeblock %}</p>

<h3>t分布を用いた検定の例題をやってみた</h3>

<p>先述の「標準正規分布を用いた検定」は母集団の分散が計算に必須です。そこで、未知の母集団の分散の代わりに、自由度(データの標本数 - 1)のt分布を元に計算する検定があります。</p>

<pre>
# t分布を用いた検定の検定統計量 :
t = (mean(x) - 母集団の平均)/sqrt(t分布の分散/標本数)
</pre>


<p>{% codeblock lang:bash %}</p>

<h1>5.4 例題</h1>

<h1>帰無仮説「心理テスト(sinri)の平均は12である」のt検定</h1>

<p>sinri <- c(13, 14, 7, 12, 10, 6, 8, 15, 4, 14, 9, 6, 10, 12, 5, 12, 8, 8, 12, 15)
t <- (mean(sinri) -12)/sqrt(var(sinri)/length(sinri)) #=> 2.616648</p>

<h1>qt(p, df)で自由度dfの場合の下側確率pとなるt値が求まる</h1>

<p>qt(0.025, 19) #=> -2.093024
qt(0.975, 19) #=> 2.093024</p>

<p>t &lt; qt(0.025, 19)</p>

<h1>=> TRUEなので帰無仮説を棄却</h1>

<h1>関数t.testによる検定</h1>

<p>t.test(sinri, mu=12)</p>

<h1>One Sample t-test</h1>

<h1>data:  sinri</h1>

<h1>t = -2.6166, df = 19, p-value = 0.01697</h1>

<h1>alternative hypothesis: true mean is not equal to 12</h1>

<h1>95 percent confidence interval:</h1>

<h1>8.400225 11.599775</h1>

<h1>sample estimates:</h1>

<h1>mean of x</h1>

<h1>10</h1>

<p>{% endcodeblock %}</p>

<h3>無相関検定の例題をやってみた</h3>

<p>母集団において相関が0であることを検定するための手法。</p>

<pre>
# t分布を用いた検定の検定統計量 :
t = (標本相関 * sqrt(標本数 - 2))/sqrt(1 - 標本相関)
</pre>


<p>{% codeblock lang:bash %}</p>

<h2>帰無仮説 => toukei1とtoukei2の相関係数は0である</h2>

<p>toukei1 &lt;- c(6,10,6,10,5,3,5,9,3,3,11,6,11,9,7,5,8,7,7,9)
toukei2 &lt;- c(10,13,8,15,8,6,9,10,7,3,18,14,18,11,12,5,7,12,7,7)
t &lt;- (cor(toukei1, toukei2)*sqrt(20-2))/sqrt(1-cor(toukei1,toukei2)<sup>2)</sup></p>

<p>qt(0.025, 18) #=> -2.100922
qt(0.975, 18) #=>  2.100922
qt(0.975, 18) &lt; t #=> TRUEとなるので帰無仮説は棄却 => 相関はある</p>

<h1>無相関検定を実行する関数cor.text</h1>

<p>cor.test(toukei1, toukei2)</p>

<h1>Pearson's product-moment correlation</h1>

<h1></h1>

<h1>data:  toukei1 and toukei2</h1>

<h1>t = 4.8057, df = 18, p-value = 0.0001416</h1>

<h1>alternative hypothesis: true correlation is not equal to 0</h1>

<h1>95 percent confidence interval:</h1>

<h1>0.4596086 0.8952048</h1>

<h1>sample estimates:</h1>

<h1>cor</h1>

<h1>0.749659</h1>

<p>{% endcodeblock %}</p>

<h3>独立性の検定(χ2乗検定)の例題をやってみた</h3>

<p>{% codeblock lang:bash %}</p>

<h2>帰無仮説 => 「数学の好き嫌い」と「統計の好き嫌い」には差がない</h2>

<p>math &lt;- c("hate", "hate", "like", "like", "hate", "hate", "hate", "hate", "hate", "like", "like", "hate", "like", "hate", "hate", "like", "hate", "hate", "hate", "hate")
staitstics &lt;- c("like", "like", "like", "like", "hate", "hate", "hate", "hate", "hate", "hate", "like", "like", "like", "hate", "like", "hate", "hate", "hate", "hate", "hate")
table(math, staitstics)</p>

<h1>staitstics</h1>

<h1>math   hate like</h1>

<h1>hate   10    4</h1>

<h1>like    2    4</h1>

<p>exp_hate_hate = 12 * 14/20
exp_like_hate = 12 *  6/20
exp_hate_like =  8 * 14/20
exp_like_like =  8 *  6/20
exp_dosu &lt;- c(exp_hate_hate, exp_like_hate, exp_hate_like, exp_like_like)</p>

<h1>=> 8.4 3.6 5.6 2.4</h1>

<p>kansoku_dosu &lt;- c(10,2,4,4)
kai_nizyo &lt;- sum((exp_dosu -kansoku_dosu)<sup>2/exp_dosu)</sup></p>

<h1>=> 2.539683</h1>

<h1>χ 2乗分布に従う棄却域を求める => qchisq関数</h1>

<p>qchisq(0.95, 1) &lt; kai_nizyo #=> FALSE</p>

<h2>結論: 「数学の好き嫌い」と「統計の好き嫌い」には優位な差はない</h2>

<h2>χ2乗検定を行う関数 correct=FALSEは連続性の補正を行わないオプション</h2>

<p>chisq.test(table(math, staitstics), correct=FALSE)</p>

<h1>Pearson's Chi-squared test</h1>

<h1></h1>

<h1>data:  table(math, staitstics)</h1>

<h1>X-squared = 2.5397, df = 1, p-value = 0.111</h1>

<h1>=> pが0.11で0.05よりも大きな値なので帰無仮説が採択される</h1>

<p>{% endcodeblock %}</p>

<h3>練習問題をやってみた</h3>

<p>{% codeblock lang:bash %}</p>

<h2>1)</h2>

<h1>帰無仮説 : 平均170cmの正規分布に従うサンプルである</h1>

<p>height &lt;- c(165, 150, 170, 168, 159, 170, 167, 178, 155, 159, 161, 162, 166, 171, 155, 160, 168, 172, 155, 167)
t.test(height, mu=170)</p>

<h1>One Sample t-test</h1>

<h1></h1>

<h1>data:  height</h1>

<h1>t = -3.8503, df = 19, p-value = 0.001079</h1>

<h1>alternative hypothesis: true mean is not equal to 170</h1>

<h1>95 percent confidence interval:</h1>

<h1>160.584 167.216</h1>

<h1>sample estimates:</h1>

<h1>mean of x</h1>

<h1>163.9</h1>

<h1>=> p = 0.00107で 0.05より小さいので帰無仮説は棄却</h1>

<h2>2)</h2>

<h2>帰無仮説 : 勉強時間とテスト結果には相関性がない</h2>

<p>hour &lt;- c(1,3,10,12,6,3,8,4,1,5)
score &lt;- c(20,40,100,80,50,50,70,50,10,60)
cor.test(hour, score)</p>

<h1>Pearson's product-moment correlation</h1>

<h1></h1>

<h1>data:  hour and score</h1>

<h1>t = 6.1802, df = 8, p-value = 0.0002651</h1>

<h1>alternative hypothesis: true correlation is not equal to 0</h1>

<h1>95 percent confidence interval:</h1>

<h1>0.6542283 0.9786369</h1>

<h1>sample estimates:</h1>

<h1>cor</h1>

<h1>0.9092974</h1>

<h2>3)</h2>

<h2>スピアマンの順位相関係数</h2>

<h2>http://goo.gl/sFitJ</h2>

<p>cor.test(hour, score, method="spearman")</p>

<h1>Spearman's rank correlation rho</h1>

<h1></h1>

<h1>data:  hour and score</h1>

<h1>S = 10.1822, p-value = 5.887e-05</h1>

<h1>alternative hypothesis: true rho is not equal to 0</h1>

<h1>sample estimates:</h1>

<h1>rho</h1>

<h1>0.9382895</h1>

<h2>ケンドールの順位相関係数</h2>

<h2>http://goo.gl/2HRaa</h2>

<p>cor.test(hour, score, method="kendall")</p>

<h1>Kendall's rank correlation tau</h1>

<h1></h1>

<h1>data:  hour and score</h1>

<h1>z = 3.2937, p-value = 0.0009889</h1>

<h1>alternative hypothesis: true tau is not equal to 0</h1>

<h1>sample estimates:</h1>

<h1>tau</h1>

<h1>0.8471174</h1>

<h2>4) χ2乗検定</h2>

<p>youwa &lt;- c('洋食', '和食', '和食', '洋食', '和食', '洋食', '洋食', '和食', '洋食', '洋食', '和食', '洋食', '和食', '洋食', '和食', '和食', '洋食', '洋食', '和食', '和食')
amakara &lt;- c('甘党', '辛党', '甘党', '甘党', '辛党', '辛党', '辛党', '辛党', '甘党', '甘党', '甘党', '甘党', '辛党', '辛党', '甘党', '辛党', '辛党', '甘党', '辛党', '辛党')
chisq.test(youwa, amakara)</p>

<h1>Pearson's Chi-squared test with Yates' continuity correction</h1>

<h1></h1>

<h1>data:  youwa and amakara</h1>

<h1>X-squared = 0.8081, df = 1, p-value = 0.3687</h1>

<h2>5-1) 無相関検定</h2>

<p>lang &lt;- c(60, 40, 30, 70, 55)
soci &lt;- c(80, 25, 35, 70, 50)
cor.test(lang, soci)</p>

<h1>Pearson's product-moment correlation</h1>

<h1></h1>

<h1>data:  lang and soci</h1>

<h1>t = 2.6952, df = 3, p-value = 0.07408</h1>

<h1>alternative hypothesis: true correlation is not equal to 0</h1>

<h1>95 percent confidence interval:</h1>

<h1>-0.1590624  0.9892731</h1>

<h1>sample estimates:</h1>

<h1>cor</h1>

<h1>0.841263</h1>

<h1>=> p値が有意水準(0.05)より高いので2つの結果に相関はない</h1>

<h2>5-2) 無相関検定 : 同じサンプルを2回使う</h2>

<p>lang2 &lt;- rep(lang, 2)
soci2 &lt;- rep(soci, 2)
cor.test(lang2, soci2)</p>

<h1>Pearson's product-moment correlation</h1>

<h1></h1>

<h1>data:  lang2 and soci2</h1>

<h1>t = 4.4013, df = 8, p-value = 0.002283</h1>

<h1>alternative hypothesis: true correlation is not equal to 0</h1>

<h1>95 percent confidence interval:</h1>

<h1>0.4499858 0.9615658</h1>

<h1>sample estimates:</h1>

<h1>cor</h1>

<h1>0.841263</h1>

<h1>=> p値が有意水準(0.05)より低いので2つの結果に相関はある</h1>

<h1>=> (5-1)と(5-2)は相関係数(cor)は一致するがp値が大きく異る</h1>

<p>{% endcodeblock %}</p>

<h3>便利なRの関数</h3>

<p>{% codeblock lang:bash %}</p>

<h1>標準正規分布の下側確率に対応する値</h1>

<p>qnorm(0.025) #=> -1.959964</p>

<h1>標準正規分布の下側確率</h1>

<p>pnorm(-1.959964) #=> 0.025</p>

<h1>t分布で下側確率に対応する値</h1>

<p>qt(0.025, 19) #=> -2.093024</p>

<h1>t分布の下側確率</h1>

<p>pt(-2.093024, 19) #=> 0.025</p>

<h1>カイ二乗分布の下側確率に対応する値</h1>

<p>qchisq(0.95, 1) #=> 3.841459</p>

<h1>カイ二乗分布の下側確率に対応する値</h1>

<p>pchisq(3.841459, 1) #=> 0.95</p>

<h1>t分布の確率</h1>

<p>dt(1, 19) #=> 0.2357406</p>

<h1>カイ二乗分布の確率密度分布</h1>

<p>dchisq(3.841,1) #=> 0.02982808
{% endcodeblock %}</p>

<p>{% include custom/google_ads_yoko_naga.html %}</p>
]]></content>
  </entry>
  
</feed>
