<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: R | 酒と泪とRubyとRailsと]]></title>
  <link href="http://morizyun.github.io/blog/categories/r/atom.xml" rel="self"/>
  <link href="http://morizyun.github.io/"/>
  <updated>2016-01-12T23:54:18+09:00</updated>
  <id>http://morizyun.github.io/</id>
  <author>
    <name><![CDATA[morizyun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[【書評: データサイエンティスト養成読本】で学ぶデータサイエンティストの生態系]]></title>
    <link href="http://morizyun.github.io/blog/data-scientist-cultivation-book-review/"/>
    <updated>2014-03-31T09:40:00+09:00</updated>
    <id>http://morizyun.github.io/blog/data-scientist-cultivation-book-review</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4774158968/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4774158968&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4774158968&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4774158968" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />統計解析の勉強をしていく中で、どうやったら実際の業務の中に活かしていけるかを模索していて、この本にいきあたりました。人々が生み出すデータがますます増加していく中で、そのデータをビジネスにどう活用していくかをエンジニアの視点で考えられる素晴らしい本だと思います。</p>

<p>今回はこの本の中で特に参考になった点を中心にピックアップしていきます。</p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_square.html %}</p>

<h2>本書のサンプルデータ</h2>

<p><strong><a href="http://gihyo.jp/book/2013/978-4-7741-5896-9/support">サポートページ：データサイエンティスト養成読本 ［ビッグデータ時代のビジネスを支えるデータ分析力が身につく！］</a></strong><br/>
インターネットの力、マジですごい！サンプルデータはほんとうに有難い！</p>

<h2>データサイエンティストの仕事術</h2>

<h3>Cross Industry Standard Process for Data Mining(CRISP-DM)</h3>

<p>データマイニングプロジェクトにおける標準的な6つの手順。</p>

<pre>
1) Business Understanging
ビジネスの全体像の理解(ヒアリング)と課題設定

2) Data Understanding
データ収集と理解に努めて、仮説構築の下地を作る

3) Data Preparation
数理モデルの作成のために、データマート(データウェアハウス)を構築。
欠損値や外れ値を除去していく。

4) Modeling
仮説にもどついて、数理モデルを構築する

5) Evaluation
数理モデルの評価。ビジネスリスクの評価

6) Deployment
数理モデルに基づく、ビジネス施策の実施
</pre>


<h3>必要なスキルセット</h3>

<pre>
# ハードスキル
1) Data Understanding 〜 Data Preparation
RDBMS(SQL)、Hadoop(HDFS)、MapReduce、Hiveなど

2) Modeling 〜 Devaluation
統計解析、機械学習の知識、R, Pythonなど

# ソフトスキル
1) Business Understanding 〜 Data Understanding
ビジネスにおける業界、業務の知識。質問力、理解力が必要

2) Deployment
分析結果の伝達力、説得力、プロジェクトの推進力が必要
</pre>


<p>根本としては好奇心。「<strong>問題を核心まで掘り下げて、そこで見つけた疑問を明確で検証可能な一連の仮説に落としこみたいという欲求</strong>」が必要。</p>

<h3>データサイエンスよりも大切なこと</h3>

<pre>
1) 統計的な正しさよりもビジネスの成功
2) 組織の構造や実務の改善効果を重視する
3) 意味のある分析のためには泥臭いクリーニングが必須
4) データは適切な保持が大切
</pre>


<h3>パラメトリックとノンパラメトリック</h3>

<p><strong><a href="https://twitter.com/TJO_datasci">@TJO_datasci</a></strong>さんからご指摘頂きました。パラメトリックとノンパラメトリックの違いの端的なまとめは次の通りです。</p>

<blockquote>
* パラメトリック検定：正規分布に限らず厳密な分布の形の仮定を必要とする<br/>
* ノンパラメトリック検定：分布の形の仮定が不要である
</blockquote>


<p>引用元: <a href="http://tjo.hatenablog.com/entry/2013/11/19/093218">「パラメトリック検定」と「ノンパラメトリック検定」の違いについて出典を明示して書いておく</a></p>

<p>僕自身幾つか文献を見ましたが、まだまだ理解できていません。引き続き勉強して理解できたら、また自分の言葉で書き直したいと思います！</p>

<h2>データ分析実践入門</h2>

<h3>事前準備: RStudio</h3>

<p>この章では主にRを使います。RStudioについては次の章で説明します。</p>

<h3>ロジスティック回帰</h3>

<p>Rにデフォルトで入っている、タイタニック号の乗客の生存情報(Titanic)にロジスティック回帰モデルを使う。乗客の各属性から、生存の要因を探す。</p>

<p>{% codeblock lang:bash %}</p>

<h2>データ整形</h2>

<p>z &lt;- data.frame(Titanic)
titanic.data &lt;- data.frame(</p>

<pre><code>Class=rep(z$Class, z$Freq),
Sex=rep(z$Sex, z$Freq),
Age=rep(z$Age, z$Freq),
Survived=rep(z$Survived, z$Freq)
</code></pre>

<p>  )
head(titanic.data)</p>

<h1>Class  Sex   Age Survived</h1>

<h1>1   3rd Male Child       No</h1>

<h1>2   3rd Male Child       No</h1>

<h1>3   3rd Male Child       No</h1>

<h1>4   3rd Male Child       No</h1>

<h1>5   3rd Male Child       No</h1>

<h1>6   3rd Male Child       No</h1>

<h2>モデル構築</h2>

<p>titanic.logit &lt;- glm(Survived~., data=titanic.data, family=binomial)
summary(titanic.logit)</p>

<h1>Call:</h1>

<h1>glm(formula = Survived ~ ., family = binomial, data = titanic.data)</h1>

<h1></h1>

<h1>Deviance Residuals:</h1>

<h1>Min       1Q   Median       3Q      Max</h1>

<h1>-2.0812  -0.7149  -0.6656   0.6858   2.1278</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error z value Pr(>|z|)</h1>

<h1>(Intercept)   0.6853     0.2730   2.510   0.0121 *</h1>

<h1>Class2nd     -1.0181     0.1960  -5.194 2.05e-07 ***</h1>

<h1>Class3rd     -1.7778     0.1716 -10.362  &lt; 2e-16 ***</h1>

<h1>ClassCrew    -0.8577     0.1573  -5.451 5.00e-08 ***</h1>

<h1>SexFemale     2.4201     0.1404  17.236  &lt; 2e-16 ***</h1>

<h1>AgeAdult     -1.0615     0.2440  -4.350 1.36e-05 ***</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>(Dispersion parameter for binomial family taken to be 1)</h1>

<h1></h1>

<h1>Null deviance: 2769.5  on 2200  degrees of freedom</h1>

<h1>Residual deviance: 2210.1  on 2195  degrees of freedom</h1>

<h1>AIC: 2222.1</h1>

<h1></h1>

<h1>Number of Fisher Scoring iterations: 4</h1>

<p>{% endcodeblock %}</p>

<p>{% include custom/google_ads_square.html %}</p>

<h3>オッズ比の計算</h3>

<p>{% codeblock lang:bash %}
install.packages('epicalc')
library('epicalc')
logistic.display(titanic.logit, simplified=T)</p>

<h1>OR lower95ci  upper95ci     Pr(>|Z|)</h1>

<h1>Class2nd   0.3612825 0.2460447  0.5304933 2.053331e-07</h1>

<h1>Class3rd   0.1690159 0.1207510  0.2365727 3.691891e-25</h1>

<h1>ClassCrew  0.4241466 0.3115940  0.5773549 5.004592e-08</h1>

<h1>SexFemale 11.2465380 8.5408719 14.8093331 1.431830e-66</h1>

<h1>AgeAdult   0.3459219 0.2144193  0.5580746 1.360490e-05</h1>

<h1>=> 例えば、女性のほうが11.2465380倍生存確率が高いということがわかる</h1>

<p>{% endcodeblock %}</p>

<h3>決定木モデル</h3>

<p>{% codeblock lang:bash %}
library(rpart)
install.packages('partykit')
library(partykit)</p>

<p>titanic.rp &lt;- rpart(Survived~., data=titanic.data)
plot(as.party(titanic.rp), tp_args=T)
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3720/13404231644_57b602b50a.jpg" width="480" height="500" alt="スクリーンショット 2014-03-25 22.45.03"></p>

<h3>主成分分析</h3>

<p>{% codeblock lang:bash %}
state.pca &lt;- prcomp(state.x77[,1:6], scale=T)
biplot(state.pca)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7456/13404045033_bd64bcfd5f.jpg" width="487" height="500" alt="スクリーンショット 2014-03-25 22.48.23"></p>

<h3>非階層クラスタリング: k-means</h3>

<p>k-meansの手順。</p>

<pre>
1) k個のクラスタの中心の初期値を決める
2) 各データをk個クラスタ中心との距離を求め、最も近いクラスタに分類
3) 形成されたクラスタの中心を求める
4) クラスタの中心が変化しない時点まで(2), (3)を繰り返す
</pre>


<p>サンプルソースはこちら。</p>

<p>{% codeblock lang:bash %}</p>

<h1>k-meansの実行(クラスタ数を3つに設定)</h1>

<p>state.km &lt;- kmeans(scale(state.x77[,1:6]),3)</p>

<h1>主成分分析の結果にクラスタ情報を付与</h1>

<p>state.pca.df &lt;- data.frame(state.pca$x)
state.pca.df$name &lt;- rownames(state.pca.df)
state.pca.df$cluster &lt;- as.factor(state.km$cluster)</p>

<h1>描画</h1>

<p>ggplot(state.pca.df, aes(x=PC1, y=PC2, label=name, col=cluster)) + geom_text() + theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7455/13404137535_783058ded9.jpg" width="456" height="500" alt="スクリーンショット 2014-03-25 23.01.10"></p>

<h3>レーダーチャート</h3>

<p>{% codeblock lang:bash %}
install.packages('fmsb')
library('fmsb')
df &lt;- as.data.frame(scale(state.km$centers))
dfmax &lt;- apply(df, 2, max) + 1
dfmin &lt;- apply(df, 2, min) - 1
df &lt;- rbind(dfmax, dfmin, df)</p>

<h1>描画</h1>

<p>radarchart(df, seg=5, plty=1, pcol=rainbow(3))
legend("topright", legend=1:3, col=rainbow(3), lty=1)</p>

<h1>クラスタ1 => 平均寿命、高卒率、収入が多い</h1>

<h1>クラスタ2 => 殺人件数と非識字率が高い</h1>

<h1>クラスタ3 => 人口が多い</h1>

<p>{% endcodeblock %}</p>

<p><img src="https://farm3.staticflickr.com/2859/13414033393_ec46db015e.jpg" width="500" height="475" alt="スクリーンショット 2014-03-26 7.44.11"></p>

<h2>RStudioのすすめ</h2>

<p>以下オススメリンク。</p>

<p><strong><a href="http://www.rstudio.com/ide/download/desktop">デスクトップアプリ R用のIDE: RStudio</a></strong><br/>
RのIDEツール。主な機能、使い方は次の通り。</p>

<pre>
1) RStudioのコード補完や、関数へのジャンプ、組み込み関数のソース閲覧機能
2) Apacheとの連携によるレポート表示、Emailでのレポート配信
3) Gitによるソースコード管理のサポート機能
</pre>


<p><strong><a href="http://yihui.name/knitr/">knitr</a></strong><br/>
RStudio上でReproducible Researchを実現するためのパッケージ。Markdown, Rファイル => HTMLへの変換を行う事ができる。また、後述のRPubsへのアップロード機能も備える。</p>

<p><strong><a href="https://rpubs.com/">RPubs</a></strong><br/>
RStudio.comが提供するサービス。再現可能な研究(Reproducible Research)を実現するために、Rの実行手順などをWebでシェアできるサービス。</p>

<p><strong><a href="http://www.rstudio.com/ide/download/server">RStudio Server</a></strong><br/>
RStudioのWebアプリケーション版。</p>

<h2>Pythonによる機械学習</h2>

<p>Pythonで使えるデータ分析に有効なライブラリ群についての紹介。</p>

<h3>データ分析と機械学習</h3>

<p>データ分析において、機械学習は単なる手段の一つ。</p>

<pre>
* ある説明変数に対して目的変数がどのくらい影響を与えているかを知ることができる
* 複数の変数から重要な変数を絞り込む事ができる
</pre>


<h3>Python環境の準備</h3>

<p>もしMachのPython環境を作る場合は、拙著<strong>[Python開発環境構築 徹底解説<a href="python-pyenv-rehash-mac-development">pyenv, mac</a></strong>がオススメです。</p>

<h3>Python のライブラリ</h3>

<p>利用するライブラリのインストール。</p>

<p>{% codeblock lang:bash %}</p>

<h1>効率的な数値計算のためのライブラリ</h1>

<p>pip install numpy</p>

<h1>最適化、補間、積分、統計、画像処理、クラスタリング解析、信号処理、空間的解析などができる</h1>

<p>pip install scipy</p>

<h1>matplotlibのインストール用の設定</h1>

<p>ln -s /usr/local/opt/freetype/include/freetype2 /usr/local/include/freetype</p>

<h1>グラフ描画ライブラリ</h1>

<p>pip install matplotlib</p>

<h1>ファイルを作成してパラメータを記述</h1>

<p>vim ~/.matplotlib/matplotlibrc</p>

<h1>↓ 以下を追加</h1>

<p>backend : TkAgg</p>

<h1>機械学習ライブラリ: 単回帰分析〜SVMなどまで対応</h1>

<h1>教師あり学習、教師なし学習、モデル選択のためのクロスバリデーション</h1>

<p>pip install scikit-learn</p>

<h1>データ構造・データ解析のライブラリ：Rのdata.frameなどがある</h1>

<p>pip install pandas</p>

<h1>PythonからR言語を呼び出すためのライブラリ</h1>

<p>pip install rpy2</p>

<h1>pythonを対話的に実行するためのライブラリ</h1>

<p>pip install ipython
{% endcodeblock %}</p>

<h3>あやめのデータを知る</h3>

<p>{% codeblock lang:bash %}
wget https://dataminingproject.googlecode.com/svn-history/r44/DataMiningApp/datasets/Iris/iris.csv</p>

<h1>IRIS.csvのあるフォルダでipythonを起動</h1>

<p>ipython</p>

<h1>iris.csvを読み込み</h1>

<p>import pandas as pd
iris = pd.read_csv("iris.csv")
iris.info()</p>

<h1>データ加工</h1>

<p>setosa = iris[iris["Species"] == "setosa"]
versicolor = iris[iris["Species"] == "versicolor"]
virginica = iris[iris["Species"] == "virginica"]</p>

<h1>基本的な統計量</h1>

<h1>合計</h1>

<p>virginica.sum()</p>

<h1>Sepal Length                                                329.4</h1>

<h1>Sepal Width                                                 148.7</h1>

<h1>Petal Length                                                277.6</h1>

<h1>Petal Width                                                 101.3</h1>

<h1>平均</h1>

<p>virginica.mean()</p>

<h1>Sepal Length    6.588</h1>

<h1>Sepal Width     2.974</h1>

<h1>Petal Length    5.552</h1>

<h1>Petal Width     2.026</h1>

<h1>中央値</h1>

<p>virginica.median()</p>

<h1>Sepal Length    6.50</h1>

<h1>Sepal Width     3.00</h1>

<h1>Petal Length    5.55</h1>

<h1>Petal Width     2.00</h1>

<h1>最小値</h1>

<p>verginica.min()</p>

<h1>Sepal Length          4.9</h1>

<h1>Sepal Width           2.2</h1>

<h1>Petal Length          4.5</h1>

<h1>Petal Width           1.4</h1>

<h1>最大値</h1>

<p>virginica.max()</p>

<h1>Sepal Length          7.9</h1>

<h1>Sepal Width           3.8</h1>

<h1>Petal Length          6.9</h1>

<h1>Petal Width           2.5</h1>

<h1>相関係数</h1>

<p>virginica.corr()</p>

<h1>Sepal Length  Sepal Width  Petal Length  Petal Width</h1>

<h1>Sepal Length      1.000000     0.457228      0.864225     0.281108</h1>

<h1>Sepal Width       0.457228     1.000000      0.401045     0.537728</h1>

<h1>Petal Length      0.864225     0.401045      1.000000     0.322108</h1>

<h1>Petal Width       0.281108     0.537728      0.322108     1.000000</h1>

<h1>分散</h1>

<p>virginica.var()</p>

<h1>Sepal Length    0.404343</h1>

<h1>Sepal Width     0.104004</h1>

<h1>Petal Length    0.304588</h1>

<h1>Petal Width     0.075433</h1>

<h1>標準偏差</h1>

<p>virginica.std()</p>

<h1>Sepal Length    0.635880</h1>

<h1>Sepal Width     0.322497</h1>

<h1>Petal Length    0.551895</h1>

<h1>Petal Width     0.274650</h1>

<h1>共分散</h1>

<p>virginica.cov()</p>

<h1>Sepal Length  Sepal Width  Petal Length  Petal Width</h1>

<h1>Sepal Length      0.404343     0.093763      0.303290     0.049094</h1>

<h1>Sepal Width       0.093763     0.104004      0.071380     0.047629</h1>

<h1>Petal Length      0.303290     0.071380      0.304588     0.048824</h1>

<h1>Petal Width       0.049094     0.047629      0.048824     0.075433</h1>

<h1>ピポッテーブルの作成</h1>

<p>import numpy as np
pd.pivot_table(iris, rows=["Species"], aggfunc=np.mean)</p>

<h1>Petal Length  Petal Width  Sepal Length  Sepal Width</h1>

<h1>Species</h1>

<h1>setosa             1.464        0.244         5.006        3.418</h1>

<h1>versicolor         4.260        1.326         5.936        2.770</h1>

<h1>virginica          5.552        2.026         6.588        2.974</h1>

<p>{% endcodeblock %}</p>

<h3>irisでヒストグラムの実行</h3>

<p>{% codeblock lang:bash %}
import matplotlib.pyplot as plt
plt.hist(iris["Sepal Length"])
plt.xlabel("Sepal Length")
plt.ylabl("Freq")
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7021/13477008725_d1cb5034c1.jpg" width="500" height="379" alt="スクリーンショット 2014-03-29 9.37.35"></p>

<h3>箱ひげ図</h3>

<p>{% codeblock lang:bash %}
data = [setosa["Petal Length"], versiclor["Petal Length"], virginica["Petal Length"]]
plt.boxplot(data)
plt.xlabel("Species")
plt.ylabel("Petal Length")
ax = plt.gca()
plt.setp(ax, xticklabels=["setosa", "versicolor", "virginica"])
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7411/13477223695_baeac09a8f.jpg" width="500" height="391" alt="スクリーンショット 2014-03-29 9.55.08"></p>

<h3>散布図</h3>

<p>{% codeblock lang:bash %}
plt.scatter(versicolor["Sepal Width"], versicolor["Petal Width"])
plt.xlabel("Sepal Width")
plt.ylabel("Petal Width")
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7239/13477676814_035b5d164d.jpg" width="500" height="392" alt="スクリーンショット 2014-03-29 10.01.24"></p>

<h3>相関係数</h3>

<p>{% codeblock lang:bash %}</p>

<h1>versicolorのSepal WidthとPetal Widthの相関係数</h1>

<p>import numpy as np
corr = np.corrcoef(versicolor["Sepal Width"], versicolor["Petal Width"])
print(corr[0,1])</p>

<h1>=> 0.663998720024</h1>

<p>{% endcodeblock %}</p>

<h3>単回帰分析</h3>

<p>{% codeblock lang:bash %}
from sklearn import linear_model
LinerRegr = linear_model.LinearRegression()
X = setosa[["Sepal Length"]]
Y = setosa[["Sepal Width"]]
LinerRegr.fit(X, Y)
plt.scatter(X, Y, color="black")
px = np.arange(X.min(), X.max(), .01)[:,np.newaxis]
py = LinerRegr.predict(px)
plt.plot(px, py, color="blue", linewidth=3)
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm6.staticflickr.com/5281/13478622995_e46e9712f8.jpg" width="500" height="385" alt="スクリーンショット 2014-03-29 11.43.26"></p>

<p>{% codeblock lang:bash %}</p>

<h1>回帰変数</h1>

<p>print(LinerRegr.coef_)</p>

<h1>=> [[ 0.80723367]]</h1>

<h1>切片</h1>

<p>print(LinerRegr.inercept_)</p>

<h1>=> [-0.62301173]</h1>

<p>{% endcodeblock %}</p>

<h3>Rの機能を使って重回帰分析 => rpy2</h3>

<p>{% codeblock lang:bash %}
import rpy2.robjects as robjects
robjects.globalenv["SepalLength"] = robjects.FloatVector(setosa["Sepal Length"])
robjects.globalenv["SepalWidth"] = robjects.FloatVector(setosa["Sepal Width"])
robjects.globalenv["PetalLength"] = robjects.FloatVector(setosa["Petal Length"])
robjects.globalenv["PetalWidth"] = robjects.FloatVector(setosa["Petal Width"])
r = robjects.r</p>

<h1>Rの重回帰分析</h1>

<p>rlm = r.lm("SepalWidth~SepalLength+PetalLength+PetalWidth")
print((r.summary(rlm)))</p>

<h1>Residuals:</h1>

<h1>Min       1Q   Median       3Q      Max</h1>

<h1>-0.75025 -0.17983 -0.00411  0.15933  0.57704</h1>

<p>#</p>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept) -0.48721    0.56900  -0.856    0.396</h1>

<h1>SepalLength  0.79304    0.11162   7.105 6.35e-09 ***</h1>

<h1>PetalLength -0.09678    0.22875  -0.423    0.674</h1>

<h1>PetalWidth   0.31530    0.37186   0.848    0.401</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<p>#</p>

<h1>Residual standard error: 0.2594 on 46 degrees of freedom</h1>

<h1>Multiple R-squared:  0.5649,  Adjusted R-squared:  0.5366</h1>

<h1>F-statistic: 19.91 on 3 and 46 DF,  p-value: 2.042e-08</h1>

<p>{% endcodeblock %}</p>

<h3>ダミー変数の利用</h3>

<p>{% codeblock lang:bash %}</p>

<h1>数値データ以外も分析に活用する</h1>

<h1>Spiciesをダミー変数(品種の0/1のカラム)に変換</h1>

<p>import pandas as pd
dummies = pd.get_dummies(iris["Species"])
iris = pd.concat([iris, dummies], axis=1)
{% endcodeblock %}</p>

<h3>回帰係数と切片を求める</h3>

<p>{% codeblock lang:bash %}
LinearRegr = linear_model.LinearRegression()
X = iris[["setosa", "versicolor"]]
Y = iris[["Sepal Length"]]
LinearRegr.fit(X, Y)</p>

<h1>回帰係数</h1>

<p>print(LinearRegr.coef_) #=> [[-1.582 -0.652]]</p>

<h1>切片</h1>

<p>print(LinearRegr.intercept_) #=> [ 6.588]
{% endcodeblock %}</p>

<h3>　データマイニングの流れ</h3>

<pre>
1) ゴールを決める
2) 変数の種類・表している事象を知る
3) 1変量の解析を行う。平均値を算出し、ヒストグラム・箱ひげ図を描写
4) 2変量の解析を行う。散布図などのグラフから理解を深める
5) 説明変数を減らして、解釈しやすいモデルに当てはめる
6) 推定された結果を(2)-(4)に振り返ってモデル検証を行う
</pre>


<h3>ロジスティック回帰モデル</h3>

<p>{% codeblock lang:bash %}
import sklearn
use_data = np.logical_or(iris["Species"] == "setosa", iris["Species"] == "virginica")
setosa_virginica = iris[use_data]
X = setosa_virginica[["Sepal Length", "Sepal Width"]]
Y = setosa_virginica[["setosa"]]
LogRegr = sklearn.linear_model.LogisticRegression(C=1.0)
LogRegr.fit(X,Y)</p>

<h1>偏回帰係数</h1>

<p>print(LogRegr.coef_)</p>

<h1>=> [[-2.15056433  3.53948104]]</h1>

<h1>切片</h1>

<p>print(LogRegr.intercept_)</p>

<h1>=> [ 0.94024115]</h1>

<p>{% endcodeblock %}</p>

<p>なんか間違っている気がするので、後で直す予定です。まずはちょっと触れるところから。</p>

<h3>モデル結果をグラフにして確認</h3>

<p>{% codeblock lang:bash %}
xMin = X["Sepal Length"].min()
xMax = X["Sepal Length"].max()
yMin = X["Sepal Width"].min()
yMax = X["Sepal Width"].max()
xx,yy = np.meshgrid(np.arange(xMin, xMax, 0.01), np.arange(yMin, yMax, 0.01))
Z = LogRegr.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure()
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
plt.scatter(X["Sepal Length"], X["Sepal Width"], c=np.array(Y), cmap=plt.cm.Paired)
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7097/13482940784_02c88ebeb3.jpg" width="500" height="389" alt="スクリーンショット 2014-03-29 17.36.16"></p>

<h2>決定木</h2>

<p>まずはhomebrewでGraphvizとpythonで使うためのライブラリpydotをインストール。</p>

<p>{% codeblock lang:bash %}
brew update
brew install gts
brew install graphviz</p>

<p>pip uninstall pyparsing
pip install -Iv https://pypi.python.org/packages/source/p/pyparsing/pyparsing-1.5.7.tar.gz#md5=9be0fcdcc595199c646ab317c1d9a709
pip install pydot
{% endcodeblock %}</p>

<p>続いてIPythonのコンソール。</p>

<p>{% codeblock lang:bash %}
from sklearn import tree
X = iris[["Sepal Length", "Sepal Width", "Petal Length", "Petal Width"]]
Y = iris[["Species"]]
treeClf = tree.DecisionTreeClassifier(max_depth=2)
treeClf.fit(X, Y)
{% endcodeblock %}</p>

<h3>決定木</h3>

<p>同じくグラフがなんか違うので、後で見直す予定。</p>

<p>{% codeblock lang:bash %}</p>

<h2>決定木の構築</h2>

<p>from sklearn import tree
X = iris[["Sepal Length", "Sepal Width", "Petal Length", "Petal Width"]]
Y = iris[["Species"]]
treeClf = tree.DecisionTreeClassifier(max_depth=2)
treeClf.fit(X, Y)</p>

<h2>決定木の可視化</h2>

<p>import StringIO
import pydot
dot_data = StringIO.StringIO()
tree.export_graphviz(treeClf, out_file=dot_data, feature_names=["Sepal Length", "Sepal Width", "Petal Length", "Petal Width"])
graph = pydot.graph_from_dot_data(dot_data.getvalue())
graph.write_pdf("iris_decision_tree.pdf")
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7051/13483376885_725bd15d81.jpg" width="500" height="313" alt="スクリーンショット 2014-03-29 18.43.37"></p>

<h3>k-means法</h3>

<p>{% codeblock lang:bash %}</p>

<h1>k-meansの実行</h1>

<p>from sklearn import cluster
X = iris[["Sepal Length", "Sepal Width"]]
kmeansCls = cluster.KMeans(n_clusters=3)
kmeansCls.fit(X)
print(kmeansCls.predict(X))</p>

<h1>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</h1>

<h1>0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</h1>

<h1>1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2</h1>

<h1>2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2</h1>

<h1>2 1]</h1>

<h1>k-means法の可視化</h1>

<p>import numpy as np
import matplotlib.pyplot as plt
def category2int(x):</p>

<pre><code>category = {"setosa": 0, "versicolor": 1, "virginica": 2}
return category[x]
</code></pre>

<p>f = lambda x: category2int(x)
Y = iris["Species"].map(f)
xMin = X["Sepal Length"].min()
xMax = X["Sepal Length"].max()
yMin = X["Sepal Width"].min()
yMax = X["Sepal Width"].max()
xx,yy = np.meshgrid(np.arange(xMin, xMax, 0.01), np.arange(yMin, yMax, 0.01))
Z = kmeansCls.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure()
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Reds)
plt.scatter(X["Sepal Length"], X["Sepal Width"], c=np.array(Y), cmap=plt.cm.Blues)
plt.show()
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7021/13483696374_cef951e923.jpg" width="500" height="384" alt="スクリーンショット 2014-03-29 18.40.37"></p>

<h2>データマイニング 10のアルゴリズム</h2>

<h3>C4.5</h3>

<pre>
* 決定木を構築する計算アルゴリズム
* 結果のイメージが直感的て理解しやすい
* 分類の精度は高くない
</pre>


<h3>k-meansアルゴリズム</h3>

<p>クラスタリングでは次のステップを反復して計算。</p>

<pre>
1) データにクラスタを割り振る(中心と一番近いクラスタを割り振る)
2) クラスタ事の平均値の計算
</pre>


<h3>サポートベクターマシン</h3>

<pre>
* 少なめのサンプルで説明変数が多い場合に適した手法
* SVMは特徴量が膨大でも高速に計算できる
* サンプル数が増えると計算時間が急に増加する
</pre>


<h3>アプリオリアルゴリズム</h3>

<pre>
* 大量のトランザクションの中から価値あるつながりを見つけるために使われる手法
* 強力な結果が得られる上、実装が比較的容易
</pre>


<h3>EMアルゴリズム</h3>

<p>Expectation-maximization algorithm(期待値最大化法)。数式展開では計算出来ないような複雑なモデルのパラメータ推定に用いる。以下はその手順。</p>

<pre>
1) パラメータの初期値を決める
2) Exceptationステップ: 未観測データの期待値を求める
3) Maximizationステップ: 未観測データの期待値を求める
</pre>


<h3>アダブースト</h3>

<pre>
* いくつかの学習木を組み合わせることで、強力な予測性能を得る
* 実装が容易で解釈がしやすい
* ディープラーニングが出るまでは最も有力な予測モデルと言われてきた
</pre>


<h3>ナイーブベイズ</h3>

<pre>
* クラスを予測するための手法で構築が容易
* テキスト分類やパターン認識で広く利用されている
</pre>


<h2>Rによるマーケティング分析</h2>

<p><strong><a href="http://www.slideshare.net/yokkuns/r-22276096">R言語で学ぶマーケティング分析 競争ポジショニング戦略</a></strong><br/>
この記事の作者さんの発表。エッセンスがうまくまとまっています。</p>

<h4>主成分分析の実行</h4>

<p>{% codeblock lang:bash %}
setwd("~/Dropbox/study-R/")</p>

<h1>データの読み込み</h1>

<p>sp.user.data &lt;- read.csv("data/sp_user_research_data.csv")</p>

<h1>主成分分析 => ユーザー間の類似関係の把握</h1>

<p>sp.user.pca &lt;- prcomp(sp.user.data[,-1], scale =T)</p>

<h1>バイプロットの表示</h1>

<p>biplot(sp.user.pca)
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7385/13503111214_0e5f8a5458.jpg" width="500" height="461" alt="スクリーンショット 2014-03-30 13.33.16"></p>

<h3>k-meansによるクラスタリングの実行</h3>

<p>{% codeblock lang:bash %}
sp.user.km &lt;- kmeans(sp.user.data[,-1], 4)</p>

<h1>主成分分析の結果にクラスタ情報を付与</h1>

<p>sp.user.pca.df &lt;- data.frame(sp.user.pca$x)
sp.user.pca.df$id &lt;- sp.user.data$id
sp.user.pca.df$cluster &lt;- as.factor(sp.user.km$cluster)</p>

<h1>描画</h1>

<p>install.packages("ggplot2")
library("ggplot2")
ggplot(sp.user.pca.df, aes(x = PC1, y = PC2, label = id, col = cluster)) + geom_text() + theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm3.staticflickr.com/2894/13502876065_8f6ff5d2ca.jpg" width="500" height="361" alt="スクリーンショット 2014-03-30 13.40.50"></p>

<h3>レーダーチャートの作成</h3>

<p>{% codeblock lang:bash %}
install.packages("fmsb")
library("fmsb")</p>

<h1>レーダーチャート用にデータを整形</h1>

<p>df &lt;- data.frame(scale(sp.user.km$centers))
dfmax &lt;- apply(df, 2, max) + 1
dfmin &lt;- apply(df, 2, min) - 1
df &lt;- rbind(dfmax, dfmin, df)</p>

<h1>レーダーチャートの描画</h1>

<p>radarchart(df, seg = 5, plty = 1, pcol = rainbow(4))
legend("topright", legend = 1:4, col = rainbow(4), lty = 1)
{% endcodeblock %}</p>

<p><img src="https://farm8.staticflickr.com/7019/13503401394_8b67f779c2.jpg" width="500" height="361" alt="スクリーンショット 2014-03-30 13.54.09"></p>

<p>{% codeblock lang:bash %}</p>

<h2>MDS(Mult-dimentional scaling: 多次元尺度構成法) - 知覚マップ</h2>

<p>install.packages("MASS")
library("MASS")</p>

<h1>先行データの読み込み</h1>

<p>setwd("~/Dropbox/study-R/data")
target.data &lt;- read.csv("target_preference_data.csv", header = T)</p>

<h1>非計算MDSの実行</h1>

<p>service.dist &lt;- dist(t(target.data[, -1]))
service.map &lt;- isoMDS(service.dist)</p>

<h1>描画用データの整形</h1>

<p>service.map.df &lt;- data.frame(scale(service.map$points))
service.map.df$service_name &lt;- names(target.data[, -1])</p>

<h1>描画</h1>

<p>ggplot(service.map.df, aes(x = X1, y = X2, label = service_name)) + geom_text() + theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm4.staticflickr.com/3749/13503351383_77ffd5a8f6.jpg" width="500" height="309" alt="スクリーンショット 2014-03-30 14.07.50"></p>

<h3>選好ベクトルの推定</h3>

<p>{% codeblock lang:bash %}</p>

<h1>選好ベクトルの推定</h1>

<p>user.preference.data &lt;-
  do.call(rbind,</p>

<pre><code>      lapply(1:nrow(target.data),
             function(i){
               preference.data &lt;- data.frame(
                 p=as.numeric(target.data[i, -1]),
                 X1=service.map.df$X1,
                 X2=service.map.df$X2)
               fit &lt;- lm(p~., data=preference.data)
               b &lt;- 2/sqrt(fit$coef["X1"]^2+fit$coef["X2"]^2)
               data.frame(X1=b*fit$coef["X1"],
                          X2=b*fit$coef["X2"],
                          service_name=i)
             }))
</code></pre>

<h1>選好ベクトルの描画</h1>

<p>ggplot(service.map.df, aes(x=X1, y=X2, label=service_name)) +
  geom_text() +
  theme_bw(16) +
  xlim(-2,2) +
  ylim(-2,2) +
  geom_point(data=user.preference.data, aes(x=X1, y=X2))
{% endcodeblock %}</p>

<p><img src="https://farm6.staticflickr.com/5111/13503847744_282c983ee8.jpg" width="500" height="309" alt="スクリーンショット 2014-03-30 14.27.52"></p>

<h3>仮想データの読み込みと散布図の描画</h3>

<p>{% codeblock lang:bash %}</p>

<h1>利用する仮想データの読み込み</h1>

<p>library(ggplot2)
library(scales)</p>

<h1>GRPと売上データの読み込み</h1>

<p>grp.data &lt;- read.csv("http://image.gihyo.co.jp/assets/files/book/2013/978-4-7741-5896-9/download/grp.csv", header = T)</p>

<h1>散布図の描画</h1>

<p>ggplot(grp.data, aes(x=grp, y=amount)) +
  geom_point() +
  scale_y_continuous(label=comma, limits=c(0,360000)) +
  ylab("Sales") +
  xlab("GRP") +
  theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm4.staticflickr.com/3707/13508727343_d9de5250f9.jpg" width="500" height="312" alt="スクリーンショット 2014-03-30 20.50.20"></p>

<h3>線形型モデルの構築</h3>

<p>{% codeblock lang:bash %}</p>

<h1>可視化はgeom_smooth関数ですぐにできる</h1>

<p>ggplot(grp.data, aes(x=grp, y=amount)) +
  geom_point() +
  scale_y_continuous(label=comma, limits=c(0, 360000)) +
  ylab("Sales") +
  xlab("GRP") +
  geom_smooth(method = "lm") +
  theme_bw(16)</p>

<h1>モデル構築</h1>

<p>fit &lt;- lm(amount~grp, data=grp.data)</p>

<h1>モデル概要の表示</h1>

<p>summary(fit)</p>

<h1>Call:</h1>

<h1>lm(formula = amount ~ grp, data = grp.data)</h1>

<h1></h1>

<h1>Residuals:</h1>

<h1>Min     1Q Median     3Q    Max</h1>

<h1>-31521  -8275    321   8701  36962</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept) 229812.53    2561.73   89.71   &lt;2e-16 ***</h1>

<h1>grp            455.54      15.93   28.60   &lt;2e-16 ***</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>Residual standard error: 13070 on 198 degrees of freedom</h1>

<h1>Multiple R-squared:  0.8051,  Adjusted R-squared:  0.8042</h1>

<h1>F-statistic: 818.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16</h1>

<p>{% endcodeblock %}</p>

<p><img src="https://farm4.staticflickr.com/3762/13508864803_8876878af0.jpg" width="500" height="292" alt="スクリーンショット 2014-03-30 20.58.40"></p>

<h3>xx減型モデルの構築</h3>

<p>読めなかった。日本語苦手っすw</p>

<p>{% codeblock lang:bash %}
fit &lt;- lm(log(amount)~log(grp), data=grp.data)</p>

<h1>モデル概要</h1>

<p>summary(fit)</p>

<h1>Call:</h1>

<h1>lm(formula = log(amount) ~ log(grp), data = grp.data)</h1>

<h1></h1>

<h1>Residuals:</h1>

<h1>Min        1Q    Median        3Q       Max</h1>

<h1>-0.121782 -0.023589 -0.003298  0.025672  0.116504</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept) 11.546963   0.032379  356.62   &lt;2e-16 ***</h1>

<h1>log(grp)     0.213933   0.006551   32.66   &lt;2e-16 ***</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>Residual standard error: 0.04064 on 198 degrees of freedom</h1>

<h1>Multiple R-squared:  0.8434,  Adjusted R-squared:  0.8426</h1>

<h1>F-statistic:  1067 on 1 and 198 DF,  p-value: &lt; 2.2e-16</h1>

<h1>予測結果を描画</h1>

<p>fit.data &lt;- data.frame(grp=grp.data$grp, amount=exp(fit$fitted.values))
ggplot(grp.data, aes(x=grp, y=amount)) +
  geom_point() +
  geom_line(data=fit.data, aes(x=grp, y=amount)) +
  scale_y_continuous(label=comma, limits=c(0,360000)) +
  ylab("Sales") +
  xlab("GRP") +
  theme_bw(16)
{% endcodeblock %}</p>

<p><img src="https://farm4.staticflickr.com/3766/13509463494_e26fa8a9e8.jpg" width="500" height="356" alt="スクリーンショット 2014-03-30 21.19.14"></p>

<h3>AとBが同じ場合のA/Bテストのシミュレーション</h3>

<p>{% codeblock lang:bash %}
library("plyr")</p>

<h1>A/Bの心のコンバージョン率を設定</h1>

<p>A1.CVR &lt;- 0.09
B1.CVR &lt;- 0.09</p>

<h1>サンプル数</h1>

<p>n &lt;- 10000
set.seed(2)</p>

<h1>シミュレーション</h1>

<p>AB1 &lt;- data.frame(Pattern=c(rep("A", n), rep("B", n)), CV=c(rbinom(n, 1, A1.CVR), rbinom(n, 1, B1.CVR)))</p>

<h1>コンバージョン率の算出</h1>

<p>ddply(AB1, .(Pattern), summarize, CVR=mean(CV))</p>

<h1>Pattern    CVR</h1>

<h1>1       A 0.0942</h1>

<h1>2       B 0.0912</h1>

<h1>カイ二乗検定</h1>

<p>chisq.test(table(AB1))</p>

<h1>Pearson's Chi-squared test with Yates' continuity correction</h1>

<h1></h1>

<h1>data:  table(AB1)</h1>

<h1>X-squared = 0.5, df = 1, p-value = 0.4795</h1>

<p>{% endcodeblock %}</p>

<h3>Bのほうが高い場合のA/Bテストのシミュレーション</h3>

<p>{% codeblock lang:bash %}
A2.CVR &lt;- 0.095
B2.CVR &lt;- 0.097
n &lt;- 10000
set.seed(2)</p>

<h1>シミュレーション</h1>

<p>AB2 &lt;- data.frame(Pattern=c(rep("A", n), rep("B", n)), CV=c(rbinom(n, 1, A2.CVR), rbinom(n, 1, B2.CVR)))</p>

<h1>コンバージョン率の算出</h1>

<p>ddply(AB2, .(Pattern), summarize, CVR = mean(CV))</p>

<h1>Pattern   CVR</h1>

<h1>1       A 0.100</h1>

<h1>2       B 0.097</h1>

<h1>カイ二乗検定</h1>

<p>chisq.test(table(AB2))</p>

<h1>Pearson's Chi-squared test with Yates' continuity correction</h1>

<h1></h1>

<h1>data:  table(AB2)</h1>

<h1>X-squared = 0.4735, df = 1, p-value = 0.4914</h1>

<p>{% endcodeblock %}</p>

<h3>50万件でのシミュレーション</h3>

<p>{% codeblock lang:bash %}
A3.CVR &lt;- 0.095
B3.CVR &lt;- 0.097
n &lt;- 500000
set.seed(2)</p>

<h1>シミュレーション</h1>

<p>AB3 &lt;- data.frame(Pattern=c(rep("A", n), rep("B", n)),</p>

<pre><code>              CV=c(rbinom(n, 1, A3.CVR), rbinom(n, 1, B3.CVR)))
</code></pre>

<h1>コンバージョン率の算出</h1>

<p>ddply(AB3, .(Pattern), summarize, CVR=mean(CV))</p>

<h1>Pattern      CVR</h1>

<h1>1       A 0.095192</h1>

<h1>2       B 0.097040</h1>

<h1>カイ二乗検定の実行</h1>

<p>chisq.test(table(AB3))</p>

<h1>Pearson's Chi-squared test with Yates' continuity correction</h1>

<h1></h1>

<h1>data:  table(AB3)</h1>

<h1>X-squared = 9.8061, df = 1, p-value = 0.001739</h1>

<p>{% endcodeblock %}</p>

<h3>直交表の作成</h3>

<p>{% codeblock lang:bash %}
install.packages("conjoint")
library(conjoint)</p>

<h1>構成要素</h1>

<p>experiment &lt;- expand.grid(
  imgA = c("ImageA1", "ImageA2"),
  imgB = c("ImageB1", "ImageB2"),
  txtA = c("TextA1", "TextA2"),
  txtB = c("TextB1", "TextB2"))</p>

<h1>交流表の作成</h1>

<p>design.ort &lt;- caFactorialDesign(data= experiment, type="orthogonal")
design.ort</p>

<h1>imgA    imgB   txtA   txtB</h1>

<h1>2  ImageA2 ImageB1 TextA1 TextB1</h1>

<h1>3  ImageA1 ImageB2 TextA1 TextB1</h1>

<h1>5  ImageA1 ImageB1 TextA2 TextB1</h1>

<h1>8  ImageA2 ImageB2 TextA2 TextB1</h1>

<h1>9  ImageA1 ImageB1 TextA1 TextB2</h1>

<h1>12 ImageA2 ImageB2 TextA1 TextB2</h1>

<h1>14 ImageA2 ImageB1 TextA2 TextB2</h1>

<h1>15 ImageA1 ImageB2 TextA2 TextB2</h1>

<p>{% endcodeblock %}</p>

<h3>ロジスティック回帰モデルの構築</h3>

<p>{% codeblock lang:bash %}</p>

<h1>仮想のテスト結果データの読み込み(csがないので以下実行していません)</h1>

<p>web.test.data &lt;- read.csv("web_test_sample.csv")</p>

<h1>上位6件だけ表示</h1>

<p>head(web.test.data)</p>

<h1>ロジスティック回帰モデルの構築</h1>

<p>fit &lt;- step(glm(cv~., data = web.test.data[,-1], family=binomial))</p>

<h1>モデル概要</h1>

<p>summary(fit)
{% endcodeblock %}</p>

<h2>Fluentd 入門</h2>

<p>ログ回収をストリーミングで行ってくれるツール。ツールについての詳しい説明は、創業者の古橋さんのインタビュー記事『<strong><a href="http://www.atmarkit.co.jp/ait/articles/1310/07/news010.html">グリー技術者が聞いた、fluentdの新機能とTreasure Data古橋氏の野心</a></strong>』がわかりやすくてオススメです。</p>

<h3>ログ解析の従来の課題</h3>

<pre>
1) ログデータの転送に長い時間が必要、帯域の局所的な消費
2) データが解析可能になるまでのタイムラグ
3) ネットワーク障害などによる不安定さ
</pre>


<h3>Fluentdの利点</h3>

<pre>
1) JSON形式で構造化されている
2) 豊富なプラグイン(プラガブル)
3) 導入が容易
4) 耐障害性、バッファ、レジューム機能、可用性
</pre>


<h3>アーキテクチャ</h3>

<pre>
1) input: さまざまなソースからデータを読み取りFluentd用の形式に変換
2) Engine: 入ってきたデータをどのBuffe/Outpuプラグインに渡すかをルーティング
3) Bufferプラグイン: Outputプラグインの前にデータを一時保持する仕組み
4) Outputプラグイン: Bufferプラグインから渡されたデータを書き出す
</pre>


<h3>ログ構造</h3>

<pre>
tag: ログの種類。ルーティングで利用
time: ログが記録されたUNIX時刻
record: JSON形式のログ内容
</pre>


<h3>活用方法</h3>

<pre>
1) ログ収集: ログを集めてS3やHadoopに渡す
2) 統計 + 可視性: ログの統計情報を取得し、グラフ表示系のツールと連携する
3) 検知/通知: しきい値ベースや、データマイニングの異常値検出の通知など
</pre>


<h3>システム構成</h3>

<pre>
forwarder: ログ発生場所に立ち上げてログ回収 => aggregatorに送信
aggregator: 複数workerのためのロードバランシング、複数台構成による冗長化
worker: ログのパースや書き換えなど負荷の高い処理を行う
serializer: workerの処理結果のログを集約して別システムに書き出す
watcher: ログからメトリックスを収集、検知や通知を行う
</pre>


<h3>事例紹介</h3>

<pre>
1) ログの回収: DSPの広告効果のレポーティングのための収集の仕組み
2) nginx ログの可視化: Real Time Bidding(RTB)のツール
</pre>


<h2>あとがき</h2>

<p>中身濃すぎる！超面白かったです＾＾</p>

<h2>Speical Thanks</h2>

<p><strong><a href="http://stackoverflow.com/questions/20572366/sudo-pip-install-matplotlib-fails-to-find-freetype-headers-os-x-mavericks">python - <code>Sudo pip install matplotlib</code> fails to find freetype headers. [OS X Mavericks / 10.9] - Stack Overflow</a></strong></p>

<p><strong><a href="http://stackoverflow.com/questions/15951748/pydot-and-graphviz-error-couldnt-import-dot-parser-loading-of-dot-files-will">python - pydot and graphviz error: Couldn't import dot_parser, loading of dot files will not be possible - Stack Overflow</a></strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「R Tips」で学ぶ R言語のプログラミング的Tips]]></title>
    <link href="http://morizyun.github.io/blog/R-tips-Grammer-PG-language/"/>
    <updated>2014-03-22T10:50:00+09:00</updated>
    <id>http://morizyun.github.io/blog/R-tips-Grammer-PG-language</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4274067831/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4274067831&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4274067831&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4274067831" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />統計学やR言語を勉強していく過程で、R言語のプログラミング的な側面や言語仕様をちゃんと理解したいと思うようになりました。ということでチャンレンジしたのが、『<strong><a href="http://cse.naro.affrc.go.jp/takezawa/r-tips/r.html">R-Tips</a></strong>』です。</p>

<p>このサイトはいくつかのトピックに分かれているので、自分のRの使い方に合わせて勉強しやすくなっています。まだまだ前半戦ですが、特に覚えておきたいコマンドを中心にメモを書いていきます。</p>

<p><strong>(03/22 10:50) 回帰分析部分の説明を追記</strong><br/></p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_yoko_naga.html %}</p>

<h3>Rの便利なメソッド</h3>

<p>{% codeblock lang:bash %}
x &lt;- "1245"</p>

<h1>中身についての情報を取得</h1>

<p>str(x) #=> chr "1245"
summary(x)</p>

<h1>Length     Class      Mode</h1>

<h1>1 character character</h1>

<h1>基本的な情報</h1>

<p>y &lt;- c(2, 3, 4, 5)
summary(y)</p>

<h1>Min. 1st Qu.  Median    Mean 3rd Qu.    Max.</h1>

<h1>2.00    2.75    3.50    3.50    4.25    5.00</h1>

<p>{% endcodeblock %}</p>

<h3>ベクトルの生成</h3>

<p>{% codeblock lang:bash %}</p>

<h1>aからbまでのベクトルを生成 => a:b</h1>

<p>1:10</p>

<h1>=> 1  2  3  4  5  6  7  8  9 10</h1>

<h1>(a, b)間をn等分するベクトルを生成 => seq(a, b, length = n)</h1>

<p>seq(1, 2, length = 10)</p>

<h1>1.000000 1.111111 1.222222 1.333333 1.444444</h1>

<h1>1.555556 1.666667 1.777778 1.888889 2.000000</h1>

<h1>aからbまでcずつ増加するベクトルを生成 => seq(a, b, by = c)</h1>

<p>seq(2, 15, by=3)</p>

<h1>=> 2  5  8 11 14</h1>

<h1>a:bをc回繰り返すベクトルを生成 => rep(a:b, times = c)</h1>

<p>rep(1:3, times = 2)</p>

<h1>=> 1 2 3 1 2 3</h1>

<h1>ベクトル内の値をユニークにする(重複した値を取り除く) => unique(x)</h1>

<p>unique(c(1,2,4,6,3,3,2,1))</p>

<h1>=> 1 2 4 6 3</h1>

<h1>0をn個並べたベクトルを生成</h1>

<p>numeric(5)</p>

<h1>=> 0 0 0 0 0</h1>

<p>x &lt;- 1:5</p>

<h1>要素の並びを逆順にする</h1>

<p>rev(x)</p>

<h1>=> 5 4 3 2 1</h1>

<p>y &lt;- 5:1</p>

<h1>要素の並び順を整列させる</h1>

<p>sort(y)</p>

<h1>=> 1 2 3 4 5</h1>

<p>{% endcodeblock %}</p>

<h3>ベクトル条件</h3>

<p>{% codeblock lang:bash %}</p>

<h1>どれか一つでも条件をみたすものがあるか？</h1>

<p>any(y > 4) #=> TRUE</p>

<h1>すべて条件をみたすか？</h1>

<p>all(y > 2) #=> FALSE</p>

<h1>NULLか否か</h1>

<p>is.null(NULL) #=> TRUE</p>

<h1>NA(データの欠損)か否か</h1>

<p>is.na(NA) #=> TRUE</p>

<h1>NaN(数では表せない)か否か</h1>

<p>is.nan(0/0) #=> TRUE</p>

<h1>有限か否か</h1>

<p>is.finite(1) #=> TRUE</p>

<h1>無限か否か</h1>

<p>is.infinite(1/0) #=> TRUE
{% endcodeblock %}</p>

<h3>ベクトルの中身</h3>

<p>{% codeblock lang:bash %}
x &lt;- 1:5</p>

<h1>2-4番目の要素</h1>

<p>x[2:4] #=> 2, 3, 4</p>

<h1>1, 4番目の要素以外を取り出す</h1>

<p>x[c(-1,-4)] #=> 2, 3, 5</p>

<h1>3より大きい要素を取り出す</h1>

<p>x[3 &lt; x] #=> 4, 5</p>

<h1>1より大きく、4より小さい要素を取り出す</h1>

<p>x[1 &lt; x &amp; x &lt; 4] #=> 2, 3</p>

<p>y &lt;- x * 2</p>

<h1>条件を満たす要素の番号を返す</h1>

<p>(1:length(y))[1 &lt; y &amp; y &lt; 8]</p>

<h1>=> 1, 2, 3</h1>

<p>{% endcodeblock %}</p>

<h3>日付</h3>

<p>{% codeblock lang:bash %}</p>

<h1>日付型に変換</h1>

<p>as.Date("2012/08/15")</p>

<h1>=> "2012-08-15"</h1>

<p>as.Date("20130222", format="%Y%m%d")</p>

<h1>=> "2013-02-22"</h1>

<h1>現在時刻を日付オブジェクトで取得</h1>

<p>today &lt;- Sys.Date()
now &lt;- Sys.time()</p>

<h1>日付の差分</h1>

<p>x &lt;- as.Date(c("2012/07/01", "2009/08/01"))
x[2] - x[1]</p>

<h1>=> Time difference of -1065 days</h1>

<p>{% endcodeblock %}</p>

<h3>文字列</h3>

<p>{% codeblock lang:bash %}</p>

<h1>文字列ベクトルの結合</h1>

<p>paste(month.abb, 1:8, c("st", "nd", "rd", rep("th",5)))</p>

<h1>"Jan 1 st" "Feb 2 nd" "Mar 3 rd" "Apr 4 th" "May 5 th" "Jun 6 th"</h1>

<h1>"Jul 7 th" "Aug 8 th" "Sep 1 st" "Oct 2 nd" "Nov 3 rd" "Dec 4 th"</h1>

<h1>文字列から特定の部分を切り出し</h1>

<p>substring("defghijklmnopqr", 3, 7)</p>

<h1>=> "fghij"</h1>

<p>{% endcodeblock %}</p>

<h3>データフレーム</h3>

<p>{% codeblock lang:bash %}</p>

<h1>データフレームの作成</h1>

<p>sex &lt;- c(rep("F", 3), rep("M", 1))
height &lt;- seq(150, 180, length=4)
weight &lt;- seq(70, 80, length=4)
df &lt;- data.frame(sex, height, weight)</p>

<h1>sex height   weight</h1>

<h1>1   F    150 70.00000</h1>

<h1>2   F    160 73.33333</h1>

<h1>3   F    170 76.66667</h1>

<h1>4   M    180 80.00000</h1>

<h1>データオブジェクトの行数を返す</h1>

<p>nrow(df) #=> 4</p>

<h1>summaryでデータフレームの要素の情報を表示</h1>

<p>summary(df)</p>

<h1>sex       height          weight</h1>

<h1>F:3   Min.   :150.0   Min.   :70.0</h1>

<h1>M:1   1st Qu.:157.5   1st Qu.:72.5</h1>

<h1>Median :165.0   Median :75.0</h1>

<h1>Mean   :165.0   Mean   :75.0</h1>

<h1>3rd Qu.:172.5   3rd Qu.:77.5</h1>

<h1>Max.   :180.0   Max.   :80.0</h1>

<h1>heightで降順に並べ直し</h1>

<p>sort_list &lt;- order(df$height,decreasing=T)
df[sort_list,]</p>

<h1>sex height   weight</h1>

<h1>4   M    180 80.00000</h1>

<h1>3   F    170 76.66667</h1>

<h1>2   F    160 73.33333</h1>

<h1>1   F    150 70.00000</h1>

<h1>typeカラムに条件に応じてBIG, MIDDLE, SMALL</h1>

<p>df$type <- ifelse(height >= 170, "BIG", ifelse(height>=160, "MIDDLE", "SMALL"))</p>

<h1>sex height   weight   type</h1>

<h1>1   F    150 70.00000  SMALL</h1>

<h1>2   F    160 73.33333 MIDDLE</h1>

<h1>3   F    170 76.66667    BIG</h1>

<h1>4   M    180 80.00000    BIG</h1>

<h1>一律でheight_mmカラムを追加</h1>

<p>transform(df, height_mm=df$height*100)</p>

<h1>sex height   weight   type height_mm</h1>

<h1>1   F    150 70.00000  SMALL     15000</h1>

<h1>2   F    160 73.33333 MIDDLE     16000</h1>

<h1>3   F    170 76.66667    BIG     17000</h1>

<h1>4   M    180 80.00000    BIG     18000</h1>

<h1>行番号を追加</h1>

<p>df$id &lt;- row.names(df)</p>

<h1>sex height   weight   type id</h1>

<h1>1   F    150 70.00000  SMALL  1</h1>

<h1>2   F    160 73.33333 MIDDLE  2</h1>

<h1>3   F    170 76.66667    BIG  3</h1>

<h1>4   M    180 80.00000    BIG  4</h1>

<h1>行方向の結合</h1>

<p>cbind(df, df)</p>

<h1>sex height   weight   type id sex height   weight   type id</h1>

<h1>1   F    150 70.00000  SMALL  1   F    150 70.00000  SMALL  1</h1>

<h1>2   F    160 73.33333 MIDDLE  2   F    160 73.33333 MIDDLE  2</h1>

<h1>3   F    170 76.66667    BIG  3   F    170 76.66667    BIG  3</h1>

<h1>4   M    180 80.00000    BIG  4   M    180 80.00000    BIG  4</h1>

<h1>列方向の結合</h1>

<p>rbind(df, df)</p>

<h1>sex height   weight   type id</h1>

<h1>1   F    150 70.00000  SMALL  1</h1>

<h1>2   F    160 73.33333 MIDDLE  2</h1>

<h1>3   F    170 76.66667    BIG  3</h1>

<h1>4   M    180 80.00000    BIG  4</h1>

<h1>5   F    150 70.00000  SMALL  1</h1>

<h1>6   F    160 73.33333 MIDDLE  2</h1>

<h1>7   F    170 76.66667    BIG  3</h1>

<h1>8   M    180 80.00000    BIG  4</h1>

<p>{% endcodeblock %}</p>

<h2>サンプルデータセット</h2>

<p>Rで勉強していく上で、避けて通れないのがサンプルデータの確保です。Rにはデフォルトで複数のデータセットが格納されています。『<strong><a href="http://d.hatena.ne.jp/hoxo_m/20120214/p1">統計を学びたい人へ贈る、統計解析に使えるデータセットまとめ</a></strong>』に初心者におすすめなデータがまとめられています！</p>

<h2>データセットを使った集計・グラフ表示</h2>

<p>デフォルトのデータセットのあやめのデータ(iris)を使って、いくつかサンプルを書いていきます。</p>

<p>{% codeblock lang:bash %}</p>

<h1>irisはあやめのデータ・セット</h1>

<p>width_set <- iris[,c("Sepal.Length", "Petal.Length", "Species")]
width_large <- width_set[width_set$Species == "versicolor",]
mean(width_large$Petal.Length) #=> 4.26</p>

<h1>plyrパッケージの導入</h1>

<h1>plyr : データフレームの加工をしやすくするパッケージ</h1>

<p>install.packages("plyr")
library("plyr")</p>

<h1>irisのデータからSpeciesをキーにSepal.Length(がくの長さ)の平均を出す</h1>

<p>data &lt;- ddply(iris, .(Species), summarise, Sepal.Length.Average=mean(Sepal.Length))</p>

<h1>Species Sepal.Length.Average</h1>

<h1>1     setosa                5.006</h1>

<h1>2 versicolor                5.936</h1>

<h1>3  virginica                6.588</h1>

<h1>Sepal.Length(がくの長さ)の平均順でならべ直す</h1>

<p>arrange(data, desc(Sepal.Length.Average))</p>

<h1>Species Sepal.Length.Average</h1>

<h1>1  virginica                6.588</h1>

<h1>2 versicolor                5.936</h1>

<h1>3     setosa                5.006</h1>

<h1>棒グラフ</h1>

<h1>棒グラフは「月ごとの販売実績」のように書くカテゴリの度数や比率を表示するために利用する</h1>

<p>index <- iris$Sepal.Length > 5.5
dat &lt;- iris[index,]
xtabs(~Species, data=dat, drop.unused.levels=T)</p>

<h1>Species</h1>

<h1>setosa versicolor  virginica</h1>

<h1>3         39         49</h1>

<p>qplot(Species, data=dat, geom="bar")
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7298/13000340725_82a36b0b98.jpg" width="413" height="439" alt="Rplot01"></p>

<p>{% codeblock lang:bash %}</p>

<h1>ヒストグラム</h1>

<h1>身長・成績・人口などのように連続量のデータの度数(割合)を表示する</h1>

<p>qplot(Sepal.Length, data=iris, geom="histogram", binwidth=0.5, xlim=c(4,8))
{% endcodeblock %}</p>

<p><img src="http://farm3.staticflickr.com/2581/13000752024_5542498537.jpg" width="413" height="439" alt="Rplot"></p>

<p>{% codeblock lang:bash %}</p>

<h1>箱ひげ図</h1>

<h1>カテゴリごとの中央値、箱、ヒゲ、外れ値を表示。ヒストグラムの簡略版</h1>

<h1>1つが複数カテゴリのデータ、もう一つが連続量の場合に用いる</h1>

<p>qplot(Species, Sepal.Length, data=iris, geom="boxplot")
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3671/13000403615_dc98e591a0.jpg" width="413" height="439" alt="Rplot02"></p>

<p>{% codeblock lang:bash %}</p>

<h1>散布図</h1>

<h1>2つの変数の値に従って座標平面上に複数の点を表示</h1>

<h1>2つの変数がともに連続量の場合に用いる</h1>

<p>index &lt;- iris$Species != 'virginica'
dat &lt;- iris[index,]
qplot(Sepal.Length, Petal.Length, data=iris, geom=c("point", "smooth"), color=Species)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7353/13000607723_53c89f2449.jpg" width="413" height="439" alt="Rplot03"></p>

<h2>サンプルデータを使った単回帰分析</h2>

<p>サンプルデータを知らなすぎて、あまり適切な分析ではないですが、手元にある範囲内で。</p>

<h3>キーワード</h3>

<pre>
独立変数: 原因となる変数
従属変数: 性質を明らかにしたい側の変数
単回帰分析: 独立変数が1つの分析
重回帰分析: 独立変数が2つ以上の分析
</pre>


<p>{% codeblock lang:bash %}</p>

<h1>データ・セットを使った回帰分析のサンプル</h1>

<p>library(ggplot2)
qplot(Petal.Length, data=dat, geom="histogram", bandwith=0.5, xlim=c(3,6))
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7044/13155728714_9b43830cb0_o.png" /></p>

<p>{% codeblock lang:bash %}
index &lt;- iris$Species == 'versicolor'
dat &lt;- iris[index,]
result &lt;- glm(Sepal.Length~Petal.Length, data=dat, family=gaussian)
summary(result)</p>

<h1>glm(formula = Sepal.Length ~ Petal.Length, family = gaussian,</h1>

<h1>data = dat)</h1>

<h1></h1>

<h1>Deviance Residuals:</h1>

<h1>Min        1Q    Median        3Q       Max</h1>

<h1>-0.73479  -0.20272  -0.02065   0.26092   0.69956</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>Estimate Std. Error t value Pr(>|t|)</h1>

<h1>(Intercept)    2.4075     0.4463   5.395 2.08e-06 ***</h1>

<h1>Petal.Length   0.8283     0.1041   7.954 2.59e-10 ***</h1>

<h1>---</h1>

<h1>Signif. codes:  0 ‘<em><strong>’ 0.001 ‘</strong>’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</h1>

<h1></h1>

<h1>(Dispersion parameter for gaussian family taken to be 0.1173364)</h1>

<h1></h1>

<h1>Null deviance: 13.0552  on 49  degrees of freedom</h1>

<h1>Residual deviance:  5.6321  on 48  degrees of freedom</h1>

<h1>AIC: 38.717</h1>

<h1></h1>

<h1>Number of Fisher Scoring iterations: 2</h1>

<h1>=> 回帰式 y = ax + bで a = 2.4075, b = 0.8283</h1>

<h1>=> Coefficientsの最後が信頼度。'***'なので高い</h1>

<p>{% endcodeblock %}</p>

<h2>適切な回帰分析式の比較</h2>

<h3>AIC : 赤池情報量規準</h3>

<p>統計モデルの良さを評価するための指標。</p>

<h3>統計モデルの比較</h3>

<p>こちらも素人のトライなので、ツッコミお待ちしております！</p>

<p>{% codeblock lang:bash %}
library(ggplot2)</p>

<h1>単純にヒストグラムでプロット</h1>

<p>qplot(Volume, data=trees, geom="histogram", bandwith=5, xlim=c(10,80))
{% endcodeblock %}</p>

<p><img src="http://farm3.staticflickr.com/2741/13155614675_20ae60951c.jpg" width="413" height="439" alt="Rplot"></p>

<p>{% codeblock lang:bash %}</p>

<h1>対数をとってヒストグラムでプロット</h1>

<p>qplot(log(Volume), data=trees, geom="histogram", bandwith=0.2, xlim=c(2,5))
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7146/13156766444_35d1a2b410.jpg" width="413" height="439" alt="Rplot02"></p>

<p>本来は正規分布に従いそうなデータですが、サンプルデータを目視する限りは対数正規分布に近そう。</p>

<p>{% codeblock lang:bash %}
result &lt;- glm(Volume~Girth, data = trees, family=gaussian)
summary(result)</p>

<h1>(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***</h1>

<h1>Girth         5.0659     0.2474   20.48  &lt; 2e-16 ***</h1>

<h1>AIC: 181.64</h1>

<p>result &lt;- glm(Volume~Girth, data = trees, family=gaussian(link="log"))
summary(result)</p>

<h1>(Intercept) 1.340516   0.097224   13.79 2.89e-14 ***</h1>

<h1>Girth       0.147910   0.005839   25.33  &lt; 2e-16 ***</h1>

<h1>AIC: 169.14</h1>

<p>result &lt;- glm(Volume~(Girth+Height), data = trees, family=gaussian)
summary(result)</p>

<h1>(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***</h1>

<h1>Girth         4.7082     0.2643  17.816  &lt; 2e-16 ***</h1>

<h1>Height        0.3393     0.1302   2.607   0.0145 *</h1>

<h1>AIC: 176.91</h1>

<p>result &lt;- glm(Volume~(Girth+Height), data = trees, family=gaussian(link="log"))
summary(result)</p>

<h1>(Intercept) 0.679294   0.258124   2.632  0.01366 *</h1>

<h1>Girth       0.134163   0.006845  19.601  &lt; 2e-16 ***</h1>

<h1>Height      0.011144   0.003975   2.804  0.00907 **</h1>

<h1>AIC: 163.37</h1>

<h1>=> 上記4つのAIC(赤池情報量規準)からみると一番最後の回帰式(対数正規分布で独立変数にGirth, Heightをとる)が最も近い</h1>

<p>{% endcodeblock %}</p>

<h2>ポワソン回帰</h2>

<p>ポワソン回帰は離散値かつ、ゼロ以上の範囲のデータを扱うときに利用する。<br/></p>

<p>適切なデータでないんで、雰囲気だけメモ。</p>

<p>{% codeblock lang:bash %}
dat &lt;- InsectSprays
qplot(count, data=dat, geom="histogram", bandwith=1, xlim=c(0,30))
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3680/13319567853_70a00eccfc.jpg" width="413" height="439" alt="Rplot04"></p>

<p>{% codeblock lang:bash %}
result &lt;- glm(count~spray, data=dat, family=poisson(link="log"))
summary(result)</p>

<h1>(Intercept)  2.67415    0.07581  35.274  &lt; 2e-16 ***</h1>

<h1>sprayB       0.05588    0.10574   0.528    0.597</h1>

<h1>sprayC      -1.94018    0.21389  -9.071  &lt; 2e-16 ***</h1>

<h1>sprayD      -1.08152    0.15065  -7.179 7.03e-13 ***</h1>

<h1>sprayE      -1.42139    0.17192  -8.268  &lt; 2e-16 ***</h1>

<h1>sprayF       0.13926    0.10367   1.343    0.179</h1>

<h1>AIC: 376.59</h1>

<p>{% endcodeblock %}</p>

<h2>独立変数の最適化</h2>

<p>複数の独立変数から、Rが効率的なアルゴリズムで自動的にモデル選択をする方法。</p>

<p>適切なサンプルデータで後でリトライする予定。。。</p>

<p>{% codeblock lang:bash %}</p>

<h1>独立変数の最適化</h1>

<p>dat &lt;- iris
result &lt;- glm(Petal.Width~Sepal.Length+Sepal.Width+Petal.Length, data = dat, family=gaussian(link="log"))
step(result, direction="backward")</p>

<h1>Start:  AIC=19.47</h1>

<h1>Petal.Width ~ Sepal.Length + Sepal.Width + Petal.Length</h1>

<h1></h1>

<h1>Df Deviance     AIC</h1>

<h1>- Sepal.Width   1    9.472  19.337</h1>

<h1><none>               9.355  19.469</h1>

<h1>- Sepal.Length  1   12.225  57.615</h1>

<h1>- Petal.Length  1   33.293 207.890</h1>

<h1></h1>

<h1>Step:  AIC=19.34</h1>

<h1>Petal.Width ~ Sepal.Length + Petal.Length</h1>

<h1></h1>

<h1>Df Deviance     AIC</h1>

<h1><none>               9.472  19.337</h1>

<h1>- Sepal.Length  1   12.371  57.394</h1>

<h1>- Petal.Length  1   38.191 226.476</h1>

<h1></h1>

<h1>Call:  glm(formula = Petal.Width ~ Sepal.Length + Petal.Length, family = gaussian(link = "log"),</h1>

<h1>data = dat)</h1>

<h1></h1>

<h1>Coefficients:</h1>

<h1>(Intercept)  Sepal.Length  Petal.Length</h1>

<h1>-0.3528       -0.2442        0.4752</h1>

<h1></h1>

<h1>Degrees of Freedom: 149 Total (i.e. Null);  147 Residual</h1>

<h1>Null Deviance:      86.57</h1>

<h1>Residual Deviance: 9.472  AIC: 19.34</h1>

<p>#</p>

<h1>=> 以上より、Petal.Width = -0.2442<em>Sepal.Length + 0.4752</em>Petal.Length + -0.3528 がAICでベスト</h1>

<p>{% endcodeblock %}</p>

<h2>ユークリッド距離を使ったクラスタリング</h2>

<p>ユークリッド距離や重心の概念を使ってクラスタリングを行う。</p>

<p>Rに難儀しているので、とりあえず暫定で出来たところまで。使い方を覚えたらちゃんと直したい！</p>

<p>{% codeblock lang:bash %}
library("plyr")
dat &lt;- InsectSprays
dat2 &lt;- ddply(dat, .(spray), summarise, count_ave=mean(count))
distw &lt;- dist(dat2, method="euclidean")
clust &lt;- hclust(distw, method="ward")
plclust(clust, hang=-1, xlab="spray", ylab="count")
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3715/13157567135_d51493c391.jpg" width="413" height="439" alt="Rplot03"></p>

<h2>Special Thanks</h2>

<p><strong><a href="http://www.slideshare.net/teramonagi/tokyo-r30-20130420">「plyrパッケージで君も前処理スタ☆」改め「plyrパッケージ徹底入門」</a></strong><br/>
第30回R勉強会＠東京(#TokyoR)の資料</p>

<h2>あとで読む</h2>

<p><strong><a href="http://matome.naver.jp/odai/2134709644082692701">統計・機械学習・データマイニングの無料で読めるPDF資料</a></strong><br/></p>

<blockquote>
オンラインで無料公開されている書籍等のPDF資料をまとめ。
</blockquote>


<p><strong><a href="http://www.r.dl.itc.u-tokyo.ac.jp/node/33">統計的機械学習 | 中川研究室</a></strong><br/></p>

<blockquote>
統計的機械学習とは、観測されたデータから統計的手法を用い新たな知識を導出すること。 統計的機械学習には種々の分類がある、この説明。
</blockquote>


<p><strong><a href="http://www.slideshare.net/pfi/deep-learning-22350063">一般向けのDeep Learning</a></strong><br/></p>

<blockquote>
PFI 全体セミナーで発表した、専門家向けではなく一般向けのDeep Learning（深層学習）の解説です。どのような場面で活躍しているのか、今までの学習手法と何が違うのかを解説しています。
</blockquote>


<p><strong><a href="http://www.toukei-kentei.jp/index.html">統計検定：Japan Statistical Society Certificate</a></strong><br/></p>

<blockquote>
「統計検定」とは、統計に関する知識や活用力を評価する全国統一試験です。 データに基づいて客観的に判断し、科学的に問題を解決する能力は、仕事や研究をするための21世紀型スキルとして国際社会で広く認められています。
</blockquote>


<p><strong><a href="http://gci.t.u-tokyo.ac.jp/tutorial/">チュートリアル | グローバル消費インテリジェンス寄附講座</a></strong><br/></p>

<blockquote>
GCI チュートリアル (Global Consumer Intelligence Tutorial) は、東京大学工学部 松尾豊研究室とグローバル消費インテリジェンス寄附講座の学生のためのチュートリアルです。ウェブと人工知能を活かした研究を行うために必要な技能を身に付けることを目的としています。
</blockquote>


<p>{% include custom/google_ads_yoko_naga.html %}</p>

<h2>変更来歴</h2>

<p>(03/15 10:45) 回帰に関する記述を追記<br/>
(03/21 17:25) データ・フレームを追記<br/>
(03/22 10:50) 回帰分析部分の説明を追記<br/></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「Rによるやさしい統計学」で学ぶ R言語と統計学の基礎徹底解説 2-5章]]></title>
    <link href="http://morizyun.github.io/blog/easy-R-statistics-book-review/"/>
    <updated>2014-03-21T17:40:00+09:00</updated>
    <id>http://morizyun.github.io/blog/easy-R-statistics-book-review</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4274067106/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4274067106&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4274067106&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4274067106" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />R言語と統計学を同時に学ぶことができる『 <strong><a href="http://www.amazon.co.jp/gp/product/4274067106/ref=as_li_qf_sp_asin_il?ie=UTF8&amp;camp=247&amp;creative=1211&amp;creativeASIN=4274067106&amp;linkCode=as2&amp;tag=morizyun00-22">Rによるやさしい統計学</a></strong>』を少しずつ読み進めています。</p>

<p>この本は難しい数学の式などは出てこずに、わかりやすい言葉や例で統計学の基礎的なキーワードとそれに紐づく「<strong>Rの使い方</strong>」に焦点を当てて、説明をしてくれています。個人的には事前に基本的な概念を説明した入門書を一冊やってから、「Rではどうやるか？」を知るにオススメな本だと思います。</p>

<p>まだ前半戦の状態ですが、この中で特に覚えておきたい部分を中心にメモを書いていきます。</p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_yoko_naga.html %}</p>

<h2>2章: 1つの変数の記述統計</h2>

<p>データ解析の出発点となる『1つの変数』の記述方法についての説明。</p>

<h2>レクチャー</h2>

<p>{% codeblock lang:bash %}</p>

<h1>c: データの値を結合させる</h1>

<p>a &lt;- c(1, 2, 5, 10, 15, 5)</p>

<h1>ベクトル/行列の要素の平均 (行平均, 列平均)</h1>

<p>mean(a) #=> 6.333333</p>

<h1>中央値</h1>

<p>median(a) #=> 5</p>

<h1>頻度を整理して得られる分布である「度数分布」の表を作る</h1>

<p>table(a)
a
 1  2  5 10 15
 1  1  2  1  1</p>

<h1>不偏分散</h1>

<h2>確率変数の分布が期待値からどれだけ散らばっているかを示す値。</h2>

<p>var(a)</p>

<h1>標準偏差</h1>

<h2>ある集団についてのデータがどのように分布しているかを表す値。</h2>

<p>sd(a)
sqrt(mean((a-mean(a))<sup>2))</sup></p>

<h2>標本分散の関数を自作</h2>

<p>varp &lt;- function(x) {</p>

<pre><code>標本分散 &lt;- var(x)*(length(x) - 1)/length(x)
標本分散
</code></pre>

<p>}
varp(a)</p>

<h2>別ファイルの関数の読み込み</h2>

<p>source("varp.R")
{% endcodeblock %}</p>

<h3>例題やってみた</h3>

<p>{% codeblock lang:bash %}
a &lt;- c(60,100,50,40,50,230,120,240,200,30)
b &lt;- c(50,60,40,50,100,80,30,20,100,120)</p>

<h1>1)</h1>

<p>hist(a)
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3830/13230876803_318d1aa460.jpg" width="413" height="439" alt="Rplot03"></p>

<p>{% codeblock lang:bash %}
hist(b)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7136/13231086804_1af98a9796.jpg" width="413" height="439" alt="Rplot"></p>

<p>{% codeblock lang:bash %}</p>

<h1>2)</h1>

<p>mean(a)
sqrt(mean((a-mean(a))<sup>2))</sup> # 標準偏差
mean(b)
sqrt(mean((b-mean(b))<sup>2))</sup> # 標準偏差</p>

<h1>3)</h1>

<p>(a - mean(a))/sqrt(mean((a-mean(a))<sup>2))</sup>
(b - mean(b))/sqrt(mean((b-mean(b))<sup>2))</sup>
{% endcodeblock %}</p>

<h2>3章: 2つの変数の記述統計</h2>

<p>『2つの変数における量的変数における関係、質的変数における関係』についての説明です。ちなみに量的変数と質的変数の説明は次の通り。</p>

<h2>レクチャー</h2>

<pre>
* 量的変数: データ間に優劣や大小が比較できる。『テストの点数』のような数値データ
=> 比率データ: 絶対的なゼロ点を有する(質量、長さ、年齢、時間、金額など)
=> 間隔データ: 数値(メモリ)の間隔が等しい(気温、知能指数など)

* 質的変数: データ間に優劣がなく、大小関係が比較できない。「男・女」や「好き・嫌い」など
=> 順序データ: 大小関係(順序)のみ意味を持つ(満足度、選好度、硬度など)
=> 名義データ: 内容を区別するために便宜的に割りあてる(電話番号、性別など)
</pre>


<p>さらに、相関(correlation)と回帰(regression)の説明は次の通り。</p>

<pre>
相関 correlation #=> xとyの間に区別をもうけず対等に見る見方や方法
回帰 regression  #=> ｘからyを見る(yからxを見る)方法
</pre>


<p>{% codeblock lang:bash %}</p>

<h1>分散図(グラフの表示)</h1>

<p>a &lt;- c(1,2,3,4,5)
b &lt;- a * 2
plot(a, b)
{% endcodeblock %}</p>

<p><img src="http://farm3.staticflickr.com/2827/13231117083_681e5133e3.jpg" width="413" height="439" alt="Rplot01"></p>

<p>{% codeblock lang:bash %}</p>

<h1>共分散(数値)</h1>

<h2>2つのデータが、どれだけ関連性・連動性があるかを示す係数</h2>

<p>cov(a, b) #=> 5</p>

<h1>=> sum((a-mean(a))*(b-mean(b)))/length(a) と同義</h1>

<h1>=> mean((a-mean(a))*(b-mean(b))) と同義</h1>

<h1>相関係数</h1>

<h2>2つのデータが、どれだけ関連性があるのかを示す係数</h2>

<p>cor(a, b) #=> 1</p>

<h1>=> cov(a, b)/(sd(a)*sd(b))と同義</h1>

<p>{% endcodeblock %}</p>

<p>相関係数は<code>-1 &lt;= r &lt;= 1</code>で次のように関係性がわかります。</p>

<pre>
-0.2 <= r <=  0.2                 #=> ほとんど相関なし
-0.4 <= r <  -0.2, 0.2 < r <= 0.4 #=> 弱い相関あり
-0.7 <= r <   0.4, 0.4 < r <= 0.7 #=> 中程度の相関あり
-1.0 <= r <  -0.7, 0.7 < r <= 1.0 #=> 強い相関あり 
</pre>


<p>{% codeblock lang:bash %}</p>

<h1>クロス集計表</h1>

<h2>質的変数の関係について表す</h2>

<p>math &lt;- c('like', 'hate', 'like', 'like', 'hate')
statistics &lt;- c('hate', 'like', 'hate', 'like', 'like')
table(math, statistics)</p>

<pre><code>  statistics
</code></pre>

<p>math   hate like
  hate    0    2
  like    2    1</p>

<h1>ファイ係数</h1>

<h2>1と0の２つの値からなる変数（二値変数）に対して計算される相関係数</h2>

<h2>math(数学)、statistics(統計)の好き=>1、嫌い=>0に変換して考える</h2>

<p>math_zero <- ifelse(math=='like', 1, 0)
math_zero #=> 1 0 1 1 0
statistics_zero <- ifelse(statistics=='like', 1, 0)
statistics_zero #=> 0 1 0 1 1
cor(math_zero, statistics_zero)
{% endcodeblock %}</p>

<h3>例題やってみた</h3>

<p>{% codeblock lang:bash %}</p>

<h1>1) 散布図</h1>

<p>hour &lt;- c(1,3,10,12,6,3,8,4,1,5)
score &lt;- c(20,40,100,80,50,50,70,50,10,60)
plot(hour, score)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7004/13276651894_85af764fd4.jpg" width="413" height="439" alt="Rplot02"></p>

<p>{% codeblock lang:bash %}</p>

<h1>2) 相関係数</h1>

<p>cor(hour, score)</p>

<h1>=>  0.9092974 なので高い相関がある</h1>

<h1>3) クロス統計</h1>

<p>youwa &lt;- c('洋食', '和食', '和食', '洋食', '和食', '洋食', '洋食', '和食', '洋食', '洋食', '和食', '洋食', '和食', '洋食', '和食', '和食', '洋食', '洋食', '和食', '和食')
amakara &lt;- c('甘党', '辛党', '甘党', '甘党', '辛党', '辛党', '辛党', '辛党', '甘党', '甘党', '甘党', '甘党', '辛党', '辛党', '甘党', '辛党', '辛党', '甘党', '辛党', '辛党')
table(youwa, amakara)</p>

<h1>amakara</h1>

<h1>youwa  甘党 辛党</h1>

<h1>洋食    6    4</h1>

<h1>和食    3    7</h1>

<h1>4) ファイ係数</h1>

<p>youwa_zo &lt;- ifelse(youwa=='洋食', 1, 0)
amaka_zo &lt;- ifelse(amakara=='甘党', 1, 0)
cor(youwa_zo, amaka_zo)</p>

<h1>=> 0.3015113</h1>

<p>{% endcodeblock %}</p>

<h3>Rの便利メソッド</h3>

<p>{% codeblock lang:bash %}</p>

<h1>Rのメソッド</h1>

<h1>ifelse(条件, 真の場合, 偽の場合) 場合分けをする</h1>

<p>hour <- c(1,3,10,12,6,3,8,4,1,5)
hour_zo <- ifelse(hour >= 5, "up", "down")</p>

<h1>=> "down" "down" "up"   "up"   "up"   "down" "up"   "down" "down" "up"</h1>

<p>{% endcodeblock %}</p>

<h2>第4章: 母集団と標本</h2>

<p>大きな集団(母集団)から取り出した少数のデータ(標本)を使って『もとの集団の性質について推測する』ための推測統計の基本的な理論についての学習。</p>

<p>母集団と標本の説明はこちら。</p>

<pre>
母集団: データ全体を指す。国民全体、工業製品全体など
標本: 母集団から取り出したデータの一部
</pre>


<p>推測統計の分類についてはこちら。</p>

<pre>
推定: 具体的な数値を用いて『母数の値は◯◯くらいだろう』という結論を導くこと
点推定: 1つの値で空いての結果を表す。日本の中学生の数学平均点が「60点」と推定
区間推定: ある程度の幅を持った区間で結果を表す。数学平均が『50-70点』と推定
</pre>


<p>{% codeblock lang:bash %}</p>

<h2>サイコロを6回ふった場合、サイコロのすべての目が1回ずつ出るのは稀</h2>

<p>dice6 &lt;- ceiling(runif(n=6, min=0, max=6))
table(dice6)</p>

<h1>dice6</h1>

<h1>2 3 4</h1>

<h1>1 2 3</h1>

<h2>サイコロを600万回ふった場合、1/6に限りなく近づく</h2>

<p>dice6m &lt;- ceiling(runif(n=6000000, min=0, max=6))
table(dice6m)</p>

<h1>dice6m</h1>

<h1>1       2       3       4       5       6</h1>

<h1>999612 1000211  998870  999613 1000366 1001328</h1>

<p>table(dice6m)/6000000</p>

<h1>dice6m</h1>

<h1>1         2         3         4         5         6</h1>

<h1>0.1666020 0.1667018 0.1664783 0.1666022 0.1667277 0.1668880</h1>

<h1>男性が2/3、女性が1/3を表す棒グラフ</h1>

<p>barplot(c(2/3, 1/3), names.arg=c('man', 'woman'))
{% endcodeblock %}</p>

<p><img src="https://s3.yimg.com/so/7310/12550357034_2b0cda0363_z.jpg" width="618" height="640" alt="スクリーンショット 2014-02-16 8.12.38"></p>

<p>{% codeblock lang:bash %}</p>

<h1>正規分布</h1>

<h2>ある1つの数値を目標とした作業で偶然生じる偶然的な誤差の分布</h2>

<p>curve(dnorm(x, mean=0, sd=1), from=-4, to=4)
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3771/13276801433_f5a02457af.jpg" width="413" height="439" alt="Rplot01"></p>

<p>{% codeblock lang:bash %}</p>

<h1>母集団が正規分布であるような集団から無作為標本を抽出</h1>

<h2>ランダムに5つの標本を取得</h2>

<p>hyohon &lt;- rnorm(n=5, mean=50, sd=10)</p>

<h1>=> 62.69624 40.50412 51.14337 58.89571 36.13923</h1>

<h2>ヒストグラムを作成</h2>

<p>hist(hyohon)
{% endcodeblock %}</p>

<p><img src="http://farm3.staticflickr.com/2825/13277023004_286acdaa89.jpg" width="413" height="439" alt="Rplot03"></p>

<p>上のように標本数が少ないと、正規分布かどうかはわかりません。ただし、サンプルサイズをある程度大きくするとヒストグラムは正規分布に近いものになっていきます。</p>

<p>{% codeblock lang:bash %}</p>

<h1>標本数を増やした場合のヒストグラム => 正規分布に近づく</h1>

<p>hyohon <- rnorm(n=10000, mean=50, sd=10) #=> 正規分布に近いヒストグラムになる
hist(hyohon)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7393/13276723015_ee600af056.jpg" width="413" height="439" alt="Rplot04"></p>

<h3>練習問題をやってみた</h3>

<p>{% codeblock lang:bash %}</p>

<h1>1) 経験的な標本分布</h1>

<p>ave &lt;- numeric(length=5000)
for(i in 1:5000) {
  hyohon &lt;- rnorm(n=20, mean=50, sd=10)
  ave[i] &lt;- mean(hyohon)
}
hist(ave, freq=FALSE)
curve(dnorm(x, mean=50, sd=sqrt(100/20)), add=TRUE)
{% endcodeblock %}</p>

<p><img src="http://farm8.staticflickr.com/7278/13276903325_6fb58ee4df.jpg" width="413" height="439" alt="Rplot05"></p>

<p>{% codeblock lang:bash %}</p>

<h1>2) 理論的な標本分布</h1>

<p>curve(dnorm(x, sd=sqrt(1/25)), -3, 3)
curve(dnorm(x, sd=sqrt(1/16)), -3, 3, add=TRUE)
curve(dnorm(x, sd=sqrt(1/9)),  -3, 3, add=TRUE)
curve(dnorm(x, sd=sqrt(1/4)),  -3, 3, add=TRUE)
curve(dnorm(x, sd=sqrt(1/1)),  -3, 3, add=TRUE)
{% endcodeblock %}</p>

<p><img src="http://farm4.staticflickr.com/3696/13276920725_45b3471037.jpg" width="413" height="439" alt="Rplot06"></p>

<h3>この章で出てくるRの関数</h3>

<p>{% codeblock lang:bash %}</p>

<h1>小数点以下の切り上げ</h1>

<p>ceiling(1.5) #=> 2</p>

<h1>一様分布に従う乱数を発生させる</h1>

<p>runif(n=4, min=6, max=12)
[1]  8.430616 11.064664  9.835286 10.971259</p>

<h1>決まったパターンの乱数を発生させる</h1>

<p>set.seed(1) #=> 乱数の種を1に設定するこれを指定後は常に同じ乱数が発生する</p>

<h1>棒グラフを作成</h1>

<p>barplot(c(2/3, 1/3), names.arg=c("man", "woman"))</p>

<h1>関数の曲線を描く</h1>

<p>curve(dnorm(x), from=-3, to=3)</p>

<h1>正規分布の確率密度変数</h1>

<p>dnorm(0, mean=0, sd=1)</p>

<h1>正規分布に従う乱数を発生</h1>

<p>rnorm(n=5, mean=50, sd=10)</p>

<h1>数値を格納する領域の確保</h1>

<p>a &lt;- numeric(length=10000)</p>

<h1>連続データを作る</h1>

<h2>0から20までの整数の連続データをつくる</h2>

<p>1:20
[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20</p>

<h1>正規分布の確率密度関数</h1>

<p>dnorm(0, mean=0, sd=1) #=> 0.3989423</p>

<h1>棒グラフを作成</h1>

<p>barplot(c(3/4, 1/4), names.arg=c('man', 'woman'))</p>

<h1>関数の曲線を描く => curve()</h1>

<h2>正規分布のグラフが出力する場合</h2>

<p>curve(dnorm(x), from=-3, to=3)
{% endcodeblock %}</p>

<h2>第5章: 統計的仮説検定</h2>

<h3>検定の手順</h3>

<pre>
1) 母集団に関する帰無仮説と対立仮説を設定する
2) 検定統計量を選ぶ
3) 有意水準αの値を決める
4) データから検定統計量の実現値を求める
5) 検定統計量の実現値が棄却域に入る => 対立仮説を採用する
</pre>


<h3>検定のキーワード</h3>

<pre>
帰無仮説　　　#=>「差がない」、「効果が無い」という仮説
対立仮説　　　#=> 帰無仮説に反して「差がある」という仮説
検定統計量　　#=> 検定に用いられる標本統計量のこと
有意水準　　　#=> 帰無仮説を棄却し対立仮説を採択する基準
棄却域     　#=> 非常に生じにくい検定統計量の値の範囲 
ｐ値　　　　　#=> 帰無仮説が正しい仮説で、標本から計算した検定統計量を実現できる確率
第１種の誤り　#=> 「帰無仮説が真の時にこれを棄却する」誤り
第２種の誤り　#=> 「帰無仮説が偽の時にこれを採択する」誤り
検定力　　　　#=> 間違っている仮説を正しく棄却できる確率のこと(100% - 第２種の誤りの確率)
</pre>


<h3>標準正規分布を用いた検定の例題をやってみた</h3>

<pre>
# 標準正規分布を用いた検定の検定統計量 :
Z = mean(x) - 標準正規分布の平均/sqrt(標準正規分布の分散/標本数)
</pre>


<p>{% codeblock lang:bash %}
sinri <- c(13, 14, 7, 12, 10, 6, 8, 15, 4, 14, 9, 6, 10, 12, 5, 12, 8, 8, 12, 15)
Z <- (mean(sinri) - 12)/sqrt(10/length(sinri))
qnorm(0.025, lower.tail=FALSE)
qnorm(0.025) #=> -1.959964
qnorm(0.975) #=> 1.959964
Z &lt; qnorm(0.025) #=> TRUEなので帰無仮説を棄却</p>

<h1>pnorm関数からp値を求める</h1>

<p>2*pnorm(abs(Z), lower.tail=FALSE)</p>

<h1>=> 0.004677735 &lt; 有意水準0.05となるので帰無仮説を棄却</h1>

<p>{% endcodeblock %}</p>

<h3>t分布を用いた検定の例題をやってみた</h3>

<p>先述の「標準正規分布を用いた検定」は母集団の分散が計算に必須です。そこで、未知の母集団の分散の代わりに、自由度(データの標本数 - 1)のt分布を元に計算する検定があります。</p>

<pre>
# t分布を用いた検定の検定統計量 :
t = (mean(x) - 母集団の平均)/sqrt(t分布の分散/標本数)
</pre>


<p>{% codeblock lang:bash %}</p>

<h1>5.4 例題</h1>

<h1>帰無仮説「心理テスト(sinri)の平均は12である」のt検定</h1>

<p>sinri <- c(13, 14, 7, 12, 10, 6, 8, 15, 4, 14, 9, 6, 10, 12, 5, 12, 8, 8, 12, 15)
t <- (mean(sinri) -12)/sqrt(var(sinri)/length(sinri)) #=> 2.616648</p>

<h1>qt(p, df)で自由度dfの場合の下側確率pとなるt値が求まる</h1>

<p>qt(0.025, 19) #=> -2.093024
qt(0.975, 19) #=> 2.093024</p>

<p>t &lt; qt(0.025, 19)</p>

<h1>=> TRUEなので帰無仮説を棄却</h1>

<h1>関数t.testによる検定</h1>

<p>t.test(sinri, mu=12)</p>

<h1>One Sample t-test</h1>

<h1>data:  sinri</h1>

<h1>t = -2.6166, df = 19, p-value = 0.01697</h1>

<h1>alternative hypothesis: true mean is not equal to 12</h1>

<h1>95 percent confidence interval:</h1>

<h1>8.400225 11.599775</h1>

<h1>sample estimates:</h1>

<h1>mean of x</h1>

<h1>10</h1>

<p>{% endcodeblock %}</p>

<h3>無相関検定の例題をやってみた</h3>

<p>母集団において相関が0であることを検定するための手法。</p>

<pre>
# t分布を用いた検定の検定統計量 :
t = (標本相関 * sqrt(標本数 - 2))/sqrt(1 - 標本相関)
</pre>


<p>{% codeblock lang:bash %}</p>

<h2>帰無仮説 => toukei1とtoukei2の相関係数は0である</h2>

<p>toukei1 &lt;- c(6,10,6,10,5,3,5,9,3,3,11,6,11,9,7,5,8,7,7,9)
toukei2 &lt;- c(10,13,8,15,8,6,9,10,7,3,18,14,18,11,12,5,7,12,7,7)
t &lt;- (cor(toukei1, toukei2)*sqrt(20-2))/sqrt(1-cor(toukei1,toukei2)<sup>2)</sup></p>

<p>qt(0.025, 18) #=> -2.100922
qt(0.975, 18) #=>  2.100922
qt(0.975, 18) &lt; t #=> TRUEとなるので帰無仮説は棄却 => 相関はある</p>

<h1>無相関検定を実行する関数cor.text</h1>

<p>cor.test(toukei1, toukei2)</p>

<h1>Pearson's product-moment correlation</h1>

<h1></h1>

<h1>data:  toukei1 and toukei2</h1>

<h1>t = 4.8057, df = 18, p-value = 0.0001416</h1>

<h1>alternative hypothesis: true correlation is not equal to 0</h1>

<h1>95 percent confidence interval:</h1>

<h1>0.4596086 0.8952048</h1>

<h1>sample estimates:</h1>

<h1>cor</h1>

<h1>0.749659</h1>

<p>{% endcodeblock %}</p>

<h3>独立性の検定(χ2乗検定)の例題をやってみた</h3>

<p>{% codeblock lang:bash %}</p>

<h2>帰無仮説 => 「数学の好き嫌い」と「統計の好き嫌い」には差がない</h2>

<p>math &lt;- c("hate", "hate", "like", "like", "hate", "hate", "hate", "hate", "hate", "like", "like", "hate", "like", "hate", "hate", "like", "hate", "hate", "hate", "hate")
staitstics &lt;- c("like", "like", "like", "like", "hate", "hate", "hate", "hate", "hate", "hate", "like", "like", "like", "hate", "like", "hate", "hate", "hate", "hate", "hate")
table(math, staitstics)</p>

<h1>staitstics</h1>

<h1>math   hate like</h1>

<h1>hate   10    4</h1>

<h1>like    2    4</h1>

<p>exp_hate_hate = 12 * 14/20
exp_like_hate = 12 *  6/20
exp_hate_like =  8 * 14/20
exp_like_like =  8 *  6/20
exp_dosu &lt;- c(exp_hate_hate, exp_like_hate, exp_hate_like, exp_like_like)</p>

<h1>=> 8.4 3.6 5.6 2.4</h1>

<p>kansoku_dosu &lt;- c(10,2,4,4)
kai_nizyo &lt;- sum((exp_dosu -kansoku_dosu)<sup>2/exp_dosu)</sup></p>

<h1>=> 2.539683</h1>

<h1>χ 2乗分布に従う棄却域を求める => qchisq関数</h1>

<p>qchisq(0.95, 1) &lt; kai_nizyo #=> FALSE</p>

<h2>結論: 「数学の好き嫌い」と「統計の好き嫌い」には優位な差はない</h2>

<h2>χ2乗検定を行う関数 correct=FALSEは連続性の補正を行わないオプション</h2>

<p>chisq.test(table(math, staitstics), correct=FALSE)</p>

<h1>Pearson's Chi-squared test</h1>

<h1></h1>

<h1>data:  table(math, staitstics)</h1>

<h1>X-squared = 2.5397, df = 1, p-value = 0.111</h1>

<h1>=> pが0.11で0.05よりも大きな値なので帰無仮説が採択される</h1>

<p>{% endcodeblock %}</p>

<h3>練習問題をやってみた</h3>

<p>{% codeblock lang:bash %}</p>

<h2>1)</h2>

<h1>帰無仮説 : 平均170cmの正規分布に従うサンプルである</h1>

<p>height &lt;- c(165, 150, 170, 168, 159, 170, 167, 178, 155, 159, 161, 162, 166, 171, 155, 160, 168, 172, 155, 167)
t.test(height, mu=170)</p>

<h1>One Sample t-test</h1>

<h1></h1>

<h1>data:  height</h1>

<h1>t = -3.8503, df = 19, p-value = 0.001079</h1>

<h1>alternative hypothesis: true mean is not equal to 170</h1>

<h1>95 percent confidence interval:</h1>

<h1>160.584 167.216</h1>

<h1>sample estimates:</h1>

<h1>mean of x</h1>

<h1>163.9</h1>

<h1>=> p = 0.00107で 0.05より小さいので帰無仮説は棄却</h1>

<h2>2)</h2>

<h2>帰無仮説 : 勉強時間とテスト結果には相関性がない</h2>

<p>hour &lt;- c(1,3,10,12,6,3,8,4,1,5)
score &lt;- c(20,40,100,80,50,50,70,50,10,60)
cor.test(hour, score)</p>

<h1>Pearson's product-moment correlation</h1>

<h1></h1>

<h1>data:  hour and score</h1>

<h1>t = 6.1802, df = 8, p-value = 0.0002651</h1>

<h1>alternative hypothesis: true correlation is not equal to 0</h1>

<h1>95 percent confidence interval:</h1>

<h1>0.6542283 0.9786369</h1>

<h1>sample estimates:</h1>

<h1>cor</h1>

<h1>0.9092974</h1>

<h2>3)</h2>

<h2>スピアマンの順位相関係数</h2>

<h2>http://goo.gl/sFitJ</h2>

<p>cor.test(hour, score, method="spearman")</p>

<h1>Spearman's rank correlation rho</h1>

<h1></h1>

<h1>data:  hour and score</h1>

<h1>S = 10.1822, p-value = 5.887e-05</h1>

<h1>alternative hypothesis: true rho is not equal to 0</h1>

<h1>sample estimates:</h1>

<h1>rho</h1>

<h1>0.9382895</h1>

<h2>ケンドールの順位相関係数</h2>

<h2>http://goo.gl/2HRaa</h2>

<p>cor.test(hour, score, method="kendall")</p>

<h1>Kendall's rank correlation tau</h1>

<h1></h1>

<h1>data:  hour and score</h1>

<h1>z = 3.2937, p-value = 0.0009889</h1>

<h1>alternative hypothesis: true tau is not equal to 0</h1>

<h1>sample estimates:</h1>

<h1>tau</h1>

<h1>0.8471174</h1>

<h2>4) χ2乗検定</h2>

<p>youwa &lt;- c('洋食', '和食', '和食', '洋食', '和食', '洋食', '洋食', '和食', '洋食', '洋食', '和食', '洋食', '和食', '洋食', '和食', '和食', '洋食', '洋食', '和食', '和食')
amakara &lt;- c('甘党', '辛党', '甘党', '甘党', '辛党', '辛党', '辛党', '辛党', '甘党', '甘党', '甘党', '甘党', '辛党', '辛党', '甘党', '辛党', '辛党', '甘党', '辛党', '辛党')
chisq.test(youwa, amakara)</p>

<h1>Pearson's Chi-squared test with Yates' continuity correction</h1>

<h1></h1>

<h1>data:  youwa and amakara</h1>

<h1>X-squared = 0.8081, df = 1, p-value = 0.3687</h1>

<h2>5-1) 無相関検定</h2>

<p>lang &lt;- c(60, 40, 30, 70, 55)
soci &lt;- c(80, 25, 35, 70, 50)
cor.test(lang, soci)</p>

<h1>Pearson's product-moment correlation</h1>

<h1></h1>

<h1>data:  lang and soci</h1>

<h1>t = 2.6952, df = 3, p-value = 0.07408</h1>

<h1>alternative hypothesis: true correlation is not equal to 0</h1>

<h1>95 percent confidence interval:</h1>

<h1>-0.1590624  0.9892731</h1>

<h1>sample estimates:</h1>

<h1>cor</h1>

<h1>0.841263</h1>

<h1>=> p値が有意水準(0.05)より高いので2つの結果に相関はない</h1>

<h2>5-2) 無相関検定 : 同じサンプルを2回使う</h2>

<p>lang2 &lt;- rep(lang, 2)
soci2 &lt;- rep(soci, 2)
cor.test(lang2, soci2)</p>

<h1>Pearson's product-moment correlation</h1>

<h1></h1>

<h1>data:  lang2 and soci2</h1>

<h1>t = 4.4013, df = 8, p-value = 0.002283</h1>

<h1>alternative hypothesis: true correlation is not equal to 0</h1>

<h1>95 percent confidence interval:</h1>

<h1>0.4499858 0.9615658</h1>

<h1>sample estimates:</h1>

<h1>cor</h1>

<h1>0.841263</h1>

<h1>=> p値が有意水準(0.05)より低いので2つの結果に相関はある</h1>

<h1>=> (5-1)と(5-2)は相関係数(cor)は一致するがp値が大きく異る</h1>

<p>{% endcodeblock %}</p>

<h3>便利なRの関数</h3>

<p>{% codeblock lang:bash %}</p>

<h1>標準正規分布の下側確率に対応する値</h1>

<p>qnorm(0.025) #=> -1.959964</p>

<h1>標準正規分布の下側確率</h1>

<p>pnorm(-1.959964) #=> 0.025</p>

<h1>t分布で下側確率に対応する値</h1>

<p>qt(0.025, 19) #=> -2.093024</p>

<h1>t分布の下側確率</h1>

<p>pt(-2.093024, 19) #=> 0.025</p>

<h1>カイ二乗分布の下側確率に対応する値</h1>

<p>qchisq(0.95, 1) #=> 3.841459</p>

<h1>カイ二乗分布の下側確率に対応する値</h1>

<p>pchisq(3.841459, 1) #=> 0.95</p>

<h1>t分布の確率</h1>

<p>dt(1, 19) #=> 0.2357406</p>

<h1>カイ二乗分布の確率密度分布</h1>

<p>dchisq(3.841,1) #=> 0.02982808
{% endcodeblock %}</p>

<p>{% include custom/google_ads_yoko_naga.html %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[「統計学が最強の学問である」で学ぶ 統計学で見える新しい世界]]></title>
    <link href="http://morizyun.github.io/blog/statistics-is-best-knowledge-book-review/"/>
    <updated>2014-03-09T18:50:00+09:00</updated>
    <id>http://morizyun.github.io/blog/statistics-is-best-knowledge-book-review</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4478022216/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4478022216&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4478022216&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4478022216" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />少しずつ統計学を勉強していく中で「統計学で学んだこと」を少しでも実践で生かせるようなマインドを養いたかったので、『<strong><a href="http://www.amazon.co.jp/gp/product/4478022216/ref=as_li_qf_sp_asin_il?ie=UTF8&amp;camp=247&amp;creative=1211&amp;creativeASIN=4478022216&amp;linkCode=as2&amp;tag=morizyun00-22">統計学が最強の学問である</a></strong>』を読み始めました。</p>

<p>この記事ではこの本の中で特に参考になった点を中心にマトメていきます！</p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_square.html %}</p>

<h2>マイクロソフトのBlogでアツい3つの専門性</h2>

<pre>
* データマイニング、機械学習、人工知能、自然言語処理
* ビジネスインテリジェンス、競合分析
* 分析、統計、特にウェブ分析、A/Bテスト、統計解析
</pre>


<h2>データをビジネスで使うための「3つの問い」</h2>

<pre>
問1) 何かの要因が変化すれば利益が向上するのか？
問2) そうした変化を起こすような行動は実際に可能か？
問3) 変化を起こす行動が可能だとしてそのコストは利益を上回るのか？
</pre>


<h2>データから適切な判断ポイントを見つけるために重要な点</h2>

<pre>
1) 適切な比較を行うこと
2) ただの集計ではなくその誤差とp値について明らかにすること
</pre>


<h2>統計学における「ランダム化」の可能性と課題</h2>

<pre>
# ランダム化の利点
* 統計的な裏付けがないのに、「正しい」と決め付けるのは間違い
* 統計的な裏付けがないのに、「間違い」と決めつけるのも間違い
* 正解がないのであればとりあえずランダムに試すのにも価値がある

# ランダム化の欠点
絶対的なサンプル数の制限 => サンプル数が少なすぎると統計学は無意味
現実は条件を制御できない => 条件を満たすことが困難な場合統計学が脆い
倫理的な問題 => 有害性や不公平などの倫理も現実にはとても重要
不公平や劣等感が感情を逆なでする => 社会に対して不信感を与えない
</pre>


<h2>回帰分析について</h2>

<pre>
# 回帰分析とは？
データ間の関係性を記述する、もしくは一方のデータから他方のデータを予測する数式を推定

# 回帰分析で注意すべき点
ばらつきをもつ現象に対する理論的な予測はそれほどうまくいかない

# 統計学ができること
サンプルの統計量から、真値の信頼度を測る

# 重回帰分析とは？
予測したい結果に影響する要因が複数ある状況へ拡張された回帰分析

# 重回帰分析で注意すべき点
重回帰分析では、2つの説明変数を掛けあわせた新しい説明変数(相互作用項)にも注意が必要

# 回帰モデルの妥当性
=> AIC(赤池情報量基準)
=> 傾向スコア: 興味のある二値の説明変数についてどちらに該当するかという確率
</pre>


<h2>統計学における6つの分野</h2>

<pre>
1) 実態把握を行う社会学調査法
=> 可能な限り偏りを減らして、求める誤差に収まる推定値を効率よく算出する

2) 原因究明のための疫学・生物統計学
=> 原因を見つけることに重視。母集団への当てはまりにはこだわらない

3) 抽象的なものを測定する心理統計学
=> いくつかの測定方法から相関性を出して、数値化したのが知能指数(IQ)

4) 機械的分類のためのデータマイニング
=> マーケティングを目的にクラスタ分析や相関を調べる。ニューラル・ネットワークやサポートベクターマシンなどのような機械学習は、予測に役立つデータマイニングのための手法

5) 自然言語処理のためのテキストマイニング
=> 大量のテキストデータから目的にマッチしたデータを抽出・集計する。形態素解析として辞書を使うMeCabや辞書を使わずに重複する単語を探しだすN-Gramなどがある

6) 演繹に感心を寄せる数量経済学
=>経済学分野で統計学を用いる。相互作用を含む説明変数の選択について慎重な検討を行う

番外) 確率に対する考え方の違うベイズ派
=> 事前確率と事後確率を使う。限られた情報と仮定を組み合わせることで、迅速に答えを出す。
</pre>


<h2>先人たちの知識をフル動員する</h2>

<p>エビデンスが重要 : 統計的レビュー、メタアナリシス。Google Scholarなどで検索可能。この先人たちの知恵をフル動員することで、「最善」にコントロールすることが重要。</p>

<h2>覚えておきたいキーワード</h2>

<pre>
最小で十分なサンプル : 正しい判断に不必要な精度を求める必要はない
χ2乗検定 : 「意味のある偏り」、「誤差」のどちらかを確かめる手法
p値 : 誤差や偶然によってたまたま差が出る確率
真値 : 無制限にデータを集めればわかる本当に知りたい値
標準誤差 : 推定値の誤差の大きさ。
</pre>


<p>{% include custom/google_ads_square.html %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[統計解析 & R言語 超初心者入門資料まとめ]]></title>
    <link href="http://morizyun.github.io/blog/statistics-analysis-bigginer-r/"/>
    <updated>2014-02-23T11:00:00+09:00</updated>
    <id>http://morizyun.github.io/blog/statistics-analysis-bigginer-r</id>
    <content type="html"><![CDATA[<p><a href="http://www.amazon.co.jp/gp/product/4873116155/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4873116155&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4873116155&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4873116155" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />興味を持ち続けていた統計解析や、R言語の勉強をはじめました！まだまだ初歩の初歩ですが、この記事がいつか偉大な一歩になれるように頑張っていく所存ですw</p>

<p>まずは、R言語や統計解析に関する入門記事や、モチベーションがアップしそうな記事をまとめていきます！</p>

<p><strong>(02/23 11:00) 初学者の人におすすめな資料にフォーカスしてまとめ直し</strong></p>

<!-- more -->


<br style="clear:both;"/>


<p>{% include custom/google_ads_square.html %}</p>

<h2>[スライド] 統計学入門</h2>

<iframe src="http://www.slideshare.net/slideshow/embed_code/5349634" width="597" height="486" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen></iframe>


<p>統計学の全体像をつかむのに最適なスライドです。初歩。。。とはちょっと呼べないくらい内容が深いです！</p>

<h2>[スライド] はじめての「R」</h2>

<iframe src="http://www.slideshare.net/slideshow/embed_code/15432969" width="597" height="486" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/m884/japan-r-15432969" title="はじめての「R」" target="_blank">はじめての「R」</a> </strong> from <strong><a href="http://www.slideshare.net/m884" target="_blank">Masahiro Hayashi</a></strong> </div></p>

<p>統計解析を始めるときにWindowsな方も、Macな方もとっつきやすのが『<strong><a href="http://www.r-project.org/">R</a></strong>』です。このRを完全初心者をターゲットに説明をして頂けている資料です。超わかりやすいです！</p>

<h2>[デスクトップアプリ] R用のIDE: RStudio</h2>

<p><strong><a href="http://www.rstudio.com/ide/download/desktop">RStudio</a></strong><br/>
RStudioはR言語用のIDEです。Window、Mac、Linuxに対応したフリーのオープンソース・ソフトです。</p>

<h2>[CUI] Mac コマンドラインのRインストール手順</h2>

<p>パッケージ版もありますが、コマンドライン版のRをインストールする手順を紹介します。</p>

<p>まず『<strong><a href="https://xquartz.macosforge.org/landing/">XQuartz</a></strong>』からXQuartzのパッケージをDL&amp;インストールします。ちなみにこの<strong>XQuartz</strong>はX Window SystemをMacで動くようにするためのものです。</p>

<p>{% codeblock lang:bash %}</p>

<h1>Homebrewをupdate</h1>

<p>brew update</p>

<h1>コンパイラ gfortranのインストール</h1>

<p>brew install gfortran</p>

<h1>フォーミュラをhomebrewの中に取り込む</h1>

<p>brew tap homebrew/science</p>

<h1>Rのインストール</h1>

<p>brew install R</p>

<h1>Rの起動</h1>

<p>R</p>

<h1>Rを終了するためのコマンド</h1>

<p>q()
{% endcodeblock %}</p>

<h2>[Web] とある弁当屋の統計技師</h2>

<p><a href="http://rmecab.jp/ranko/chap1.html"><img title="とある弁当屋の統計技師（データサイエンティスト）" src="http://capture.heartrails.com/400x300/cool/1392461821673?http://rmecab.jp/ranko/chap1.html" alt="http://rmecab.jp/ranko/chap1.html" width="400" height="300" /></a></p>

<p>こちらはRの導入から始まって基本的な使い方のチュートリアルまでをかなり細かく解説をしてくれています。導入の時には一番オススメなサイトです！</p>

<h2>[Web・英語] Code School Try R</h2>

<p><strong><a href="http://tryr.codeschool.com/levels/1/challenges/3">Code School Try R</a></strong><br/>
インタラクティブにRを学ぶことができるサイト。唯一日本語に対応していないのが残念。</p>

<h2>[書籍] Rによるやさしい統計学</h2>

<p><a href="http://www.amazon.co.jp/gp/product/4274067106/ref=as_li_qf_sp_asin_il?ie=UTF8&camp=247&creative=1211&creativeASIN=4274067106&linkCode=as2&tag=morizyun00-22"><img border="0" src="http://ws.assoc-amazon.jp/widgets/q?_encoding=UTF8&ASIN=4274067106&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=morizyun00-22" width="150" style="float: left; margin: 0 20px 20px 0;" ></a><img src="http://www.assoc-amazon.jp/e/ir?t=morizyun00-22&l=as2&o=9&a=4274067106" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />これは最近購入して読み進めている『<strong><a href="http://www.amazon.co.jp/gp/product/4274067106/ref=as_li_qf_sp_asin_il?ie=UTF8&amp;camp=247&amp;creative=1211&amp;creativeASIN=4274067106&amp;linkCode=as2&amp;tag=morizyun00-22">Rによる統計解析</a></strong>』です。まだ前半戦の状態ですが、基本的には難しい数式ではなく、Rを使いながら基本的な統計の用語を学んでいけるので、初学者の人で数式の書かれた本が読めない僕のような人に超おすすめな本です。僕はあとは統計学の漫画系の書籍を買えるだけ買ってみました！そちらもおいおいレポートしていきます＾＾</p>

<br style="clear:both;"/>


<p>ここに書いていた書籍の中のメモ書きは増えてきたので『<strong><a href="/blog/easy-R-statistics-book-review/">「Rによるやさしい統計学」で学ぶ R言語と統計学の基礎徹底解説</a></strong>』に移動しました！</p>

<h2>[Web] R-Tips : Rの基本的なコマンドの使い方がわかる</h2>

<p><a href="http://cse.naro.affrc.go.jp/takezawa/r-tips/r.html"><img title="R-Tips" src="http://capture.heartrails.com/400x300/cool?http://cse.naro.affrc.go.jp/takezawa/r-tips/r.html" alt="http://cse.naro.affrc.go.jp/takezawa/r-tips/r.html" width="400" height="300" /></a></p>

<p>Rをプログラミング言語の側面で特に覚えておくべきことをわかりやすく項目別にまとめてくれています。</p>

<h2>[Web] R Markdownのシェアサイト</h2>

<p><strong><a href="http://rpubs.com/">RPubs</a></strong><br/>
RStudio用のプラグイン R Markdown形式で書かれた情報をシェアできるサイト。なかなか面白う！</p>

<p>{% include custom/google_ads_square.html %}</p>

<h2>Speical Thanks</h2>

<p><strong><a href="http://qiita.com/dkkoma/items/8b45ce94c2c5d89d567f">MacOSX に homebrew で R をインストール - Qiita</a></strong></p>

<p><strong><a href="http://statsguild.blog102.fc2.com/blog-entry-4.html">新人OLのはじめての統計 質的変数・量的変数とはなんぞや？</a></strong></p>

<h2>変更来歴</h2>

<p>(02/23 11:00) 初学者の人におすすめな資料にフォーカスしてまとめ直し<br/></p>
]]></content>
  </entry>
  
</feed>
